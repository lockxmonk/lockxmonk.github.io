<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  caffe框架学习 - LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">统计学习方法</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Python练习</a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">图像去雾技术</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14963894105351.html">
                
                  <h1>提交测试</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	
                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14955929300450.html">
                
                  <h1>Caffe目录结构</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>我们这里先了解一下caffe的目录结构:<br/>
<img src="media/14955929300450/14961936456285.jpg" alt=""/><br/>
<img src="media/14955929300450/14961936868967.jpg" alt=""/><br/>
<img src="media/14955929300450/14961937240927.jpg" alt=""/><br/>
其中主要关注的就是:include/,src/和tools/,三个目录,需要分析的代码都包含在这里面.</p>

<p>在阅读代码时,如何快速追踪某个关键词?传统的方法是打开某个文件,之后用查找命令来查找关键词.<br/>
这了我们介绍另一种方法,利用<code>grep</code>命令:</p>

<pre><code>➜  caffe git:(master) ✗ grep -n -H -R &quot;REGISTER_LAYER_CREATOR&quot; *
</code></pre>

<p><img src="media/14955929300450/14955931415464.jpg" alt=""/><br/>
我们可以看到,日志输出了很多文件.这种方法无需分别打开每个文件,也能直观的显示了所有包含这个宏的文件名和行号.</p>

<p>这里我们用<code>grep</code>命令来搜索一个宏调用:<code>REGISTER_LAYER_CREATOR</code>,<br/>
命令行参数解释为:</p>

<blockquote>
<p>-n  ---显示行号,便于定位<br/>
-H  ---显示文件名,便与定位<br/>
-R  ---递归查找每个子目录,适合工程较大,分多个目录存放的场景</p>
</blockquote>

<p>利用这种方法可以很容易的在caffe源码中定位很多内容.</p>

<h2 id="toc_0">卷基层</h2>

<p>这里我们假定卷积层有<code>L</code>个输出通道和<code>K</code>个输入通道,于是需要有<code>LK,L=50,K=20</code>个卷积核实现通道数目的转换.这里我们假定卷集核大小为<code>I*J = 5*5</code>,每个输出通道的特征图大小为<code>M*N = 8*8</code>,则该层每个样本做一次前向传播时卷积层计算量为:<br/>
<code>Calculations(MAC) = I*J*M*N*K*L = 5*5*8*8*50*20=1600000MAC</code><br/>
实际中使送入一批样本(batch),所以我们这里还需要计算量乘上批量尺寸.</p>

<p>我们这里卷积层的学习参数量为:<br/>
<code>Params = I*J*K*L = 25000</code><br/>
所以计算量-参数量之比为<code>CPR=Calculations/Params = M*N = 64</code><br/>
<font color=red><strong>所以我们得出结论是:卷基层的输出特征图尺寸越大,CPR值越大,参数重复利用率越高.,若一次性输入一批数据(B个样本),则CPR值可再提高B倍.</strong></font></p>

<h2 id="toc_1">全连接层</h2>

<p>早在卷积神经网络出现之前,最早的深度学习网络计算类型都是全连接形式的.如下所示.<br/>
<img src="media/14955929300450/14961962825939.jpg" alt=""/><br/>
每个节点与相邻层的所有节点都有连接关系,这是全连接层名称的由来.</p>

<p>全连接层的主要计算类型为矩阵-向量乘(GEMV).假设输入节点组成的向量为x,维度为D,输出节点组成的向量为y,维度为V,则全连接层计算可以表示为:<br/>
\[y=Wx\]<br/>
其中,W为<code>V*D</code>维权值矩阵.</p>

<p>我们分析全连接层的参数:<br/>
<img src="media/14955929300450/14961986814721.jpg" alt=""/></p>

<p><img src="media/14955929300450/14961986616938.jpg" alt=""/></p>

<p>得出输出<code>V=500</code>,输入<code>D=50*4*4 = 800</code>(其中50是输入数量,4*4位图的尺寸)</p>

<p>则全连接层单样本前向传播计算量为:<br/>
\[<br/>
CalculationsMAC = V*D<br/>
                = 800 * 500<br/>
                = 400000<br/>
\]<br/>
参数统计量为:<br/>
\[<br/>
Params = V*D<br/>
       = 800 * 500<br/>
       = 400000<br/>
\]<br/>
所以CPR值为1</p>

<p>所以得出结论,全连接层的CPR值始终为1,与输入,输出维度无关.所以单样本前向向传播计算时,权值重复利用率很低.<br/>
    我们将一批(B个)样本逐列拼接成矩阵X,一次性通过全连接层,得到一批输出向量构成的矩阵Y,称作批处理(矩阵-矩阵乘计算GEMM):<br/>
    \[<br/>
    Y=WX<br/>
    \]<br/>
这样全连接层前向计算量提高了B倍,而参数量不变,因此CPR提高了B倍.</p>

<p>与卷积层相比，全连接层参数量是其16倍，而计算量只有其25%.如果输出特征图尺寸相同<code>（M*V = V)</code>，卷积层的CPR值为全连接层的<code>M*N</code>倍。也就是说，卷积层在输出特征图维度实现了<mark>权值共享</mark>。这是降低参数量的重要举措。与此同吋，卷枳层<mark>局部连接</mark>特性 (相比全连接）也大幅减少了参数量,这使得CNN网络中前几层卷积层参数量占比小，计算量占比大；而后几层全连接层参数量占比大，计算量占比小。大多数CNN模型都符合这个特点。<font color=red><strong>因此我们在进行计算加速优化时,重点放在卷积层；而在进行参数优化、权值剪裁时，重点放在全连接层，</strong></font></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14955245049549.html">
                
                  <h1>运行caffe框架中的cifar10样例</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>1.先运行caffe目录下的data/get_cifar10.sh脚本.</p>

<pre><code>#!/usr/bin/env sh
# This scripts downloads the CIFAR10 (binary version) data and unzips it.

DIR=&quot;$( cd &quot;$(dirname &quot;$0&quot;)&quot; ; pwd -P )&quot;
cd &quot;$DIR&quot;

echo &quot;Downloading...&quot;

wget --no-check-certificate http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz

echo &quot;Unzipping...&quot;

tar -xf cifar-10-binary.tar.gz &amp;&amp; rm -f cifar-10-binary.tar.gz
mv cifar-10-batches-bin/* . &amp;&amp; rm -rf cifar-10-batches-bin

# Creation is split out because leveldb sometimes causes segfault
# and needs to be re-created.

echo &quot;Done.&quot;
</code></pre>

<p>获取数据集.</p>

<p>之后运行example下的cifar10/create_cifar10.sh<br/>
但是会遇到以下报错:<br/>
<img src="media/14955245049549/14955247436003.jpg" alt=""/></p>

<p>这里要运行下面这个命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/examples/cifar10/convert_cifar_data.bin
</code></pre>

<p><img src="media/14955245049549/14955249112611.jpg" alt=""/></p>

<p>再次运行./examples/cifar10/create_cifar10.sh后又会出现一个错误:<br/>
<img src="media/14955245049549/14955249879358.jpg" alt=""/><br/>
这里要再次执行以下命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/tools/compute_image_mean
</code></pre>

<p><img src="media/14955245049549/14955250166086.jpg" alt=""/></p>

<p>然后再次执行:</p>

<pre><code>./examples/cifar10/create_cifar10.sh
</code></pre>

<p>结果成功了!如下图所示:<br/>
<img src="media/14955245049549/14955251049094.jpg" alt=""/></p>

<h1 id="toc_0">Training and Testing the &quot;Quick&quot; Model</h1>

<p>因为例子中已经给出定义好的protobuf和solver protobuf文件,所以我们直接运行<code>train_quick.sh</code></p>

<p>该文件内容为:</p>

<pre><code class="language-sh">#!/usr/bin/env sh
set -e

TOOLS=./build/tools

$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver.prototxt $@

# reduce learning rate by factor of 10 after 8 epochs
$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver_lr1.prototxt \
  --snapshot=examples/cifar10/cifar10_quick_iter_4000.solverstate $@
</code></pre>

<p>执行如下命令:</p>

<pre><code>➜ caffe git:(master) ✗ ./examples/cifar10/train_quick.sh
</code></pre>

<p>然后输出为:</p>

<pre><code>I0523 15:43:36.608793 2712679360 caffe.cpp:211] Use CPU.
I0523 15:43:36.609737 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 4000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 4000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0523 15:43:36.610075 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.610931 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 15:43:36.610961 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 15:43:36.610966 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.611205 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.611467 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 15:43:36.611524 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.611531 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.611549 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.611565 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.611686 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.617992 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.618022 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.618028 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.618032 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.618041 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.618052 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.618057 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.618063 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.618175 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.618180 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.618185 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 15:43:36.618192 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.618199 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.618202 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.618206 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.618216 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.618219 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618224 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 15:43:36.618228 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.618234 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.618238 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.618242 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.618247 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.618250 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618255 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 15:43:36.618263 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.618273 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.618276 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.618281 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.618585 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.618592 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618597 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 15:43:36.618602 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.618607 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.618609 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.618614 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.618619 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.618623 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618628 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 15:43:36.618630 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.618634 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.618638 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.618643 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.618647 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.618654 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.618662 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 15:43:36.618669 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.618680 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.618685 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.618695 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.619361 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.619372 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619379 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 15:43:36.619385 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.619390 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.619393 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.619398 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.619403 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.619447 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619459 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 15:43:36.619467 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.619477 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.619484 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.619493 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.619505 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.619513 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.619523 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 15:43:36.619529 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.619539 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.619546 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.619555 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.620586 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.620602 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.620607 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 15:43:36.620613 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.620620 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.620625 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.620630 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.620649 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.620656 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.620662 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 15:43:36.620673 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620682 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.620689 2712679360 net.cpp:406] loss &lt;- ip2
I0523 15:43:36.620697 2712679360 net.cpp:406] loss &lt;- label
I0523 15:43:36.620703 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.620730 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620749 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.620756 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.620764 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.620787 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 15:43:36.620795 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.620800 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.620807 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.620813 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.620820 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.620832 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.620851 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.620859 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.620867 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.620875 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.620882 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.620889 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.620896 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.620904 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.620916 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.621170 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.621199 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 15:43:36.621210 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.621821 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.621913 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 15:43:36.621933 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.621943 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.621950 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.621958 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.622017 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.624790 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.624822 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.624830 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624835 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.624840 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 15:43:36.624851 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 15:43:36.624856 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 15:43:36.624862 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 15:43:36.624869 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 15:43:36.624876 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 15:43:36.624878 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624882 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624886 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 15:43:36.624917 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.624927 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.624930 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.624935 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.624987 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.624991 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.624996 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 15:43:36.625002 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.625008 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.625011 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.625015 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.625022 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.625026 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625031 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 15:43:36.625036 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.625041 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.625043 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.625048 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.625053 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.625056 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625061 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 15:43:36.625064 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.625071 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.625074 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.625084 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.625396 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.625402 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625407 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 15:43:36.625412 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.625417 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.625422 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.625425 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.625429 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.625433 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625437 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 15:43:36.625440 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.625445 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.625448 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.625452 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.625458 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.625460 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.625464 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 15:43:36.625468 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.625474 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.625479 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.625483 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.626077 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.626083 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626088 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 15:43:36.626093 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.626098 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.626101 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.626106 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.626111 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.626113 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626117 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 15:43:36.626121 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.626126 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.626129 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.626145 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.626152 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.626154 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.626159 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 15:43:36.626163 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.626168 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.626173 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.626176 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.626969 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.626981 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.626986 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 15:43:36.626992 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.626999 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.627003 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.627008 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.627024 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.627028 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627032 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 15:43:36.627039 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 15:43:36.627046 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 15:43:36.627053 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 15:43:36.627059 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 15:43:36.627068 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 15:43:36.627076 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 15:43:36.627081 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627085 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627089 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 15:43:36.627094 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 15:43:36.627099 2712679360 net.cpp:84] Creating Layer accuracy
I0523 15:43:36.627102 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 15:43:36.627106 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 15:43:36.627110 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 15:43:36.627116 2712679360 net.cpp:122] Setting up accuracy
I0523 15:43:36.627120 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627123 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 15:43:36.627126 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627133 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.627169 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 15:43:36.627178 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 15:43:36.627183 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.627189 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627198 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.627202 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627207 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.627213 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 15:43:36.627215 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.627219 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 15:43:36.627223 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 15:43:36.627228 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.627230 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.627234 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.627321 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.627334 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.627341 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.627348 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.627354 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.627387 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.627394 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.627400 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.627409 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 15:43:36.627418 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.627432 2712679360 net.cpp:242] This network produces output accuracy
I0523 15:43:36.627454 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.627470 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.627553 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 15:43:36.627593 2712679360 caffe.cpp:248] Starting Optimization
I0523 15:43:36.627602 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 15:43:36.627610 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 15:43:36.627933 2712679360 solver.cpp:330] Iteration 0, Testing net (#0)
I0523 15:43:46.157997 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:43:46.542196 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.0865
I0523 15:43:46.542232 2712679360 solver.cpp:397]     Test net output #1: loss = 2.3025 (* 1 = 2.3025 loss)
I0523 15:43:46.784966 2712679360 solver.cpp:218] Iteration 0 (0 iter/s, 10.157s/100 iters), loss = 2.30202
I0523 15:43:46.785002 2712679360 solver.cpp:237]     Train net output #0: loss = 2.30202 (* 1 = 2.30202 loss)
I0523 15:43:46.785009 2712679360 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0523 15:44:08.112608 2712679360 solver.cpp:218] Iteration 100 (4.68889 iter/s, 21.327s/100 iters), loss = 1.67773
I0523 15:44:08.112664 2712679360 solver.cpp:237]     Train net output #0: loss = 1.67773 (* 1 = 1.67773 loss)
I0523 15:44:08.112673 2712679360 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0523 15:44:29.336644 2712679360 solver.cpp:218] Iteration 200 (4.71187 iter/s, 21.223s/100 iters), loss = 1.59886
I0523 15:44:29.336683 2712679360 solver.cpp:237]     Train net output #0: loss = 1.59886 (* 1 = 1.59886 loss)
I0523 15:44:29.336693 2712679360 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0523 15:44:50.573981 2712679360 solver.cpp:218] Iteration 300 (4.70876 iter/s, 21.237s/100 iters), loss = 1.31839
I0523 15:44:50.574038 2712679360 solver.cpp:237]     Train net output #0: loss = 1.31839 (* 1 = 1.31839 loss)
I0523 15:44:50.574044 2712679360 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0523 15:45:12.080576 2712679360 solver.cpp:218] Iteration 400 (4.64987 iter/s, 21.506s/100 iters), loss = 1.24876
I0523 15:45:12.080610 2712679360 solver.cpp:237]     Train net output #0: loss = 1.24876 (* 1 = 1.24876 loss)
I0523 15:45:12.080618 2712679360 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0523 15:45:32.450579 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:33.342396 2712679360 solver.cpp:330] Iteration 500, Testing net (#0)
I0523 15:45:42.732501 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:43.134589 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.5366
I0523 15:45:43.134620 2712679360 solver.cpp:397]     Test net output #1: loss = 1.31952 (* 1 = 1.31952 loss)
I0523 15:45:43.360550 2712679360 solver.cpp:218] Iteration 500 (3.19703 iter/s, 31.279s/100 iters), loss = 1.22391
I0523 15:45:43.360582 2712679360 solver.cpp:237]     Train net output #0: loss = 1.22391 (* 1 = 1.22391 loss)
I0523 15:45:43.360589 2712679360 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0523 15:46:06.734716 2712679360 solver.cpp:218] Iteration 600 (4.27826 iter/s, 23.374s/100 iters), loss = 1.23177
I0523 15:46:06.734771 2712679360 solver.cpp:237]     Train net output #0: loss = 1.23177 (* 1 = 1.23177 loss)
I0523 15:46:06.734779 2712679360 sgd_solver.cpp:105] Iteration 600, lr = 0.001
.......数据形式基本相同 故省略...
I0523 16:00:46.286926 2712679360 solver.cpp:218] Iteration 3900 (4.08731 iter/s, 24.466s/100 iters), loss = 0.557826
I0523 16:00:46.286960 2712679360 solver.cpp:237]     Train net output #0: loss = 0.557826 (* 1 = 0.557826 loss)
I0523 16:00:46.286967 2712679360 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0523 16:01:09.469552 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:10.472170 2712679360 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/cifar10_quick_iter_4000.caffemodel
I0523 16:01:10.475755 2712679360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:10.590515 2712679360 solver.cpp:310] Iteration 4000, loss = 0.641508
I0523 16:01:10.590548 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:21.619536 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:22.054498 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:22.054538 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:22.054548 2712679360 solver.cpp:315] Optimization Done.
I0523 16:01:22.054555 2712679360 caffe.cpp:259] Optimization Done.
I0523 16:01:22.119184 2712679360 caffe.cpp:211] Use CPU.
I0523 16:01:22.120214 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.0001
display: 100
max_iter: 5000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
snapshot_format: HDF5
I0523 16:01:22.120556 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.120817 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 16:01:22.120833 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 16:01:22.120841 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.121104 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.121320 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 16:01:22.121383 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.121393 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.121413 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.121431 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.121585 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.128842 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.128867 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.128875 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.128880 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.128890 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.128902 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.128907 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.128914 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.129009 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.129017 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.129022 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 16:01:22.129030 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.129039 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.129042 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.129047 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.129057 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.129062 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129067 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 16:01:22.129071 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.129078 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.129083 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.129087 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.129093 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.129097 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129102 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 16:01:22.129106 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.129117 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.129120 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.129125 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.129482 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.129487 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129493 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 16:01:22.129500 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.129505 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.129509 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.129514 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.129520 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.129524 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129528 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 16:01:22.129534 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.129537 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.129541 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.129547 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.129554 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.129557 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.129562 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 16:01:22.129566 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.129573 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.129577 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.129585 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.130280 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.130286 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130292 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 16:01:22.130298 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.130304 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.130308 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.130313 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.130318 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.130353 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130360 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 16:01:22.130364 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.130370 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.130374 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.130379 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.130385 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.130389 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.130396 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 16:01:22.130400 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.130409 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.130414 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.130419 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.131337 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.131347 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.131352 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 16:01:22.131358 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.131364 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.131369 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.131374 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.131392 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.131397 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.131400 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 16:01:22.131407 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131413 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.131417 2712679360 net.cpp:406] loss &lt;- ip2
I0523 16:01:22.131422 2712679360 net.cpp:406] loss &lt;- label
I0523 16:01:22.131427 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.131435 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131448 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.131453 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.131458 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.131471 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 16:01:22.131476 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.131495 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.131505 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.131510 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.131515 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.131518 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.131522 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.131527 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.131531 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.131536 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.131541 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.131544 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.131548 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.131552 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.131561 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.131786 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.131814 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:01:22.131826 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.132225 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.132313 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:01:22.132342 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.132356 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.132364 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.132372 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.132438 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.134943 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.134956 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.134963 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.134968 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.134974 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:01:22.134984 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:01:22.135015 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:01:22.135064 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:01:22.135078 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:01:22.135116 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:01:22.135167 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135203 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135241 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:01:22.135313 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.135330 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.135335 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.135342 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.135398 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.135404 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.135411 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:01:22.135418 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.135463 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.135473 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.135514 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.135565 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.135574 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135581 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:01:22.135586 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.135593 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.135598 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.135603 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.135609 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.135613 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135666 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:01:22.135673 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.135681 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.135686 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.135700 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.136068 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.136076 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136081 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:01:22.136088 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.136095 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.136098 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.136103 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.136108 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.136112 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136117 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:01:22.136121 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.136127 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.136132 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.136135 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.136142 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.136147 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.136152 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:01:22.136157 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.136163 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.136168 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.136173 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.136878 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.136888 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136893 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:01:22.136899 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.136904 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.136909 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.136914 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.136919 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.136930 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136961 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:01:22.136968 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.136976 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.137001 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.137008 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.137017 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.137022 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.137027 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:01:22.137032 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.137039 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.137044 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.137050 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.137981 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.137995 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.138002 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:01:22.138008 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.138016 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.138021 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.138027 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.138046 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.138051 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138056 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:01:22.138062 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:01:22.138085 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:01:22.138103 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:01:22.138115 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:01:22.138129 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:01:22.138142 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:01:22.138150 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138160 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138170 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:01:22.138177 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:01:22.138187 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:01:22.138219 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:01:22.138231 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:01:22.138242 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:01:22.138257 2712679360 net.cpp:122] Setting up accuracy
I0523 16:01:22.138264 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138274 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:01:22.138279 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138286 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.138290 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:01:22.138327 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:01:22.138334 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.138342 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138352 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.138357 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138362 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.138368 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:01:22.138372 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.138377 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:01:22.138382 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:01:22.138386 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.138391 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.138396 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.138401 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.138404 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.138408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.138412 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.138417 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.138444 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.138449 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.138454 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.138463 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:01:22.138468 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.138470 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:01:22.138476 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.138485 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.138537 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 16:01:22.138566 2712679360 caffe.cpp:242] Resuming from examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:22.139786 2712679360 sgd_solver.cpp:318] SGDSolver: restoring history
I0523 16:01:22.140019 2712679360 caffe.cpp:248] Starting Optimization
I0523 16:01:22.140027 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 16:01:22.140031 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 16:01:22.140113 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:32.383680 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:32.807214 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:32.807250 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:33.065510 2712679360 solver.cpp:218] Iteration 4000 (366.133 iter/s, 10.925s/100 iters), loss = 0.641508
I0523 16:01:33.065546 2712679360 solver.cpp:237]     Train net output #0: loss = 0.641508 (* 1 = 0.641508 loss)
I0523 16:01:33.065553 2712679360 sgd_solver.cpp:105] Iteration 4000, lr = 0.0001
I0523 16:01:56.950950 2712679360 solver.cpp:218] Iteration 4100 (4.18673 iter/s, 23.885s/100 iters), loss = 0.603556
I0523 16:01:56.951002 2712679360 solver.cpp:237]     Train net output #0: loss = 0.603556 (* 1 = 0.603556 loss)
I0523 16:01:56.951010 2712679360 sgd_solver.cpp:105] Iteration 4100, lr = 0.0001
I0523 16:02:21.127391 2712679360 solver.cpp:218] Iteration 4200 (4.13633 iter/s, 24.176s/100 iters), loss = 0.491505
I0523 16:02:21.127429 2712679360 solver.cpp:237]     Train net output #0: loss = 0.491505 (* 1 = 0.491505 loss)
I0523 16:02:21.127437 2712679360 sgd_solver.cpp:105] Iteration 4200, lr = 0.0001
I0523 16:02:46.283135 2712679360 solver.cpp:218] Iteration 4300 (3.97535 iter/s, 25.155s/100 iters), loss = 0.495313
I0523 16:02:46.283190 2712679360 solver.cpp:237]     Train net output #0: loss = 0.495313 (* 1 = 0.495313 loss)
I0523 16:02:46.283198 2712679360 sgd_solver.cpp:105] Iteration 4300, lr = 0.0001
I0523 16:03:10.841265 2712679360 solver.cpp:218] Iteration 4400 (4.07199 iter/s, 24.558s/100 iters), loss = 0.438567
I0523 16:03:10.841303 2712679360 solver.cpp:237]     Train net output #0: loss = 0.438567 (* 1 = 0.438567 loss)
I0523 16:03:10.841310 2712679360 sgd_solver.cpp:105] Iteration 4400, lr = 0.0001
I0523 16:03:33.942627 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:34.958622 2712679360 solver.cpp:330] Iteration 4500, Testing net (#0)
I0523 16:03:45.910739 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:46.349741 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.752
I0523 16:03:46.349779 2712679360 solver.cpp:397]     Test net output #1: loss = 0.748076 (* 1 = 0.748076 loss)
I0523 16:03:46.589071 2712679360 solver.cpp:218] Iteration 4500 (2.79744 iter/s, 35.747s/100 iters), loss = 0.503921
I0523 16:03:46.589107 2712679360 solver.cpp:237]     Train net output #0: loss = 0.503921 (* 1 = 0.503921 loss)
I0523 16:03:46.589113 2712679360 sgd_solver.cpp:105] Iteration 4500, lr = 0.0001
I0523 16:04:10.851019 2712679360 solver.cpp:218] Iteration 4600 (4.12184 iter/s, 24.261s/100 iters), loss = 0.562534
I0523 16:04:10.851088 2712679360 solver.cpp:237]     Train net output #0: loss = 0.562534 (* 1 = 0.562534 loss)
I0523 16:04:10.851095 2712679360 sgd_solver.cpp:105] Iteration 4600, lr = 0.0001
I0523 16:04:35.547813 2712679360 solver.cpp:218] Iteration 4700 (4.04924 iter/s, 24.696s/100 iters), loss = 0.464102
I0523 16:04:35.547852 2712679360 solver.cpp:237]     Train net output #0: loss = 0.464102 (* 1 = 0.464102 loss)
I0523 16:04:35.547860 2712679360 sgd_solver.cpp:105] Iteration 4700, lr = 0.0001
I0523 16:05:00.517423 2712679360 solver.cpp:218] Iteration 4800 (4.00497 iter/s, 24.969s/100 iters), loss = 0.474584
I0523 16:05:00.517478 2712679360 solver.cpp:237]     Train net output #0: loss = 0.474584 (* 1 = 0.474584 loss)
I0523 16:05:00.517487 2712679360 sgd_solver.cpp:105] Iteration 4800, lr = 0.0001
I0523 16:05:24.429520 2712679360 solver.cpp:218] Iteration 4900 (4.182 iter/s, 23.912s/100 iters), loss = 0.417258
I0523 16:05:24.429554 2712679360 solver.cpp:237]     Train net output #0: loss = 0.417258 (* 1 = 0.417258 loss)
I0523 16:05:24.429563 2712679360 sgd_solver.cpp:105] Iteration 4900, lr = 0.0001
I0523 16:05:47.148733 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:48.086921 2712679360 solver.cpp:457] Snapshotting to HDF5 file examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5
I0523 16:05:48.101351 2712679360 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_quick_iter_5000.solverstate.h5
I0523 16:05:48.215885 2712679360 solver.cpp:310] Iteration 5000, loss = 0.487594
I0523 16:05:48.215921 2712679360 solver.cpp:330] Iteration 5000, Testing net (#0)
I0523 16:05:58.710295 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:59.149840 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.754
I0523 16:05:59.149875 2712679360 solver.cpp:397]     Test net output #1: loss = 0.742307 (* 1 = 0.742307 loss)
I0523 16:05:59.149883 2712679360 solver.cpp:315] Optimization Done.
I0523 16:05:59.149888 2712679360 caffe.cpp:259] Optimization Done.
</code></pre>

<p>训练完毕.并且在最后已经创建好了测试网络.</p>

<p>下面我们用训练好的cifar10模型来对数据进行预测:</p>

<p>运行如下命令:</p>

<pre><code>➜  caffe git:(master) ✗ ./build/tools/caffe.bin test \
-model examples/cifar10/cifar10_quick_train_test.prototxt \
-weights examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5 \
-iterations 100
</code></pre>

<p>对测试数据集进行预测:</p>

<pre><code>I0523 16:25:41.234220 2712679360 caffe.cpp:284] Use CPU.
I0523 16:25:41.238044 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:25:41.238080 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:25:41.238523 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:25:41.238731 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:25:41.238788 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:25:41.238796 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:25:41.238816 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:25:41.238834 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:25:41.238957 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:25:41.246219 2712679360 net.cpp:122] Setting up cifar
I0523 16:25:41.246245 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:25:41.246253 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246258 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:25:41.246266 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:25:41.246278 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:25:41.246282 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:25:41.246343 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:25:41.246367 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:25:41.246381 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:25:41.246390 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246400 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246409 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:25:41.246417 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:25:41.246438 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:25:41.246448 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:25:41.246457 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:25:41.246606 2712679360 net.cpp:122] Setting up conv1
I0523 16:25:41.246637 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:25:41.246680 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:25:41.246693 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:25:41.246708 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:25:41.246721 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:25:41.246731 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:25:41.246752 2712679360 net.cpp:122] Setting up pool1
I0523 16:25:41.246781 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246788 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:25:41.246793 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:25:41.246804 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:25:41.246809 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:25:41.246814 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:25:41.246821 2712679360 net.cpp:122] Setting up relu1
I0523 16:25:41.246825 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246830 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:25:41.246834 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:25:41.246841 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:25:41.246846 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:25:41.246851 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:25:41.247228 2712679360 net.cpp:122] Setting up conv2
I0523 16:25:41.247236 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247242 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:25:41.247249 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:25:41.247259 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:25:41.247264 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:25:41.247269 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:25:41.247274 2712679360 net.cpp:122] Setting up relu2
I0523 16:25:41.247278 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247283 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:25:41.247287 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:25:41.247293 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:25:41.247298 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:25:41.247301 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:25:41.247308 2712679360 net.cpp:122] Setting up pool2
I0523 16:25:41.247313 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:25:41.247318 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:25:41.247321 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:25:41.247329 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:25:41.247334 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:25:41.247339 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:25:41.248001 2712679360 net.cpp:122] Setting up conv3
I0523 16:25:41.248008 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248013 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:25:41.248020 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:25:41.248025 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:25:41.248051 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:25:41.248057 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:25:41.248067 2712679360 net.cpp:122] Setting up relu3
I0523 16:25:41.248072 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248077 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:25:41.248081 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:25:41.248085 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:25:41.248090 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:25:41.248095 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:25:41.248102 2712679360 net.cpp:122] Setting up pool3
I0523 16:25:41.248109 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:25:41.248114 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:25:41.248117 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:25:41.248124 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:25:41.248152 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:25:41.248162 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:25:41.248950 2712679360 net.cpp:122] Setting up ip1
I0523 16:25:41.248993 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:25:41.249008 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:25:41.249014 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:25:41.249020 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:25:41.249024 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:25:41.249038 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:25:41.249080 2712679360 net.cpp:122] Setting up ip2
I0523 16:25:41.249097 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249102 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:25:41.249115 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:25:41.249120 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:25:41.249125 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:25:41.249130 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:25:41.249143 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:25:41.249150 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:25:41.249155 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249164 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249171 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:25:41.249174 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:25:41.249183 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:25:41.249187 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:25:41.249191 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:25:41.249195 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:25:41.249202 2712679360 net.cpp:122] Setting up accuracy
I0523 16:25:41.249205 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249209 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:25:41.249214 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249219 2712679360 net.cpp:84] Creating Layer loss
I0523 16:25:41.249223 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:25:41.249236 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:25:41.249241 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:25:41.249249 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249266 2712679360 net.cpp:122] Setting up loss
I0523 16:25:41.249274 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249279 2712679360 net.cpp:132]     with loss weight 1
I0523 16:25:41.249300 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:25:41.249305 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:25:41.249310 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:25:41.249320 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:25:41.249325 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:25:41.249330 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:25:41.249366 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:25:41.249388 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:25:41.249392 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:25:41.249408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:25:41.249413 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:25:41.249416 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:25:41.249420 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:25:41.249424 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:25:41.249428 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:25:41.249431 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:25:41.249436 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:25:41.249439 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:25:41.249444 2712679360 net.cpp:242] This network produces output loss
I0523 16:25:41.249451 2712679360 net.cpp:255] Network initialization done.
I0523 16:25:41.251152 2712679360 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 16:25:41.252013 2712679360 caffe.cpp:290] Running for 100 iterations.
I0523 16:25:41.367466 2712679360 caffe.cpp:313] Batch 0, accuracy = 0.81
I0523 16:25:41.367501 2712679360 caffe.cpp:313] Batch 0, loss = 0.650321
I0523 16:25:41.465518 2712679360 caffe.cpp:313] Batch 1, accuracy = 0.75
I0523 16:25:41.465550 2712679360 caffe.cpp:313] Batch 1, loss = 0.767328
I0523 16:25:41.560680 2712679360 caffe.cpp:313] Batch 2, accuracy = 0.71
I0523 16:25:41.560712 2712679360 caffe.cpp:313] Batch 2, loss = 0.810281
I0523 16:25:41.656878 2712679360 caffe.cpp:313] Batch 3, accuracy = 0.7
I0523 16:25:41.656913 2712679360 caffe.cpp:313] Batch 3, loss = 0.807916
I0523 16:25:41.757275 2712679360 caffe.cpp:313] Batch 4, accuracy = 0.71
I0523 16:25:41.757313 2712679360 caffe.cpp:313] Batch 4, loss = 0.797028
I0523 16:25:41.855583 2712679360 caffe.cpp:313] Batch 5, accuracy = 0.84
I0523 16:25:41.855613 2712679360 caffe.cpp:313] Batch 5, loss = 0.422262
I0523 16:25:41.953912 2712679360 caffe.cpp:313] Batch 6, accuracy = 0.73
I0523 16:25:41.953946 2712679360 caffe.cpp:313] Batch 6, loss = 0.696204
I0523 16:25:42.052671 2712679360 caffe.cpp:313] Batch 7, accuracy = 0.72
I0523 16:25:42.052705 2712679360 caffe.cpp:313] Batch 7, loss = 0.896313
I0523 16:25:42.155107 2712679360 caffe.cpp:313] Batch 8, accuracy = 0.73
I0523 16:25:42.155153 2712679360 caffe.cpp:313] Batch 8, loss = 0.862504
I0523 16:25:42.258592 2712679360 caffe.cpp:313] Batch 9, accuracy = 0.78
I0523 16:25:42.258627 2712679360 caffe.cpp:313] Batch 9, loss = 0.642714
I0523 16:25:42.362510 2712679360 caffe.cpp:313] Batch 10, accuracy = 0.75
I0523 16:25:42.362543 2712679360 caffe.cpp:313] Batch 10, loss = 0.827924
I0523 16:25:42.463922 2712679360 caffe.cpp:313] Batch 11, accuracy = 0.76
I0523 16:25:42.463953 2712679360 caffe.cpp:313] Batch 11, loss = 0.674977
I0523 16:25:42.567791 2712679360 caffe.cpp:313] Batch 12, accuracy = 0.7
I0523 16:25:42.567822 2712679360 caffe.cpp:313] Batch 12, loss = 0.717463
I0523 16:25:42.664435 2712679360 caffe.cpp:313] Batch 13, accuracy = 0.75
I0523 16:25:42.664469 2712679360 caffe.cpp:313] Batch 13, loss = 0.640668
I0523 16:25:42.759980 2712679360 caffe.cpp:313] Batch 14, accuracy = 0.78
I0523 16:25:42.760013 2712679360 caffe.cpp:313] Batch 14, loss = 0.62553
I0523 16:25:42.856386 2712679360 caffe.cpp:313] Batch 15, accuracy = 0.76
I0523 16:25:42.856417 2712679360 caffe.cpp:313] Batch 15, loss = 0.721462
I0523 16:25:42.954746 2712679360 caffe.cpp:313] Batch 16, accuracy = 0.73
I0523 16:25:42.954777 2712679360 caffe.cpp:313] Batch 16, loss = 0.858499
I0523 16:25:43.053562 2712679360 caffe.cpp:313] Batch 17, accuracy = 0.75
I0523 16:25:43.053593 2712679360 caffe.cpp:313] Batch 17, loss = 0.746772
I0523 16:25:43.155479 2712679360 caffe.cpp:313] Batch 18, accuracy = 0.74
I0523 16:25:43.155508 2712679360 caffe.cpp:313] Batch 18, loss = 0.893995
I0523 16:25:43.254688 2712679360 caffe.cpp:313] Batch 19, accuracy = 0.68
I0523 16:25:43.254716 2712679360 caffe.cpp:313] Batch 19, loss = 0.943102
I0523 16:25:43.364045 2712679360 caffe.cpp:313] Batch 20, accuracy = 0.7
I0523 16:25:43.364076 2712679360 caffe.cpp:313] Batch 20, loss = 0.786499
I0523 16:25:43.465351 2712679360 caffe.cpp:313] Batch 21, accuracy = 0.76
I0523 16:25:43.465384 2712679360 caffe.cpp:313] Batch 21, loss = 0.742349
I0523 16:25:43.560330 2712679360 caffe.cpp:313] Batch 22, accuracy = 0.8
I0523 16:25:43.560362 2712679360 caffe.cpp:313] Batch 22, loss = 0.707087
I0523 16:25:43.662050 2712679360 caffe.cpp:313] Batch 23, accuracy = 0.69
I0523 16:25:43.662077 2712679360 caffe.cpp:313] Batch 23, loss = 0.854361
I0523 16:25:43.760444 2712679360 caffe.cpp:313] Batch 24, accuracy = 0.74
I0523 16:25:43.760473 2712679360 caffe.cpp:313] Batch 24, loss = 0.844035
I0523 16:25:43.858397 2712679360 caffe.cpp:313] Batch 25, accuracy = 0.68
I0523 16:25:43.858425 2712679360 caffe.cpp:313] Batch 25, loss = 1.02302
I0523 16:25:43.959595 2712679360 caffe.cpp:313] Batch 26, accuracy = 0.82
I0523 16:25:43.959627 2712679360 caffe.cpp:313] Batch 26, loss = 0.493385
I0523 16:25:44.057914 2712679360 caffe.cpp:313] Batch 27, accuracy = 0.76
I0523 16:25:44.057942 2712679360 caffe.cpp:313] Batch 27, loss = 0.78877
I0523 16:25:44.157359 2712679360 caffe.cpp:313] Batch 28, accuracy = 0.78
I0523 16:25:44.157388 2712679360 caffe.cpp:313] Batch 28, loss = 0.709657
I0523 16:25:44.285976 2712679360 caffe.cpp:313] Batch 29, accuracy = 0.78
I0523 16:25:44.286007 2712679360 caffe.cpp:313] Batch 29, loss = 0.674438
I0523 16:25:44.390980 2712679360 caffe.cpp:313] Batch 30, accuracy = 0.79
I0523 16:25:44.391010 2712679360 caffe.cpp:313] Batch 30, loss = 0.65947
I0523 16:25:44.491211 2712679360 caffe.cpp:313] Batch 31, accuracy = 0.77
I0523 16:25:44.491241 2712679360 caffe.cpp:313] Batch 31, loss = 0.716022
I0523 16:25:44.593423 2712679360 caffe.cpp:313] Batch 32, accuracy = 0.73
I0523 16:25:44.593457 2712679360 caffe.cpp:313] Batch 32, loss = 0.805526
I0523 16:25:44.692994 2712679360 caffe.cpp:313] Batch 33, accuracy = 0.68
I0523 16:25:44.693023 2712679360 caffe.cpp:313] Batch 33, loss = 0.903316
I0523 16:25:44.795087 2712679360 caffe.cpp:313] Batch 34, accuracy = 0.72
I0523 16:25:44.795116 2712679360 caffe.cpp:313] Batch 34, loss = 0.834438
I0523 16:25:44.897828 2712679360 caffe.cpp:313] Batch 35, accuracy = 0.73
I0523 16:25:44.897874 2712679360 caffe.cpp:313] Batch 35, loss = 0.908751
I0523 16:25:44.996119 2712679360 caffe.cpp:313] Batch 36, accuracy = 0.74
I0523 16:25:44.996150 2712679360 caffe.cpp:313] Batch 36, loss = 0.981981
I0523 16:25:45.093991 2712679360 caffe.cpp:313] Batch 37, accuracy = 0.76
I0523 16:25:45.094023 2712679360 caffe.cpp:313] Batch 37, loss = 0.725703
I0523 16:25:45.195551 2712679360 caffe.cpp:313] Batch 38, accuracy = 0.78
I0523 16:25:45.195585 2712679360 caffe.cpp:313] Batch 38, loss = 0.686703
I0523 16:25:45.292881 2712679360 caffe.cpp:313] Batch 39, accuracy = 0.8
I0523 16:25:45.292912 2712679360 caffe.cpp:313] Batch 39, loss = 0.650689
I0523 16:25:45.397084 2712679360 caffe.cpp:313] Batch 40, accuracy = 0.79
I0523 16:25:45.397115 2712679360 caffe.cpp:313] Batch 40, loss = 0.755663
I0523 16:25:45.495128 2712679360 caffe.cpp:313] Batch 41, accuracy = 0.82
I0523 16:25:45.495160 2712679360 caffe.cpp:313] Batch 41, loss = 0.855221
I0523 16:25:45.597597 2712679360 caffe.cpp:313] Batch 42, accuracy = 0.81
I0523 16:25:45.597626 2712679360 caffe.cpp:313] Batch 42, loss = 0.552907
I0523 16:25:45.695441 2712679360 caffe.cpp:313] Batch 43, accuracy = 0.8
I0523 16:25:45.695472 2712679360 caffe.cpp:313] Batch 43, loss = 0.688889
I0523 16:25:45.796842 2712679360 caffe.cpp:313] Batch 44, accuracy = 0.8
I0523 16:25:45.796875 2712679360 caffe.cpp:313] Batch 44, loss = 0.713613
I0523 16:25:45.899427 2712679360 caffe.cpp:313] Batch 45, accuracy = 0.76
I0523 16:25:45.899462 2712679360 caffe.cpp:313] Batch 45, loss = 0.819739
I0523 16:25:46.003129 2712679360 caffe.cpp:313] Batch 46, accuracy = 0.77
I0523 16:25:46.003190 2712679360 caffe.cpp:313] Batch 46, loss = 0.79499
I0523 16:25:46.101080 2712679360 caffe.cpp:313] Batch 47, accuracy = 0.73
I0523 16:25:46.101112 2712679360 caffe.cpp:313] Batch 47, loss = 0.784097
I0523 16:25:46.199532 2712679360 caffe.cpp:313] Batch 48, accuracy = 0.82
I0523 16:25:46.199563 2712679360 caffe.cpp:313] Batch 48, loss = 0.509592
I0523 16:25:46.296840 2712679360 caffe.cpp:313] Batch 49, accuracy = 0.76
I0523 16:25:46.296872 2712679360 caffe.cpp:313] Batch 49, loss = 0.775396
I0523 16:25:46.399880 2712679360 caffe.cpp:313] Batch 50, accuracy = 0.77
I0523 16:25:46.399914 2712679360 caffe.cpp:313] Batch 50, loss = 0.61452
I0523 16:25:46.500458 2712679360 caffe.cpp:313] Batch 51, accuracy = 0.79
I0523 16:25:46.500488 2712679360 caffe.cpp:313] Batch 51, loss = 0.631971
I0523 16:25:46.599107 2712679360 caffe.cpp:313] Batch 52, accuracy = 0.78
I0523 16:25:46.599139 2712679360 caffe.cpp:313] Batch 52, loss = 0.613152
I0523 16:25:46.699442 2712679360 caffe.cpp:313] Batch 53, accuracy = 0.74
I0523 16:25:46.699475 2712679360 caffe.cpp:313] Batch 53, loss = 0.813763
I0523 16:25:46.802717 2712679360 caffe.cpp:313] Batch 54, accuracy = 0.69
I0523 16:25:46.802749 2712679360 caffe.cpp:313] Batch 54, loss = 0.79753
I0523 16:25:46.903400 2712679360 caffe.cpp:313] Batch 55, accuracy = 0.81
I0523 16:25:46.903430 2712679360 caffe.cpp:313] Batch 55, loss = 0.683275
I0523 16:25:47.007345 2712679360 caffe.cpp:313] Batch 56, accuracy = 0.78
I0523 16:25:47.007377 2712679360 caffe.cpp:313] Batch 56, loss = 0.785579
I0523 16:25:47.107044 2712679360 caffe.cpp:313] Batch 57, accuracy = 0.84
I0523 16:25:47.107076 2712679360 caffe.cpp:313] Batch 57, loss = 0.455638
I0523 16:25:47.204998 2712679360 caffe.cpp:313] Batch 58, accuracy = 0.7
I0523 16:25:47.205029 2712679360 caffe.cpp:313] Batch 58, loss = 0.685973
I0523 16:25:47.307816 2712679360 caffe.cpp:313] Batch 59, accuracy = 0.74
I0523 16:25:47.307848 2712679360 caffe.cpp:313] Batch 59, loss = 0.815847
I0523 16:25:47.409512 2712679360 caffe.cpp:313] Batch 60, accuracy = 0.79
I0523 16:25:47.409544 2712679360 caffe.cpp:313] Batch 60, loss = 0.694609
I0523 16:25:47.509786 2712679360 caffe.cpp:313] Batch 61, accuracy = 0.72
I0523 16:25:47.509819 2712679360 caffe.cpp:313] Batch 61, loss = 0.721049
I0523 16:25:47.608265 2712679360 caffe.cpp:313] Batch 62, accuracy = 0.76
I0523 16:25:47.608304 2712679360 caffe.cpp:313] Batch 62, loss = 0.649006
I0523 16:25:47.711271 2712679360 caffe.cpp:313] Batch 63, accuracy = 0.77
I0523 16:25:47.711302 2712679360 caffe.cpp:313] Batch 63, loss = 0.620039
I0523 16:25:47.812440 2712679360 caffe.cpp:313] Batch 64, accuracy = 0.71
I0523 16:25:47.812471 2712679360 caffe.cpp:313] Batch 64, loss = 0.706689
I0523 16:25:47.911661 2712679360 caffe.cpp:313] Batch 65, accuracy = 0.77
I0523 16:25:47.911694 2712679360 caffe.cpp:313] Batch 65, loss = 0.824431
I0523 16:25:48.011318 2712679360 caffe.cpp:313] Batch 66, accuracy = 0.73
I0523 16:25:48.011351 2712679360 caffe.cpp:313] Batch 66, loss = 0.739382
I0523 16:25:48.117573 2712679360 caffe.cpp:313] Batch 67, accuracy = 0.7
I0523 16:25:48.117606 2712679360 caffe.cpp:313] Batch 67, loss = 0.800725
I0523 16:25:48.214515 2712679360 caffe.cpp:313] Batch 68, accuracy = 0.68
I0523 16:25:48.214545 2712679360 caffe.cpp:313] Batch 68, loss = 0.807705
I0523 16:25:48.314254 2712679360 caffe.cpp:313] Batch 69, accuracy = 0.7
I0523 16:25:48.314283 2712679360 caffe.cpp:313] Batch 69, loss = 0.952385
I0523 16:25:48.412657 2712679360 caffe.cpp:313] Batch 70, accuracy = 0.74
I0523 16:25:48.412686 2712679360 caffe.cpp:313] Batch 70, loss = 0.781932
I0523 16:25:48.512931 2712679360 caffe.cpp:313] Batch 71, accuracy = 0.73
I0523 16:25:48.512964 2712679360 caffe.cpp:313] Batch 71, loss = 0.895561
I0523 16:25:48.608669 2712679360 caffe.cpp:313] Batch 72, accuracy = 0.8
I0523 16:25:48.608700 2712679360 caffe.cpp:313] Batch 72, loss = 0.615967
I0523 16:25:48.705847 2712679360 caffe.cpp:313] Batch 73, accuracy = 0.78
I0523 16:25:48.705878 2712679360 caffe.cpp:313] Batch 73, loss = 0.588951
I0523 16:25:48.803540 2712679360 caffe.cpp:313] Batch 74, accuracy = 0.72
I0523 16:25:48.803591 2712679360 caffe.cpp:313] Batch 74, loss = 0.784208
I0523 16:25:48.906528 2712679360 caffe.cpp:313] Batch 75, accuracy = 0.77
I0523 16:25:48.906565 2712679360 caffe.cpp:313] Batch 75, loss = 0.529825
I0523 16:25:49.007186 2712679360 caffe.cpp:313] Batch 76, accuracy = 0.77
I0523 16:25:49.007216 2712679360 caffe.cpp:313] Batch 76, loss = 0.794115
I0523 16:25:49.107000 2712679360 caffe.cpp:313] Batch 77, accuracy = 0.76
I0523 16:25:49.107033 2712679360 caffe.cpp:313] Batch 77, loss = 0.726804
I0523 16:25:49.205263 2712679360 caffe.cpp:313] Batch 78, accuracy = 0.77
I0523 16:25:49.205294 2712679360 caffe.cpp:313] Batch 78, loss = 0.919712
I0523 16:25:49.304277 2712679360 caffe.cpp:313] Batch 79, accuracy = 0.69
I0523 16:25:49.304309 2712679360 caffe.cpp:313] Batch 79, loss = 0.87618
I0523 16:25:49.404642 2712679360 caffe.cpp:313] Batch 80, accuracy = 0.77
I0523 16:25:49.404672 2712679360 caffe.cpp:313] Batch 80, loss = 0.704637
I0523 16:25:49.501708 2712679360 caffe.cpp:313] Batch 81, accuracy = 0.75
I0523 16:25:49.501739 2712679360 caffe.cpp:313] Batch 81, loss = 0.71787
I0523 16:25:49.599267 2712679360 caffe.cpp:313] Batch 82, accuracy = 0.76
I0523 16:25:49.599304 2712679360 caffe.cpp:313] Batch 82, loss = 0.613339
I0523 16:25:49.698971 2712679360 caffe.cpp:313] Batch 83, accuracy = 0.78
I0523 16:25:49.699002 2712679360 caffe.cpp:313] Batch 83, loss = 0.689216
I0523 16:25:49.803320 2712679360 caffe.cpp:313] Batch 84, accuracy = 0.72
I0523 16:25:49.803352 2712679360 caffe.cpp:313] Batch 84, loss = 0.817351
I0523 16:25:49.904433 2712679360 caffe.cpp:313] Batch 85, accuracy = 0.78
I0523 16:25:49.904467 2712679360 caffe.cpp:313] Batch 85, loss = 0.62069
I0523 16:25:50.005846 2712679360 caffe.cpp:313] Batch 86, accuracy = 0.75
I0523 16:25:50.005878 2712679360 caffe.cpp:313] Batch 86, loss = 0.680651
I0523 16:25:50.103121 2712679360 caffe.cpp:313] Batch 87, accuracy = 0.78
I0523 16:25:50.103153 2712679360 caffe.cpp:313] Batch 87, loss = 0.788875
I0523 16:25:50.200103 2712679360 caffe.cpp:313] Batch 88, accuracy = 0.8
I0523 16:25:50.200134 2712679360 caffe.cpp:313] Batch 88, loss = 0.620548
I0523 16:25:50.299957 2712679360 caffe.cpp:313] Batch 89, accuracy = 0.74
I0523 16:25:50.299989 2712679360 caffe.cpp:313] Batch 89, loss = 0.779962
I0523 16:25:50.399699 2712679360 caffe.cpp:313] Batch 90, accuracy = 0.75
I0523 16:25:50.399731 2712679360 caffe.cpp:313] Batch 90, loss = 0.70084
I0523 16:25:50.502117 2712679360 caffe.cpp:313] Batch 91, accuracy = 0.79
I0523 16:25:50.502148 2712679360 caffe.cpp:313] Batch 91, loss = 0.576651
I0523 16:25:50.599150 2712679360 caffe.cpp:313] Batch 92, accuracy = 0.71
I0523 16:25:50.599181 2712679360 caffe.cpp:313] Batch 92, loss = 0.9778
I0523 16:25:50.699782 2712679360 caffe.cpp:313] Batch 93, accuracy = 0.78
I0523 16:25:50.699813 2712679360 caffe.cpp:313] Batch 93, loss = 0.795732
I0523 16:25:50.802847 2712679360 caffe.cpp:313] Batch 94, accuracy = 0.77
I0523 16:25:50.802877 2712679360 caffe.cpp:313] Batch 94, loss = 0.803904
I0523 16:25:50.900668 2712679360 caffe.cpp:313] Batch 95, accuracy = 0.77
I0523 16:25:50.900702 2712679360 caffe.cpp:313] Batch 95, loss = 0.664654
I0523 16:25:50.902439 102174720 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:25:50.999625 2712679360 caffe.cpp:313] Batch 96, accuracy = 0.74
I0523 16:25:50.999656 2712679360 caffe.cpp:313] Batch 96, loss = 0.700099
I0523 16:25:51.100697 2712679360 caffe.cpp:313] Batch 97, accuracy = 0.66
I0523 16:25:51.100728 2712679360 caffe.cpp:313] Batch 97, loss = 0.937044
I0523 16:25:51.201591 2712679360 caffe.cpp:313] Batch 98, accuracy = 0.79
I0523 16:25:51.201622 2712679360 caffe.cpp:313] Batch 98, loss = 0.677679
I0523 16:25:51.299702 2712679360 caffe.cpp:313] Batch 99, accuracy = 0.76
I0523 16:25:51.299736 2712679360 caffe.cpp:313] Batch 99, loss = 0.687144
I0523 16:25:51.299741 2712679360 caffe.cpp:318] Loss: 0.742307
I0523 16:25:51.299762 2712679360 caffe.cpp:330] accuracy = 0.754
I0523 16:25:51.299773 2712679360 caffe.cpp:330] loss = 0.742307 (* 1 = 0.742307 loss)
</code></pre>

<p>得到最终的测试集准确率可以到达<code>accuracy = 0.754</code></p>

<p>到这里我们对于练习 cifar10模型<br/>
就结束了.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/23</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14946429015704.html">
                
                  <h1>caffe框架运行手写体数字识别例子（MNIST）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>关于caffe环境的搭建暂时不做讨论，之后有时间整理一下（mac系统上cpu-only的caffe环境搭建）。</p>

<h2 id="toc_0">LeNet-5模型描述</h2>

<p>caffe框架中给的LeNet-5模型与原版有所不同，其中将Sigmoid激活函数换成了ReLu，它的描述文件为<code>examples/mnist/lenet_train_test.prototxt</code><br/>
，它的内容主要为：</p>

<pre><code>name: &quot;LeNet&quot;           //网络（Net）的名称为LeNet
layer {                 //定义一个层（Layer）
  name: &quot;mnist&quot;         //层的名称为mnist
  type: &quot;Data&quot;          //层的类型为数据层
  top: &quot;data&quot;           //层的输出blob有两个：data和label
  top: &quot;label&quot;
  include {
    phase: TRAIN        //表明该层参数只在训练阶段有效
  }
  transform_param {
    scale: 0.00390625   //数据变换使用的数据缩放因子
  }
  data_param {          //数据层参数
    source: &quot;examples/mnist/mnist_train_lmdb&quot;       //LMDB的路径
    batch_size: 64      //批量数目，一次性读取64张图
    backend: LMDB       //数据格式为LMDB
  }
}
layer {                 //一个新的数据层，名字也叫mnist，输出blob也是data和Label，但是这里定义的参数只在分类阶段有效
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST         //表明只在测试分类阶段有效
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;         //定义一个新的卷积层conv1，输入blob为data，输出blob为conv1
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1          //权值学习速率倍乘因子，1蓓表示与全局参数一致
  }
  param {
    lr_mult: 2          //bias学习速率倍乘因子，是全局参数的两倍
  }
  convolution_param {   //卷积计算参数
    num_output: 20      //输出feature map数目为20
    kernel_size: 5      //卷积核尺寸，5*5
    stride: 1           //卷积输出跳跃间隔，1表示连续输出，无跳跃
    weight_filler {     //权值使用Xavier填充器
      type: &quot;xavier&quot;
    }
    bias_filler {       //bias使用常熟填充器，默认为0
      type: &quot;constant&quot;
    }
  }
}
layer {                 //定义新的下采样层pool1，输入blob为conv1，输出blob为pool1
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {       //下采样参数
    pool: MAX           //使用最大值下采样方法
    kernel_size: 2      //下采样窗口尺寸为2*2
    stride: 2           //下采样输出跳跃间隔2*2
  }
}   
layer {                 //新的卷积层，和conv1类似
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //新的下采样层，和pool1类似
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {                 //新的全连接层，输入blob为pool2，输出blob为ip1
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {     //全连接层参数
    num_output: 500         //该层输出元素个数为500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //新的非线性层，用ReLU方法
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //分类准确率层，输入blob为ip2和Label，输出blob为accuracy，该层用于计算分类准确率
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {             //损失层，损失函数SoftmaxLoss，输入blob为ip2和label，输出blob为loss
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}

</code></pre>

<p>LeNet模型原图为：<br/>
<img src="media/14946429015704/14946445265310.jpg" alt=""/></p>

<p><img src="media/14946429015704/14946445430850.jpg" alt=""/></p>

<h2 id="toc_1">训练超参数：</h2>

<p>上面已近给出了LeNet模型中的网络结构图和一些参数定义，下面我们正式来训练，这是一个分类准确率可以达到99%以上的模型。</p>

<p>首先进入caffe所在目录：<br/>
执行：<code>examples/mnist/train_lenet.sh</code></p>

<p><code>train_lenet.sh</code>的代码为：</p>

<pre><code>#!/usr/bin/env sh
set -e          #暂时不知道具体作用

./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt $@

</code></pre>

<p>这里调用了之前编译好的<code>build/tools/caffe.bin</code>二进制文件，参数为：--<code>solver=examples/mnist/lenet_solver.prototxt $@</code>指定了训练超参数文件，内容如下：</p>

<pre><code class="language-py"># The train/test net protocol buffer definition
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.（训练时每迭代500次，进行一次预测）
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.（网络的基础学习速率，冲量和衰减量）
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy（学习速率的衰减策略）
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterations（每100次迭代，在屏幕上打印一次运行log）
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results（每5000次迭代打印一次快照）
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
# solver mode: CPU or GPU（求解模式为CPU模式，因为mac没有N卡）
solver_mode: CPU
</code></pre>

<h2 id="toc_2">训练日志</h2>

<p>执行上面<code>examples/mnist/train_lenet.sh</code>文件后会产生如下的日志输出：</p>

<pre><code>//使用cpu模式运行
I0513 11:18:42.330993 3659862976 caffe.cpp:211] Use CPU.
I0513 11:18:42.331964 3659862976 solver.cpp:44] Initializing solver from parameters:
//打印训练超参数文件lenet_solver.prototxt中经过解析的内容
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
solver_mode: CPU
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0513 11:18:42.332221 3659862976 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
//解析CNN网络描述文件中的网络参数，创建训练网络
I0513 11:18:42.332438 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0513 11:18:42.332453 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0513 11:18:42.332459 3659862976 net.cpp:51] Initializing net from parameters:
//打印训练网路参数描述
name: &quot;LeNet&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_train_lmdb&quot;
    batch_size: 64
    backend: LMDB
  }
}
//........中间省略
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:18:42.332698 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.332906 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0513 11:18:42.332963 3659862976 net.cpp:84] Creating Layer mnist
//产生两个输出，data为图片数据，label为标签数据
I0513 11:18:42.332970 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.332989 3659862976 net.cpp:380] mnist -&gt; label
//之后打开了训练集LMDB，data为四维数组，又称blob，尺寸为64,1,28,28
I0513 11:18:42.333026 3659862976 data_layer.cpp:45] output data size: 64,1,28,28
I0513 11:18:42.337728 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.337738 3659862976 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0513 11:18:42.337759 3659862976 net.cpp:129] Top shape: 64 (64)
//统计占用内存，会逐层累加
I0513 11:18:42.337762 3659862976 net.cpp:137] Memory required for data: 200960
//盖一楼，conv1
I0513 11:18:42.337769 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.337776 3659862976 net.cpp:84] Creating Layer conv1
//conv1需要一个输入data（来自上一层mnist），产生一个输出conv1(送入下一层)
I0513 11:18:42.337780 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.337785 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.337836 3659862976 net.cpp:122] Setting up conv1
//conv1的输出尺寸为（64,20,24,24）
I0513 11:18:42.337842 3659862976 net.cpp:129] Top shape: 64 20 24 24 (737280)
//统计内存逐层累加
I0513 11:18:42.337847 3659862976 net.cpp:137] Memory required for data: 3150080
I0513 11:18:42.337853 3659862976 layer_factory.hpp:77] Creating layer pool1
//中间层创建类似
I0513 11:18:42.337877 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.337882 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.337887 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.337895 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.337899 3659862976 net.cpp:129] Top shape: 64 20 12 12 (184320)
I0513 11:18:42.337904 3659862976 net.cpp:137] Memory required for data: 3887360
I0513 11:18:42.337908 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.337913 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.337916 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.337921 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.338141 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.338146 3659862976 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0513 11:18:42.338162 3659862976 net.cpp:137] Memory required for data: 4706560
I0513 11:18:42.338167 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.338174 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.338178 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.338182 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.338210 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.338215 3659862976 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0513 11:18:42.338220 3659862976 net.cpp:137] Memory required for data: 4911360
I0513 11:18:42.338224 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.338232 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.338235 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.338240 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.341404 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.341413 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341418 3659862976 net.cpp:137] Memory required for data: 5039360
I0513 11:18:42.341424 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.341433 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.341435 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.341440 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.341444 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.341449 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341451 3659862976 net.cpp:137] Memory required for data: 5167360
I0513 11:18:42.341455 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.341470 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.341473 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.341478 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.341531 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.341536 3659862976 net.cpp:129] Top shape: 64 10 (640)
I0513 11:18:42.341539 3659862976 net.cpp:137] Memory required for data: 5169920
//盖最后一层loss
I0513 11:18:42.341544 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341550 3659862976 net.cpp:84] Creating Layer loss
//该层需要两个输入ip2和label，产生一个输出loss
I0513 11:18:42.341554 3659862976 net.cpp:406] loss &lt;- ip2
I0513 11:18:42.341557 3659862976 net.cpp:406] loss &lt;- label
I0513 11:18:42.341563 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.341572 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341583 3659862976 net.cpp:122] Setting up loss
//输出loss尺寸为1，loss weight参数为1
I0513 11:18:42.341586 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.341590 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.341598 3659862976 net.cpp:137] Memory required for data: 5169924
//从后往前统计哪些层需要做反向传播计算（BP）
I0513 11:18:42.341601 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.341606 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.341609 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.341614 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.341616 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.341620 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.341624 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.341627 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.341631 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.341655 3659862976 net.cpp:242] This network produces output loss
//盖楼完毕
I0513 11:18:42.341662 3659862976 net.cpp:255] Network initialization done.
//还需要创建测试网络，在盖一次楼
I0513 11:18:42.341949 3659862976 solver.cpp:172] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0513 11:18:42.341986 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:18:42.341996 3659862976 net.cpp:51] Initializing net 
from parameters:
//类似于第一座楼的情况，只是地基mnist改了一下lmdb源和输出尺寸，顶楼加了一个accuracy阁楼
name: &quot;LeNet&quot;
state {
  phase: TEST
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}

//....中间重复，不表
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
//具体盖楼过程与训练网络类似
I0513 11:18:42.342216 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.342300 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:18:42.342319 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:18:42.342329 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.342335 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:18:42.342345 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:18:42.343029 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.343037 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:18:42.343057 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343061 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:18:42.343065 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:18:42.343073 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:18:42.343077 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:18:42.343082 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:18:42.343087 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:18:42.343093 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:18:42.343097 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343101 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343106 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:18:42.343109 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.343137 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:18:42.343144 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.343152 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.343175 3659862976 net.cpp:122] Setting up conv1
I0513 11:18:42.343181 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:18:42.343186 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:18:42.343196 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:18:42.343206 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.343214 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.343219 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.343228 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.343232 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:18:42.343236 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:18:42.343240 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.343245 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.343250 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.343253 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.343482 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.343488 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:18:42.343503 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:18:42.343509 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.343513 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.343518 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.343521 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.343526 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.343530 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:18:42.343534 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:18:42.343538 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.343564 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.343569 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.343575 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.346873 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.346884 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346889 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:18:42.346895 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.346901 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.346905 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.346909 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.346915 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.346917 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346921 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:18:42.346925 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.346931 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.346935 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.346938 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.346987 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.346992 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.346997 3659862976 net.cpp:137] Memory required for data: 8078800
//注意这里，ip2_ip2_0_split在网络描述中没有显示给出，是caffe解析后自动加上的
I0513 11:18:42.347002 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:18:42.347007 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
//ip2_ip2_0_split接受一个输入ip2，产生两个输出ip2_ip2_0_split_0和ip2_ip2_0_split_1，是复制关系
I0513 11:18:42.347010 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:18:42.347014 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:18:42.347019 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:18:42.347024 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:18:42.347028 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347033 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347036 3659862976 net.cpp:137] Memory required for data: 8086800
//ip2_ip2_0_split_0给了accuracy层
I0513 11:18:42.347039 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:18:42.347069 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:18:42.347074 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:18:42.347077 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:18:42.347082 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:18:42.347088 3659862976 net.cpp:122] Setting up accuracy
//accuracy层输出尺寸为1，即分类准确率
I0513 11:18:42.347091 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347095 3659862976 net.cpp:137] Memory required for data: 8086804
//ip2_ip2_0_split_1给了loss层
I0513 11:18:42.347100 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347103 3659862976 net.cpp:84] Creating Layer loss
I0513 11:18:42.347107 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:18:42.347111 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:18:42.347115 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.347121 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347131 3659862976 net.cpp:122] Setting up loss
I0513 11:18:42.347133 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347137 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.347143 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:18:42.347147 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.347151 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:18:42.347156 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:18:42.347159 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.347162 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.347167 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.347169 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.347173 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.347177 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.347180 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.347184 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:18:42.347189 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.347193 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:18:42.347196 3659862976 net.cpp:242] This network produces output loss
//第二座楼盖好了
I0513 11:18:42.347203 3659862976 net.cpp:255] Network initialization done.
//装修方案确定了
I0513 11:18:42.347247 3659862976 solver.cpp:56] Solver scaffolding done.
//开始装修
I0513 11:18:42.347271 3659862976 caffe.cpp:248] Starting Optimization
I0513 11:18:42.347275 3659862976 solver.cpp:272] Solving LeNet
I0513 11:18:42.347278 3659862976 solver.cpp:273] Learning Rate Policy: inv
//先测试一次，得到出事分类准确率和损失
I0513 11:18:42.348048 3659862976 solver.cpp:330] Iteration 0, Testing net (#0)
I0513 11:18:44.611253 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:18:44.703907 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.077
I0513 11:18:44.703938 3659862976 solver.cpp:397]     Test net output #1: loss = 2.41516 (* 1 = 2.41516 loss)
//现在分类效果肯定很差，准确率只有0.077，损失值约为2.3
I0513 11:18:44.741230 3659862976 solver.cpp:218] Iteration 0 (0 iter/s, 2.393s/100 iters), loss = 2.42047
//0次迭代后，依旧很差，训练网络没有accuracy输出，只有loss输出
I0513 11:18:44.741261 3659862976 solver.cpp:237]     Train net output #0: loss = 2.42047 (* 1 = 2.42047 loss)
I0513 11:18:44.741287 3659862976 sgd_solver.cpp:105] Iteration 0, lr = 0.01
//迭代100次之后，效果就出来了，loss已经降到0.21（之前是2.42）
I0513 11:18:47.874459 3659862976 solver.cpp:218] Iteration 100 (31.9183 iter/s, 3.133s/100 iters), loss = 0.215375
I0513 11:18:47.874493 3659862976 solver.cpp:237]     Train net output #0: loss = 0.215375 (* 1 = 0.215375 loss)
I0513 11:18:47.874500 3659862976 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0513 11:18:50.998973 3659862976 solver.cpp:218] Iteration 200 (32.0102 iter/s, 3.124s/100 iters), loss = 0.144389
I0513 11:18:50.999003 3659862976 solver.cpp:237]     Train net output #0: loss = 0.144389 (* 1 = 0.144389 loss)
I0513 11:18:50.999011 3659862976 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0513 11:18:54.100409 3659862976 solver.cpp:218] Iteration 300 (32.2477 iter/s, 3.101s/100 iters), loss = 0.192488
I0513 11:18:54.100476 3659862976 solver.cpp:237]     Train net output #0: loss = 0.192488 (* 1 = 0.192488 loss)
I0513 11:18:54.100483 3659862976 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0513 11:18:57.210686 3659862976 solver.cpp:218] Iteration 400 (32.1543 iter/s, 3.11s/100 iters), loss = 0.0663644
I0513 11:18:57.210728 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0663644 (* 1 = 0.0663644 loss)
I0513 11:18:57.210737 3659862976 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
//迭代500次之后，进行一次测试。
I0513 11:19:00.279249 3659862976 solver.cpp:330] Iteration 500, Testing net (#0)
I0513 11:19:02.608597 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//发现准确度accuracy已经显著提升到0.9744了，loss为0.08
I0513 11:19:02.703658 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9744
I0513 11:19:02.703694 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0836155 (* 1 = 0.0836155 loss)
I0513 11:19:02.735476 3659862976 solver.cpp:218] Iteration 500 (18.1028 iter/s, 5.524s/100 iters), loss = 0.0916289
I0513 11:19:02.735512 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0916288 (* 1 = 0.0916288 loss)
I0513 11:19:02.735520 3659862976 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0513 11:19:05.931562 3659862976 solver.cpp:218] Iteration 600 (31.2891 iter/s, 3.196s/100 iters), loss = 0.0844364
I0513 11:19:05.931597 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0844363 (* 1 = 0.0844363 loss)
I0513 11:19:05.931604 3659862976 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0513 11:19:09.116649 3659862976 solver.cpp:218] Iteration 700 (31.3972 iter/s, 3.185s/100 iters), loss = 0.134004
I0513 11:19:09.116684 3659862976 solver.cpp:237]     Train net output #0: loss = 0.134004 (* 1 = 0.134004 loss)
I0513 11:19:09.116691 3659862976 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
//中间是训练过程。。。。。。
I0513 11:22:17.536756 3659862976 solver.cpp:218] Iteration 4800 (19.3311 iter/s, 5.173s/100 iters), loss = 0.0179583
I0513 11:22:17.536806 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0179581 (* 1 = 0.0179581 loss)
I0513 11:22:17.536818 3659862976 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0513 11:22:22.731861 3659862976 solver.cpp:218] Iteration 4900 (19.2493 iter/s, 5.195s/100 iters), loss = 0.00556874
I0513 11:22:22.731927 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00556857 (* 1 = 0.00556857 loss)
I0513 11:22:22.731940 3659862976 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
//每迭代到5000次之后，打印一次快照，保存lenet_iter_5000.caffemodel和lenet_iter_5000.solverstate
I0513 11:22:28.143353 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0513 11:22:28.167670 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0513 11:22:28.171842 3659862976 solver.cpp:330] Iteration 5000, Testing net (#0)
I0513 11:22:32.514833 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:22:32.699314 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9888
I0513 11:22:32.699359 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0334435 (* 1 = 0.0334435 loss)
I0513 11:22:32.754936 3659862976 solver.cpp:218] Iteration 5000 (9.97705 iter/s, 10.023s/100 iters), loss = 0.0241056
I0513 11:22:32.754987 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0241055 (* 1 = 0.0241055 loss)
I0513 11:22:32.754999 3659862976 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
//中间继续训练。。。。。
I0513 11:26:53.808578 3659862976 solver.cpp:218] Iteration 9900 (21.097 iter/s, 4.74s/100 iters), loss = 0.00466773
I0513 11:26:53.808624 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00466757 (* 1 = 0.00466757 loss)
I0513 11:26:53.808635 3659862976 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
//最后一次打印快照
I0513 11:26:58.671659 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0513 11:26:58.688323 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0513 11:26:58.715297 3659862976 solver.cpp:310] Iteration 10000, loss = 0.00293942
I0513 11:26:58.715337 3659862976 solver.cpp:330] Iteration 10000, Testing net (#0)
I0513 11:27:02.099313 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//最终分类准确率为99%
I0513 11:27:02.230465 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.991
//最终loss值为0.03
I0513 11:27:02.230509 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0304018 (* 1 = 0.0304018 loss)
I0513 11:27:02.230518 3659862976 solver.cpp:315] Optimization Done.
I0513 11:27:02.230525 3659862976 caffe.cpp:259] Optimization Done.
//装修结束
</code></pre>

<h2 id="toc_3">用训练好的模型对数据进行预测</h2>

<p>从上面的输出结果可以看到最终训练的模型权值存在lenet_iter_10000.caffemodal中，之后可以对测试数据集进行预测。运行如下命令就可以了：</p>

<pre><code>➜  caffe git:(master) ✗ ./build/tools/caffe.bin test \
-model examples/mnist/lenet_train_test.prototxt \
-weights examples/mnist/lenet_iter_10000.caffemodel \
    -iterations 100
</code></pre>

<p>上述命令解释：<br/>
./build/tools/caffe.bin test，表示只做预测（前向传播急速那），不进行参数更新（BP反向传播计算）</p>

<p>-model examples/mnist/lenet_train_test.prototxt ，指定模型描述文本文件</p>

<p>-weights examples/mnist/lenet_iter_10000.caffemodel ，指定模型预先训练好的权值文件<br/>
-iterations 100 ， 指定测试迭代次数。参与测试的样例数目为（iterations*batch_size）,batch_size在model prototxt中设定，为100时刚好覆盖全部10000个测试样本。</p>

<p>我们运行上述命令得到：</p>

<pre><code>I0513 11:37:08.827889 3659862976 caffe.cpp:284] Use CPU.
I0513 11:37:08.830747 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:37:08.830780 3659862976 net.cpp:51] Initializing net from parameters:
name: &quot;LeNet&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:37:08.831130 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:37:08.831360 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:37:08.831418 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:37:08.831425 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:37:08.831444 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:37:08.831480 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:37:08.836457 3659862976 net.cpp:122] Setting up mnist
I0513 11:37:08.836468 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:37:08.836488 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836491 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:37:08.836498 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:37:08.836505 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:37:08.836509 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:37:08.836513 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:37:08.836519 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:37:08.836525 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:37:08.836529 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836534 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836539 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:37:08.836542 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:37:08.836550 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:37:08.836555 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:37:08.836558 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:37:08.836611 3659862976 net.cpp:122] Setting up conv1
I0513 11:37:08.836616 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:37:08.836639 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:37:08.836648 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:37:08.836653 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:37:08.836658 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:37:08.836661 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:37:08.836671 3659862976 net.cpp:122] Setting up pool1
I0513 11:37:08.836675 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:37:08.836680 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:37:08.836683 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:37:08.836691 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:37:08.836695 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:37:08.836700 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:37:08.836917 3659862976 net.cpp:122] Setting up conv2
I0513 11:37:08.836923 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:37:08.836971 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:37:08.837033 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:37:08.837041 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:37:08.837045 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:37:08.837049 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:37:08.837059 3659862976 net.cpp:122] Setting up pool2
I0513 11:37:08.837062 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:37:08.837067 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:37:08.837070 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:37:08.837076 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:37:08.837080 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:37:08.837085 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:37:08.840445 3659862976 net.cpp:122] Setting up ip1
I0513 11:37:08.840461 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840467 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:37:08.840476 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:37:08.840487 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:37:08.840492 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:37:08.840497 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:37:08.840504 3659862976 net.cpp:122] Setting up relu1
I0513 11:37:08.840507 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840512 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:37:08.840517 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:37:08.840523 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:37:08.840528 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:37:08.840533 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:37:08.840591 3659862976 net.cpp:122] Setting up ip2
I0513 11:37:08.840597 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840601 3659862976 net.cpp:137] Memory required for data: 8078800
I0513 11:37:08.840606 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:37:08.840612 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
I0513 11:37:08.840616 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:37:08.840623 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:37:08.840631 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:37:08.840637 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:37:08.840641 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840646 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840649 3659862976 net.cpp:137] Memory required for data: 8086800
I0513 11:37:08.840653 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:37:08.840659 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:37:08.840663 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:37:08.840668 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:37:08.840672 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:37:08.840678 3659862976 net.cpp:122] Setting up accuracy
I0513 11:37:08.840708 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840714 3659862976 net.cpp:137] Memory required for data: 8086804
I0513 11:37:08.840718 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840724 3659862976 net.cpp:84] Creating Layer loss
I0513 11:37:08.840728 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:37:08.840733 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:37:08.840737 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:37:08.840746 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840759 3659862976 net.cpp:122] Setting up loss
I0513 11:37:08.840762 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840767 3659862976 net.cpp:132]     with loss weight 1
I0513 11:37:08.840776 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:37:08.840780 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:37:08.840785 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:37:08.840790 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:37:08.840793 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:37:08.840798 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:37:08.840802 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:37:08.840806 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:37:08.840811 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:37:08.840814 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:37:08.840818 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:37:08.840822 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:37:08.840827 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:37:08.840831 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:37:08.840836 3659862976 net.cpp:242] This network produces output loss
I0513 11:37:08.840843 3659862976 net.cpp:255] Network initialization done.
I0513 11:37:08.843325 3659862976 caffe.cpp:290] Running for 100 iterations.
I0513 11:37:08.871536 3659862976 caffe.cpp:313] Batch 0, accuracy = 1
I0513 11:37:08.871567 3659862976 caffe.cpp:313] Batch 0, loss = 0.0085843
I0513 11:37:08.894382 3659862976 caffe.cpp:313] Batch 1, accuracy = 1
I0513 11:37:08.894414 3659862976 caffe.cpp:313] Batch 1, loss = 0.00573037
I0513 11:37:08.918002 3659862976 caffe.cpp:313] Batch 2, accuracy = 0.99
I0513 11:37:08.918031 3659862976 caffe.cpp:313] Batch 2, loss = 0.0333053
I0513 11:37:08.943091 3659862976 caffe.cpp:313] Batch 3, accuracy = 0.99
I0513 11:37:08.943127 3659862976 caffe.cpp:313] Batch 3, loss = 0.0271862
I0513 11:37:08.967147 3659862976 caffe.cpp:313] Batch 4, accuracy = 0.99
I0513 11:37:08.967177 3659862976 caffe.cpp:313] Batch 4, loss = 0.0571239
I0513 11:37:08.989929 3659862976 caffe.cpp:313] Batch 5, accuracy = 0.99
I0513 11:37:08.989961 3659862976 caffe.cpp:313] Batch 5, loss = 0.0569953
I0513 11:37:09.015426 3659862976 caffe.cpp:313] Batch 6, accuracy = 0.98
I0513 11:37:09.015463 3659862976 caffe.cpp:313] Batch 6, loss = 0.0698283
I0513 11:37:09.039398 3659862976 caffe.cpp:313] Batch 7, accuracy = 0.99
I0513 11:37:09.039432 3659862976 caffe.cpp:313] Batch 7, loss = 0.0349087
I0513 11:37:09.063937 3659862976 caffe.cpp:313] Batch 8, accuracy = 1
I0513 11:37:09.063967 3659862976 caffe.cpp:313] Batch 8, loss = 0.0115442
I0513 11:37:09.086630 3659862976 caffe.cpp:313] Batch 9, accuracy = 0.99
I0513 11:37:09.086663 3659862976 caffe.cpp:313] Batch 9, loss = 0.0361095
I0513 11:37:09.111706 3659862976 caffe.cpp:313] Batch 10, accuracy = 0.98
I0513 11:37:09.111735 3659862976 caffe.cpp:313] Batch 10, loss = 0.0702643
I0513 11:37:09.135445 3659862976 caffe.cpp:313] Batch 11, accuracy = 0.97
I0513 11:37:09.135478 3659862976 caffe.cpp:313] Batch 11, loss = 0.0508112
I0513 11:37:09.159065 3659862976 caffe.cpp:313] Batch 12, accuracy = 0.95
I0513 11:37:09.159097 3659862976 caffe.cpp:313] Batch 12, loss = 0.148118
I0513 11:37:09.181542 3659862976 caffe.cpp:313] Batch 13, accuracy = 0.98
I0513 11:37:09.181607 3659862976 caffe.cpp:313] Batch 13, loss = 0.036772
I0513 11:37:09.205440 3659862976 caffe.cpp:313] Batch 14, accuracy = 1
I0513 11:37:09.205476 3659862976 caffe.cpp:313] Batch 14, loss = 0.00694412
I0513 11:37:09.228198 3659862976 caffe.cpp:313] Batch 15, accuracy = 0.99
I0513 11:37:09.228229 3659862976 caffe.cpp:313] Batch 15, loss = 0.0389514
I0513 11:37:09.251550 3659862976 caffe.cpp:313] Batch 16, accuracy = 0.98
I0513 11:37:09.251581 3659862976 caffe.cpp:313] Batch 16, loss = 0.0298825
I0513 11:37:09.275153 3659862976 caffe.cpp:313] Batch 17, accuracy = 1
I0513 11:37:09.275182 3659862976 caffe.cpp:313] Batch 17, loss = 0.0170967
I0513 11:37:09.298004 3659862976 caffe.cpp:313] Batch 18, accuracy = 0.99
I0513 11:37:09.298035 3659862976 caffe.cpp:313] Batch 18, loss = 0.0189575
I0513 11:37:09.321348 3659862976 caffe.cpp:313] Batch 19, accuracy = 0.99
I0513 11:37:09.321379 3659862976 caffe.cpp:313] Batch 19, loss = 0.0455956
I0513 11:37:09.344025 3659862976 caffe.cpp:313] Batch 20, accuracy = 0.98
I0513 11:37:09.344058 3659862976 caffe.cpp:313] Batch 20, loss = 0.108723
I0513 11:37:09.368069 3659862976 caffe.cpp:313] Batch 21, accuracy = 0.98
I0513 11:37:09.368101 3659862976 caffe.cpp:313] Batch 21, loss = 0.0780955
I0513 11:37:09.390791 3659862976 caffe.cpp:313] Batch 22, accuracy = 0.99
I0513 11:37:09.390823 3659862976 caffe.cpp:313] Batch 22, loss = 0.0368689
I0513 11:37:09.414577 3659862976 caffe.cpp:313] Batch 23, accuracy = 0.97
I0513 11:37:09.414621 3659862976 caffe.cpp:313] Batch 23, loss = 0.0296016
I0513 11:37:09.437597 3659862976 caffe.cpp:313] Batch 24, accuracy = 0.97
I0513 11:37:09.437628 3659862976 caffe.cpp:313] Batch 24, loss = 0.0589915
I0513 11:37:09.460636 3659862976 caffe.cpp:313] Batch 25, accuracy = 0.99
I0513 11:37:09.460669 3659862976 caffe.cpp:313] Batch 25, loss = 0.0754509
I0513 11:37:09.483229 3659862976 caffe.cpp:313] Batch 26, accuracy = 0.99
I0513 11:37:09.483261 3659862976 caffe.cpp:313] Batch 26, loss = 0.118656
I0513 11:37:09.508059 3659862976 caffe.cpp:313] Batch 27, accuracy = 0.98
I0513 11:37:09.508092 3659862976 caffe.cpp:313] Batch 27, loss = 0.0222734
I0513 11:37:09.530911 3659862976 caffe.cpp:313] Batch 28, accuracy = 0.99
I0513 11:37:09.530943 3659862976 caffe.cpp:313] Batch 28, loss = 0.0315118
I0513 11:37:09.555687 3659862976 caffe.cpp:313] Batch 29, accuracy = 0.97
I0513 11:37:09.555721 3659862976 caffe.cpp:313] Batch 29, loss = 0.129427
I0513 11:37:09.579476 3659862976 caffe.cpp:313] Batch 30, accuracy = 1
I0513 11:37:09.579507 3659862976 caffe.cpp:313] Batch 30, loss = 0.0196561
I0513 11:37:09.602957 3659862976 caffe.cpp:313] Batch 31, accuracy = 1
I0513 11:37:09.602993 3659862976 caffe.cpp:313] Batch 31, loss = 0.00242798
I0513 11:37:09.626893 3659862976 caffe.cpp:313] Batch 32, accuracy = 0.99
I0513 11:37:09.626924 3659862976 caffe.cpp:313] Batch 32, loss = 0.0169622
I0513 11:37:09.650236 3659862976 caffe.cpp:313] Batch 33, accuracy = 1
I0513 11:37:09.650270 3659862976 caffe.cpp:313] Batch 33, loss = 0.00425847
I0513 11:37:09.673212 3659862976 caffe.cpp:313] Batch 34, accuracy = 0.99
I0513 11:37:09.673243 3659862976 caffe.cpp:313] Batch 34, loss = 0.0726783
I0513 11:37:09.696039 3659862976 caffe.cpp:313] Batch 35, accuracy = 0.95
I0513 11:37:09.696071 3659862976 caffe.cpp:313] Batch 35, loss = 0.173234
I0513 11:37:09.719209 3659862976 caffe.cpp:313] Batch 36, accuracy = 1
I0513 11:37:09.719241 3659862976 caffe.cpp:313] Batch 36, loss = 0.0126433
I0513 11:37:09.741852 3659862976 caffe.cpp:313] Batch 37, accuracy = 0.99
I0513 11:37:09.741884 3659862976 caffe.cpp:313] Batch 37, loss = 0.0380185
I0513 11:37:09.766039 3659862976 caffe.cpp:313] Batch 38, accuracy = 1
I0513 11:37:09.766072 3659862976 caffe.cpp:313] Batch 38, loss = 0.0161337
I0513 11:37:09.788811 3659862976 caffe.cpp:313] Batch 39, accuracy = 0.98
I0513 11:37:09.788844 3659862976 caffe.cpp:313] Batch 39, loss = 0.0317039
I0513 11:37:09.812556 3659862976 caffe.cpp:313] Batch 40, accuracy = 1
I0513 11:37:09.812587 3659862976 caffe.cpp:313] Batch 40, loss = 0.0283054
I0513 11:37:09.835418 3659862976 caffe.cpp:313] Batch 41, accuracy = 0.98
I0513 11:37:09.835450 3659862976 caffe.cpp:313] Batch 41, loss = 0.0595546
I0513 11:37:09.858765 3659862976 caffe.cpp:313] Batch 42, accuracy = 0.98
I0513 11:37:09.858793 3659862976 caffe.cpp:313] Batch 42, loss = 0.033258
I0513 11:37:09.881479 3659862976 caffe.cpp:313] Batch 43, accuracy = 1
I0513 11:37:09.881510 3659862976 caffe.cpp:313] Batch 43, loss = 0.00560485
I0513 11:37:09.906558 3659862976 caffe.cpp:313] Batch 44, accuracy = 1
I0513 11:37:09.906590 3659862976 caffe.cpp:313] Batch 44, loss = 0.0164246
I0513 11:37:09.932261 3659862976 caffe.cpp:313] Batch 45, accuracy = 0.99
I0513 11:37:09.932294 3659862976 caffe.cpp:313] Batch 45, loss = 0.047733
I0513 11:37:09.957159 3659862976 caffe.cpp:313] Batch 46, accuracy = 1
I0513 11:37:09.957190 3659862976 caffe.cpp:313] Batch 46, loss = 0.00406718
I0513 11:37:09.979852 3659862976 caffe.cpp:313] Batch 47, accuracy = 0.99
I0513 11:37:09.979883 3659862976 caffe.cpp:313] Batch 47, loss = 0.0176224
I0513 11:37:10.003631 3659862976 caffe.cpp:313] Batch 48, accuracy = 0.95
I0513 11:37:10.003666 3659862976 caffe.cpp:313] Batch 48, loss = 0.0918992
I0513 11:37:10.027333 3659862976 caffe.cpp:313] Batch 49, accuracy = 1
I0513 11:37:10.027365 3659862976 caffe.cpp:313] Batch 49, loss = 0.00535747
I0513 11:37:10.050904 3659862976 caffe.cpp:313] Batch 50, accuracy = 1
I0513 11:37:10.050935 3659862976 caffe.cpp:313] Batch 50, loss = 0.000293352
I0513 11:37:10.076280 3659862976 caffe.cpp:313] Batch 51, accuracy = 1
I0513 11:37:10.076314 3659862976 caffe.cpp:313] Batch 51, loss = 0.00675426
I0513 11:37:10.099964 3659862976 caffe.cpp:313] Batch 52, accuracy = 1
I0513 11:37:10.099993 3659862976 caffe.cpp:313] Batch 52, loss = 0.0113504
I0513 11:37:10.123363 3659862976 caffe.cpp:313] Batch 53, accuracy = 1
I0513 11:37:10.123394 3659862976 caffe.cpp:313] Batch 53, loss = 0.00080642
I0513 11:37:10.146338 3659862976 caffe.cpp:313] Batch 54, accuracy = 1
I0513 11:37:10.146368 3659862976 caffe.cpp:313] Batch 54, loss = 0.0119724
I0513 11:37:10.170075 3659862976 caffe.cpp:313] Batch 55, accuracy = 1
I0513 11:37:10.170106 3659862976 caffe.cpp:313] Batch 55, loss = 9.95353e-05
I0513 11:37:10.192754 3659862976 caffe.cpp:313] Batch 56, accuracy = 1
I0513 11:37:10.192785 3659862976 caffe.cpp:313] Batch 56, loss = 0.00792123
I0513 11:37:10.215930 3659862976 caffe.cpp:313] Batch 57, accuracy = 1
I0513 11:37:10.215963 3659862976 caffe.cpp:313] Batch 57, loss = 0.0106224
I0513 11:37:10.238731 3659862976 caffe.cpp:313] Batch 58, accuracy = 1
I0513 11:37:10.238765 3659862976 caffe.cpp:313] Batch 58, loss = 0.00865888
I0513 11:37:10.261700 3659862976 caffe.cpp:313] Batch 59, accuracy = 0.98
I0513 11:37:10.261731 3659862976 caffe.cpp:313] Batch 59, loss = 0.0758659
I0513 11:37:10.284554 3659862976 caffe.cpp:313] Batch 60, accuracy = 1
I0513 11:37:10.284585 3659862976 caffe.cpp:313] Batch 60, loss = 0.00406362
I0513 11:37:10.310072 3659862976 caffe.cpp:313] Batch 61, accuracy = 1
I0513 11:37:10.310102 3659862976 caffe.cpp:313] Batch 61, loss = 0.00472714
I0513 11:37:10.332813 3659862976 caffe.cpp:313] Batch 62, accuracy = 1
I0513 11:37:10.332845 3659862976 caffe.cpp:313] Batch 62, loss = 0.00013836
I0513 11:37:10.356101 3659862976 caffe.cpp:313] Batch 63, accuracy = 1
I0513 11:37:10.356132 3659862976 caffe.cpp:313] Batch 63, loss = 0.000318341
I0513 11:37:10.378556 3659862976 caffe.cpp:313] Batch 64, accuracy = 1
I0513 11:37:10.378587 3659862976 caffe.cpp:313] Batch 64, loss = 0.000235923
I0513 11:37:10.402688 3659862976 caffe.cpp:313] Batch 65, accuracy = 0.94
I0513 11:37:10.402724 3659862976 caffe.cpp:313] Batch 65, loss = 0.174556
I0513 11:37:10.426704 3659862976 caffe.cpp:313] Batch 66, accuracy = 0.98
I0513 11:37:10.426736 3659862976 caffe.cpp:313] Batch 66, loss = 0.0710799
I0513 11:37:10.450608 3659862976 caffe.cpp:313] Batch 67, accuracy = 0.99
I0513 11:37:10.450641 3659862976 caffe.cpp:313] Batch 67, loss = 0.0471492
I0513 11:37:10.474786 3659862976 caffe.cpp:313] Batch 68, accuracy = 1
I0513 11:37:10.474853 3659862976 caffe.cpp:313] Batch 68, loss = 0.00714237
I0513 11:37:10.497565 3659862976 caffe.cpp:313] Batch 69, accuracy = 1
I0513 11:37:10.497596 3659862976 caffe.cpp:313] Batch 69, loss = 0.00141993
I0513 11:37:10.520592 3659862976 caffe.cpp:313] Batch 70, accuracy = 1
I0513 11:37:10.520623 3659862976 caffe.cpp:313] Batch 70, loss = 0.00206052
I0513 11:37:10.543385 3659862976 caffe.cpp:313] Batch 71, accuracy = 1
I0513 11:37:10.543418 3659862976 caffe.cpp:313] Batch 71, loss = 0.000801532
I0513 11:37:10.567934 3659862976 caffe.cpp:313] Batch 72, accuracy = 0.99
I0513 11:37:10.567965 3659862976 caffe.cpp:313] Batch 72, loss = 0.0175235
I0513 11:37:10.591750 3659862976 caffe.cpp:313] Batch 73, accuracy = 1
I0513 11:37:10.591784 3659862976 caffe.cpp:313] Batch 73, loss = 0.000181734
I0513 11:37:10.617092 3659862976 caffe.cpp:313] Batch 74, accuracy = 1
I0513 11:37:10.617122 3659862976 caffe.cpp:313] Batch 74, loss = 0.00376508
I0513 11:37:10.639822 3659862976 caffe.cpp:313] Batch 75, accuracy = 1
I0513 11:37:10.639853 3659862976 caffe.cpp:313] Batch 75, loss = 0.00211647
I0513 11:37:10.664058 3659862976 caffe.cpp:313] Batch 76, accuracy = 1
I0513 11:37:10.664090 3659862976 caffe.cpp:313] Batch 76, loss = 0.000218412
I0513 11:37:10.686815 3659862976 caffe.cpp:313] Batch 77, accuracy = 1
I0513 11:37:10.686847 3659862976 caffe.cpp:313] Batch 77, loss = 0.000203503
I0513 11:37:10.710923 3659862976 caffe.cpp:313] Batch 78, accuracy = 1
I0513 11:37:10.710953 3659862976 caffe.cpp:313] Batch 78, loss = 0.0013391
I0513 11:37:10.733860 3659862976 caffe.cpp:313] Batch 79, accuracy = 1
I0513 11:37:10.733891 3659862976 caffe.cpp:313] Batch 79, loss = 0.00335708
I0513 11:37:10.758643 3659862976 caffe.cpp:313] Batch 80, accuracy = 0.99
I0513 11:37:10.758677 3659862976 caffe.cpp:313] Batch 80, loss = 0.0256179
I0513 11:37:10.781409 3659862976 caffe.cpp:313] Batch 81, accuracy = 1
I0513 11:37:10.781440 3659862976 caffe.cpp:313] Batch 81, loss = 0.0023732
I0513 11:37:10.805886 3659862976 caffe.cpp:313] Batch 82, accuracy = 0.99
I0513 11:37:10.805920 3659862976 caffe.cpp:313] Batch 82, loss = 0.0162458
I0513 11:37:10.828743 3659862976 caffe.cpp:313] Batch 83, accuracy = 1
I0513 11:37:10.828775 3659862976 caffe.cpp:313] Batch 83, loss = 0.00678432
I0513 11:37:10.852507 3659862976 caffe.cpp:313] Batch 84, accuracy = 0.99
I0513 11:37:10.852538 3659862976 caffe.cpp:313] Batch 84, loss = 0.0189542
I0513 11:37:10.875788 3659862976 caffe.cpp:313] Batch 85, accuracy = 0.99
I0513 11:37:10.875819 3659862976 caffe.cpp:313] Batch 85, loss = 0.0198986
I0513 11:37:10.899011 3659862976 caffe.cpp:313] Batch 86, accuracy = 1
I0513 11:37:10.899040 3659862976 caffe.cpp:313] Batch 86, loss = 0.000146087
I0513 11:37:10.921692 3659862976 caffe.cpp:313] Batch 87, accuracy = 1
I0513 11:37:10.921723 3659862976 caffe.cpp:313] Batch 87, loss = 0.000129989
I0513 11:37:10.944453 3659862976 caffe.cpp:313] Batch 88, accuracy = 1
I0513 11:37:10.944484 3659862976 caffe.cpp:313] Batch 88, loss = 4.1275e-05
I0513 11:37:10.968449 3659862976 caffe.cpp:313] Batch 89, accuracy = 1
I0513 11:37:10.968482 3659862976 caffe.cpp:313] Batch 89, loss = 4.4345e-05
I0513 11:37:10.994932 3659862976 caffe.cpp:313] Batch 90, accuracy = 0.97
I0513 11:37:10.994962 3659862976 caffe.cpp:313] Batch 90, loss = 0.0680957
I0513 11:37:11.018280 3659862976 caffe.cpp:313] Batch 91, accuracy = 1
I0513 11:37:11.018312 3659862976 caffe.cpp:313] Batch 91, loss = 2.29651e-05
I0513 11:37:11.044423 3659862976 caffe.cpp:313] Batch 92, accuracy = 1
I0513 11:37:11.044457 3659862976 caffe.cpp:313] Batch 92, loss = 0.000162702
I0513 11:37:11.068132 3659862976 caffe.cpp:313] Batch 93, accuracy = 1
I0513 11:37:11.068163 3659862976 caffe.cpp:313] Batch 93, loss = 0.000582345
I0513 11:37:11.090775 3659862976 caffe.cpp:313] Batch 94, accuracy = 1
I0513 11:37:11.090806 3659862976 caffe.cpp:313] Batch 94, loss = 0.000352066
I0513 11:37:11.115216 3659862976 caffe.cpp:313] Batch 95, accuracy = 1
I0513 11:37:11.115247 3659862976 caffe.cpp:313] Batch 95, loss = 0.00453322
I0513 11:37:11.115762 84811776 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:37:11.137984 3659862976 caffe.cpp:313] Batch 96, accuracy = 0.97
I0513 11:37:11.138017 3659862976 caffe.cpp:313] Batch 96, loss = 0.0792528
I0513 11:37:11.162164 3659862976 caffe.cpp:313] Batch 97, accuracy = 0.98
I0513 11:37:11.162194 3659862976 caffe.cpp:313] Batch 97, loss = 0.106678
I0513 11:37:11.184717 3659862976 caffe.cpp:313] Batch 98, accuracy = 1
I0513 11:37:11.184751 3659862976 caffe.cpp:313] Batch 98, loss = 0.0035934
I0513 11:37:11.208353 3659862976 caffe.cpp:313] Batch 99, accuracy = 0.99
I0513 11:37:11.208385 3659862976 caffe.cpp:313] Batch 99, loss = 0.0180797
I0513 11:37:11.208390 3659862976 caffe.cpp:318] Loss: 0.0304018
I0513 11:37:11.208411 3659862976 caffe.cpp:330] accuracy = 0.991
I0513 11:37:11.208425 3659862976 caffe.cpp:330] loss = 0.0304018 (* 1 = 0.0304018 loss)
</code></pre>

<p>最后accuracy为0.991，loss为0.03</p>

<h2 id="toc_4">总结</h2>

<p>通过上述内容,我们可以初步了解一个完整的深度学习系统最核心的两个方面:<strong>数据和模型</strong>.数据是带标签的图片集,分训练集和测试集;模型是描述CNN结构的有向无环图(DAG),表示对原始数据的处理方式.</p>

<p><font color=red>Caffe并不直接处理原始数据,由预处理程序将原始数据存储为<code>LMDB</code>格式,来保持较高的IO效率.模型通常用ProtoBuffer文本格式表述,训练结果保存为ProtoBuffer二进制文件或HDF5格式文件.</font><strong>深度学习的过程就是利用训练数据对模型进行训练,将数据中蕴藏的大量信息通过机器学习算法不断收集到模型中,利用训练好的模型对现实世界中相似数据进行特定处理(如分类,识别,检测,定位).</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>LZH007</h1>
                <div class="site-des">LZH的技术杂事小博客~</div>
                <div class="social">











  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>统计学习方法</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Python练习</strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>图像去雾技术</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="14963894105351.html">提交测试</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957648679184.html">UDP编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957594179111.html">网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957005904776.html">图形界面</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14956930097908.html">HTMLParser</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
