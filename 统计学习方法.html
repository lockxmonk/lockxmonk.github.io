
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  统计学习方法 - LZH007
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="LZH的技术杂事小博客~">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">LZH007</a></h1>
  
    <h2>LZH的技术杂事小博客~</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:lockxmonk.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="14893694433396.html">朴素贝叶斯法</a></h1>
			<p class="meta"><time datetime="2017-03-13T09:44:03+08:00" 
			pubdate data-updated="true">2017/3/13</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>朴素贝叶斯（naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布：然后基于此模型，对给定的输入:x，利用贝叶斯定理求出后验概率最大的输出:y。朴素贝叶斯法实现简单，学习与预测的效率都很髙，是一种常用的方法.</p>

<h2 id="toc_0">朴素贝叶斯的学习与分类</h2>

<h3 id="toc_1">基本方法</h3>

<p>设输入空间\(\chi \subseteq R^n\)为n维向量的集合，输出空间为类标记集合\(\mathcal{Y}={c_1,c_2,...c_k}\)。输入为特征向量\(x\in \chi\),输出为类标记\(y\in mathcal{Y}\)。X是定义在输入空间\(\chi\)上的随机向量，Y是定义在输出空间\(\mathcal{Y}\)上的随机变量。P(X,Y)是X和Y的联合概率分布。训练数据集：<br/>
\(T = {(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)<br/>
由P(X,Y)独立同分布产生。</p>

<p>朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y)。具体的，学习以下先验概率分布及条件概率分布。先验概率分布：<br/>
\(P(Y = c_k), k=1,2,3...K\)</p>

<p>条件概率分布：<br/>
\(P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,...K\)<br/>
于是学习到联合概率分布P(X,Y)。</p>

<p>条件概率分布P(X=x|Y=\(c_k\))有指数级数量的参数，其估计实际是不可行的，事实上，假设\(x^{(j)}可取值有S_j个，j=1,2,3...n,Y可取值为K个，那么参数个数为K\prod_{j=1}^n{S_j}\)</p>

<p>朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：<br/>
<img src="media/14893694433396/14897352338918.jpg" alt=""/></p>

<p>朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴 素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。<br/>
朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布\(P(Y=C_k|X=x)\),将后验概率最大的类作为x的类输出。后验概率计算根据贝 叶斯定理进行：<br/>
<img src="media/14893694433396/14897352467770.jpg" alt=""/></p>

<h3 id="toc_2">后验概率最大化的含义</h3>

<p>朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。假设选择0-1损失函数：<br/>
<img src="media/14893694433396/14897368963380.jpg" alt=""/><br/>
式中f(X)是分类决策函数。这时，期望风险函数为：<br/>
<img src="media/14893694433396/14897369463646.jpg" alt=""/><br/>
期望是对联合分布P(X,Y)取的，由此取条件期望：<br/>
<img src="media/14893694433396/14897370507779.jpg" alt=""/><br/>
为了是期望风险最小化，只需要对\(X=x\)逐个极小化，由此得到：<br/>
<img src="media/14893694433396/14897371045644.jpg" alt=""/><br/>
这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：<br/>
<img src="media/14893694433396/14897380067414.jpg" alt=""/><br/>
即朴素贝叶斯法所采用的原理。</p>

<h2 id="toc_3">朴素贝叶斯发的参数估计</h2>

<h3 id="toc_4">极大似然估计</h3>

<p>在朴素贝叶斯法中，学习意味着估计\(P(Y=c_k)\)和\(P(X^{(j)}=x^{(j)} |Y=c_k)\)。可以应用极大似然估计法估计相应的概率。先验概率\(P(Y=c_k)\)的极大似然估计是:<br/>
<img src="media/14893694433396/14897383598198.jpg" alt=""/><br/>
设第j个特征\(x^{(j)}\)可能取值的几何为{\(a_{j1},a_{j2}....a_{jS_j}\)},条件概率\(P(X^{j}=a_{jl}|Y=c_k)\)的极大似然估计是：<br/>
<img src="media/14893694433396/14897397226736.jpg" alt=""/></p>

<h2 id="toc_5">学习与分类算法</h2>

<p>下面给出朴素贝叶斯的学习与分类方法。</p>

<p>算法(朴素贝叶斯算法)：<br/>
<img src="media/14893694433396/14897403452985.jpg" alt=""/></p>

<p>下面用一个例题来解释说明上述算法：<br/>
<img src="media/14893694433396/14897405687808.jpg" alt=""/><br/>
<img src="media/14893694433396/14897405876725.jpg" alt=""/></p>

<h2 id="toc_6">贝叶斯估计</h2>

<p>用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差，解决这一问题的方法是采用贝叶斯估计。具 体地，条件概率的贝叶斯估计是：<br/>
<img src="media/14893694433396/14897409238342.jpg" alt=""/></p>

<p>这次再用上述例题来举例，如下所示：<br/>
<img src="media/14893694433396/14897415528528.jpg" alt=""/></p>

<blockquote>
<p><strong>总结：朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。</strong></p>
</blockquote>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="14891936534415.html">k近邻法</a></h1>
			<p class="meta"><time datetime="2017-03-11T08:54:13+08:00" 
			pubdate data-updated="true">2017/3/11</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">k近邻算法</a>
</li>
<li>
<a href="#toc_1">k近邻模型</a>
<ul>
<li>
<a href="#toc_2">模型</a>
</li>
<li>
<a href="#toc_3">距离度量</a>
</li>
<li>
<a href="#toc_4">k值的选择</a>
</li>
<li>
<a href="#toc_5">分类决策规则</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">k近邻法的实现：kd树</a>
<ul>
<li>
<a href="#toc_7">构造kd树</a>
</li>
<li>
<a href="#toc_8">搜索kd树</a>
</li>
</ul>
</li>
</ul>


<p>k近邻法（k-nearestneighbor,k-NN)是一种基本分类与回归方法。本书只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间 的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别己定。分类时，对新的实例，根据其k个最近邻的训练实例的类别， 通过多数表决等方式进行预测。因此，k近邻法不具有显式的学习过程。近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型“，k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。k近邻法1968年由Cover和Hart提出。</p>

<h2 id="toc_0">k近邻算法</h2>

<p>k近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类，下面叙述k邻近算法。<br/>
<img src="media/14891936534415/14891944741165.jpg" alt=""/><br/>
<img src="media/14891936534415/14891946432255.jpg" alt=""/></p>

<h2 id="toc_1">k近邻模型</h2>

<p>k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。</p>

<h3 id="toc_2">模型</h3>

<p>k近邻算法中，当训练集、距离度量（如欧氏距离）、k值及分类局侧规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。</p>

<p>特征空间中，对每个训练实例点\(x_i\),距离该点比其他点更近的所有点组成一个区域，叫作单元（cell).每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例\(x_i\)的类\(y_i\),作为其单元中所有点的类标记（class label)。这样，每个单元的实例点的类别是确定的。下图是二维特征空间划分的一个例子。<br/>
<img src="media/14891936534415/14891961608422.jpg" alt=""/></p>

<h3 id="toc_3">距离度量</h3>

<p>特征空间中两个实例点的距离是两个实例点相似程度的反映.k近邻模型的特征空间一般是n维实数向量空间\(R^n\)。使用的距离是欧氏距离，但也可以是其他距离，如更一般的\(L_p\)距离（\(L_p -distance\))或Minkowski距离（Minkowski distance)。<br/>
关于特征空间中两坐标点的距离定义为：<br/>
<img src="media/14891936534415/14891974598310.jpg" alt=""/></p>

<p>不同的距离定义，所求出的最近邻点是不同的。<br/>
<img src="media/14891936534415/14891979526147.jpg" alt=""/></p>

<h3 id="toc_4">k值的选择</h3>

<p>k值的选择会对k近邻法的结果产生重大影响。</p>

<p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error)会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error) 会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错.换句话说，值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>

<p>如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例 较远的（不相似的）训练实例也会对预测起作用，使预测发生错误.k值的增大就意味着整体的模型变得简单。</p>

<p>如果k=N,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。</p>

<p>在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p>

<h3 id="toc_5">分类决策规则</h3>

<p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</p>

<p>多数表决规则（majority voting rule)有如下解释：如果分类的损失函数为0-1损失函数，分类函数为：<br/>
<img src="media/14891936534415/14891994327344.jpg" alt=""/></p>

<p>那么误分类的概率为：<br/>
<img src="media/14891936534415/14891994564311.jpg" alt=""/><br/>
对给定的实例\(x \in \chi\),其中最邻近的k个训练实例点构成集合\(N_k(x)\),如果涵盖\(N_k(x)\)的区域的类别是\(C_j\),那么误分类率是：<br/>
<img src="media/14891936534415/14891997004694.jpg" alt=""/><br/>
要使误分类率最小即经验风险最小，就要使\(\sum_{x_i\in{N_k(x)}}I(y_i=c_j)\)最大，<mark>所以多数表决规则等价于风险最小化</mark>。</p>

<h2 id="toc_6">k近邻法的实现：kd树</h2>

<p>实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻捜索。这点在特征空间的维数大及训练数据容量大时尤其必要。</p>

<p>k近邻法最简单的实现方法是线性扫描（linear  scan)。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。</p>

<p>为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的kd树(kd tree)方法.</p>

<h3 id="toc_7">构造kd树</h3>

<p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition)。构造kd树相 当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。</p>

<p>构造kd树的方法如下：构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；通过下面的递归方法，不断地对k维空间进行切分，生成子结 点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）：这时，实例被分到两个子区域.这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。</p>

<p>通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数（median)为切分点，这样得到的kd树是平衡的。<mark>注意，平衡的kd树搜索时的效率未必是最优的</mark>。<br/>
下面给出构造kd树的算法：<br/>
<img src="media/14891936534415/14892010553846.jpg" alt=""/><br/>
<img src="media/14891936534415/14892019093068.jpg" alt=""/></p>

<p>下面给出一个例子，来反应上述算法的用途：<br/>
<strong>（中位数：一组数据按大小顺序排列起来，处在中间位置的一个数或两个数的平均值。）</strong><br/>
<img src="media/14891936534415/14892021282433.jpg" alt=""/><br/>
<img src="media/14891936534415/14892121527765.jpg" alt=""/></p>

<h3 id="toc_8">搜索kd树</h3>

<p>下面介绍如何利用kd树进行k近邻搜索。可以看到，利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。这里以最近邻为例加以叙述，同样的方法可以应用到k近邻。</p>

<p>给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点：然后从该叶结点出发，依次回退到父结点；不断査找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提髙。</p>

<p>包含目标点的叶结点对应包含目标点的最小超矩形区域。以此叶结点的实例点作为当前最近点。目标点的最近邻一定在以目标点为中心并通过当前最近点的 超球体的内部（参阅图3.5)。然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点，将此点作为新的当前最近点。算法转到更上一级的父结点，继续上述过程。如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。<br/>
下面叙述用kd树的最近邻搜索算法：</p>

<pre><code>算法3.3 (用kd树的最近邻搜索）

输入：己构造的kd树；目标点X；
输出：x的最近邻。
(1) 在kd树中找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移 动到右子结点。直到子结点为叶结点为止.
(2) 以此叶结点为”当前最近点“
(3) 递归地向上回退，在每个结点进行以下操作：
(a) 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”
(b) 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检査另一子结点对应
的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。
如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索：
如果不相交，向上回退。
(4)当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。
    
如果实例点是随机分布的，kd树搜索的平均计算复杂度是O(logN)，这里N是训练实例数.kd树更适用于训练实例数远大于空间维数时的k近邻搜索。当空 间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。
</code></pre>

<p>下面通过一个例题来说明搜索方法：</p>

<p><img src="media/14891936534415/14892141583109.jpg" alt=""/></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="14891071598954.html">感知机算法的收敛性</a></h1>
			<p class="meta"><time datetime="2017-03-10T08:52:39+08:00" 
			pubdate data-updated="true">2017/3/10</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">感知机学习算法的对偶形式</a>
</li>
</ul>


<p>算法的收敛性主要是证明，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限 次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p>

<p>将偏置b并入权重向量w，记做\(\hat{w} = (w^T,b)^T\)同样也将输入向量加以扩充，加进常数1，记做：\(\hat{x} = (x^T,1)^T\),这样，\(\hat{x}\in R^{n+1}\)，\(\hat{w}\in R^{n+1}\) 显然，\(\hat{w}*\hat{x}=w*x+b\)。<br/>
<img src="media/14891071598954/14891094628135.jpg" alt=""/></p>

<p>上述定理主要说明误分类的次数<code>k</code>是有上界的，算法具有收敛性。</p>

<p>下面是该定理的证明过程：<br/>
<img src="media/14891071598954/14891100385544.jpg" alt=""/><br/>
<img src="media/14891071598954/14891100637572.jpg" alt=""/><br/>
<img src="media/14891071598954/14891105642852.jpg" alt=""/><br/>
<img src="media/14891071598954/14891105771778.jpg" alt=""/></p>

<blockquote>
<p>所以定理表明，误分类的次数<code>k</code>是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。但是感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。这就是线性支持向量机的想法。当训练集线性不可分时，感知机学习算法不收敛，迭代结果 就会发生震荡。</p>
</blockquote>

<h2 id="toc_0">感知机学习算法的对偶形式</h2>

<p>现在考虑感知机学习算法的对偶形式。感知机学习算法的原始形式和对偶形式与支持向量机学习算法的原始形式和对偶形式相对应。</p>

<p>对偶形式的基本思想是，将w和b表示为实例\(x_{i}\)和标记\(y_i\)的线性组合形式，通过求解其系数而求得w和b。不失一般性,在算法2.1中可假设初始值\(w_0,b_0\)均为0，对误分类点\((x_i,y_i)\)通过：<br/>
\(w\leftarrow w+ηy_ix_i\)<br/>
\(b\leftarrow b+ηy_i\)<br/>
逐步修改w，b，设修改n次，则w,b关于\((x_i,y_i)\)的增量分别是\(α_ix_iy_i和α_iy_i\)这里\(α=n_iη\)。这样，从学习过程不难看出，最后学习到的w,b可以分别表示为：<img src="media/14891071598954/14891128523346.jpg" alt=""/><br/>
这里，\(\alpha_i\geq0,i=1,2,...N,当\eta=1时\)，表示第i个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响最大。</p>

<p>下面对照原始形式来叙述感知机学习算法的对偶形式。<br/>
<img src="media/14891071598954/14891134427824.jpg" alt=""/><br/>
<img src="media/14891071598954/14891134670827.jpg" alt=""/><br/>
对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。<br/>
\(G = [x_i * x_j]_{N*N}\)</p>

<p>同样根据该算法有例题如下：<br/>
<img src="media/14891071598954/14891137936409.jpg" alt=""/><br/>
<img src="media/14891071598954/14891170220533.jpg" alt=""/><br/>
对照例2.1，结果一致，迭代步骤也是互相对应的。<br/>
与原始形式一样，感知机学习算法的对偶形式迭代是收敛的。</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="14890437581129.html">感知机</a></h1>
			<p class="meta"><time datetime="2017-03-09T15:15:58+08:00" 
			pubdate data-updated="true">2017/3/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">感知机模型</a>
</li>
<li>
<a href="#toc_1">感知机的学习策略</a>
<ul>
<li>
<a href="#toc_2">数据集的线性可分性</a>
</li>
<li>
<a href="#toc_3">感知机学习策略</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">感知机学习算法</a>
<ul>
<li>
<a href="#toc_5">感知机学习算法的原始形式</a>
</li>
</ul>
</li>
</ul>


<p>感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别（一般为+1和-1），感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面。感知机也是神经网络与支持向量机的基础。</p>

<h2 id="toc_0">感知机模型</h2>

<p>感知机的定义为：<br/>
<img src="media/14890437581129/14890447646559.jpg" alt=""/></p>

<p>感知机有如下的几何解释：<br/>
线性方程：w*x + b = 0<br/>
对应于特征空间R中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点就被划分为正，负两类，如下图所示：<br/>
<img src="media/14890437581129/14890449623708.jpg" alt=""/></p>

<p>所以求感知机的模型，也就是去求得模型参数w，b。感知机预测，通过学习得到的感知机模型，对于新输入实例给出其对应的输出类别。</p>

<h2 id="toc_1">感知机的学习策略</h2>

<h3 id="toc_2">数据集的线性可分性</h3>

<p><img src="media/14890437581129/14890453486666.jpg" alt=""/></p>

<h3 id="toc_3">感知机学习策略</h3>

<p>如果训练数据集是线性可分的，那么我们需要确定一个学习策略，也就是定义（经验）损失函数并将损失函数极小化。</p>

<p>损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数w,b的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。为此，首先写出输入控件R中任一点x到平面S的距离：<img src="media/14890437581129/14890456220821.jpg" alt=""/><br/>
这里，||w||是w的2范数。<br/>
其次，对于五分类的数据（x，y）来说，<br/>
<code>-y(w*x+b)&gt;0</code>成立。因为当<code>w*x+b&gt;0</code>时，<code>y=-1</code>，而当<code>w*x+b&lt;0</code>时<code>y=+1</code>，因此，误分类点x到超平面S的距离是：<br/>
<img src="media/14890437581129/14890460147136.jpg" alt=""/><br/>
这样，假设超平面S的误分类点集合为M，那么所有的误分类点到超平面S的总距离为：<br/>
<img src="media/14890437581129/14890462066356.jpg" alt=""/></p>

<p>所以，感知机的损失函数定义为：<img src="media/14890437581129/14890462888502.jpg" alt=""/></p>

<p>显然，损失函数是非负的，如果没有误分类点，那么损失函数为0，而误分类点越少，误分类点离超平面越近，损失函数值越小。一个特定的样本点的损失函数：在误分类时是参数w，b的线性函数，在正确分类时是0，因此给定训练数据集T，损失函数是w，b的连续可导函数。<br/>
总的来说我们的感知机学习策略就是在假设空间中选取使损失函数式最小的模型参数w，b。</p>

<h2 id="toc_4">感知机学习算法</h2>

<p>我们的策略已经明确，就是求解损失函数式的最优化，我们这里最优化的方法是随机梯度下降法。</p>

<h3 id="toc_5">感知机学习算法的原始形式</h3>

<p>感知机学习算法是对以下最优化问题的算法，给定一个训练集：<img src="media/14890437581129/14890473616479.jpg" alt=""/><br/>
感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent)。首先，任意选取一个超平面然后用梯度下降法不断地极小化目标函数<code>（2.5）</code>。极小化过程中不是一次使M中所有误分类点的梯度下降， 而是一次随机选取一个误分类点使其梯度下降。<br/>
假设误分类点集合M是固定的，那么损失函数的梯度由<img src="media/14890437581129/14890475151426.jpg" alt=""/><br/>
给出。<br/>
随机选取一个误分类点<code>(x,y)</code>，对w,b进行更新：<img src="media/14890437581129/14890475562013.jpg" alt=""/><br/>
式中η(0&lt;η≤1)是步长，在统计学习中又称为学习率，这样，通过迭代可以期待损失函数不断减小，直到为0，综上所述，得到如下算法：<img src="media/14890437581129/14890483224015.jpg" alt=""/><br/>
这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p>

<p>上述算法是感知机学习的基本算法，对应于后面的对偶形式，称为原始形式。感知机学习算法简单且易于实现。</p>

<p>该算法使用方法如下：<br/>
<img src="media/14890437581129/14890503525005.jpg" alt=""/><br/>
<img src="media/14890437581129/14890503748369.jpg" alt=""/><br/>
<img src="media/14890437581129/14890503942221.jpg" alt=""/><br/>
这是在计算中误分类点先后取<code>x1,x3,x3,x3,x1,x3,x3</code>得到的分离超平面和感 知机模型，如果在计算中误分类点依次取<code>x1,x3,x3,x3,x2,x3,x3,x3,x1,x3,x3</code>那么得到的分离超平面是2x<sup>1</sup>+x<sup>2</sup>-5=0</p>

<p>可见，感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。</p>


		</div>

		

	</article>
  
	<div class="pagination">
	
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="Effective%20OC2.0.html"><strong>Effective OC2.0&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="English%20Study.html"><strong>English Study&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>深度学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>统计学习方法&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="14893694433396.html">朴素贝叶斯法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14891936534415.html">k近邻法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14891071598954.html">感知机算法的收敛性</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14890437581129.html">感知机</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14878150947508.html">MNIST机器学习入门</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>