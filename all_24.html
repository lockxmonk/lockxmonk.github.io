<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="MAC%20OS.html">MAC OS</a></li>
        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">统计学习方法</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Python练习</a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">图像去雾技术</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html">算法学习</a></li>
        
            <li><a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html">常见面试问题</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14970763342670.html">
                
                  <h1>caffe数据结构描述</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>打开caffe目录下的<code>src/caffe/proto/caffe.proto</code>文件,首先讲的就是Blob的描述.</p>

<pre><code class="language-protobuf">// 该结构描述了 Blob的形状信息
message BlobShape {
  repeated int64 dim = 1 [packed = true];  //只包括若干int64类型值，分别表示Blob每个维度的大小。packed表示这些值在内存中紧密排布，没有空洞
}

//该结构描述Blob在磁盘中序列化后的形态
message BlobProto {
  optional BlobShape shape = 7;    //可选，包括一个BlobShape对象
  repeated float data = 5 [packed = true]; // //包括若千浮点元素，存储数据或权值，元素数目由shape或（num, channels, height, width)确定，这些元素在内存中紧密排布.
  repeated float diff = 6 [packed = true];  ////包括若干浮点元素，用于存储增量信息，维度与data 数组一致
  repeated double double_data = 8 [packed = true];  // 与 data并列，只是类型为double
  repeated double double_diff = 9 [packed = true];  // 与 diff 并列，只是类型为 double

  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}
</code></pre>

<p><strong>这里我们使用protobuffer主要是因为它具有很好的健壮性,将编程最容易出问题的地方加以隐藏，让机器自动处理.</strong></p>

<h2 id="toc_0">Blob的构成</h2>

<p>Blob是一个模板类,声明在<code>include/caffe/blob.hpp中</code>,里面封装了一些基本的Layer,Net,Solver等,还有syncedmem类:</p>

<pre><code class="language-C++">
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;//由protoc生成的头文件，声明了 BlobProto、BlobShape等遵循caffe.proto协议的数据结构 可以在src/caffe/proto文件下运行protoc caffe.proto --cpp_out=./命令生成该头文件.
#include &quot;caffe/syncedmem.hpp&quot;  //CPU/GPU共享内存类，用于数据同步

const int kMaxBlobAxes = 32;    //Blob最大维数目
template &lt;typename Dtype&gt;
class Blob {    //类声明
 public:
    //默认构造函数
  Blob()
       : data_(), diff_(), count_(0), capacity_(0) {}
    //显式构造函数
  explicit Blob(const int num, const int channels, const int height, const int width);
  explicit Blob(const vector&lt;int&gt;&amp; shape);

 //变形函数，报据输入参数重新设置当前Blob形状,必要时重新分配内存
  void Reshape(const int num, const int channels, const int height,
      const int width);
  
  void Reshape(const vector&lt;int&gt;&amp; shape);
  void Reshape(const BlobShape&amp; shape);
  void ReshapeLike(const Blob&amp; other);
  //得到Blob形状字符串用于打印log,见Caffe运行log,类似&quot;Top shape: 100 1 28 28 (78400)&quot;
  inline string shape_string() const {
    ostringstream stream;
    for (int i = 0; i &lt; shape_.size(); ++i) {
      stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;
    }
    stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;
    return stream.str();
  }
  //返回Blob形状
  inline const vector&lt;int&gt;&amp; shape() const { return shape_; }
    //返回某1维度的尺寸
  inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  //返回维度数目
  inline int num_axes() const { return shape_.size(); }
  //返回Blob中元素总数
  inline int count() const { return count_; }
    //返回Blob中某几维子集的元素总数
    inline int count(int start_axis, int end_axis) const {
    CHECK_LE(start_axis, end_axis); //保证 start_axis &lt;= end_axis
    CHECK_GE(start_axis, 0);  // 保证 start_axis &gt;= 0
    CHECK_GE(end_axis, 0);      // 保证 end_axis &gt;= 0
    CHECK_LE(start_axis, num_axes()); //保证start_axis    &lt;=总的维度数目
    CHECK_LE(end_axis, num_axes()); //保证end_axis &lt;=总的维度数目
    int count = 1;
    for (int i = start_axis; i &lt; end_axis; ++i) {
      count *= shape(i);
    }
    return count;
  }
  //计算从某一维度开始的元素总数
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
  //转换坐标轴索引[-N,N)为普通索引[0,N)
  inline int CanonicalAxisIndex(int axis_index) const {
    CHECK_GE(axis_index, -num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    CHECK_LT(axis_index, num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    if (axis_index &lt; 0) {
    //负索引表示从后向前访问，-1表示最后一个个元素，普通索引值为 N-1:同理，-2 =&gt; N-2, -3 =&gt; N-3,…
      return axis_index + num_axes();
    }
    return axis_index;
  }
  //获取某一维的尺寸
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.
  inline int num() const { return LegacyShape(0); }
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.
  inline int channels() const { return LegacyShape(1); }
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.
  inline int height() const { return LegacyShape(2); }
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.
  inline int width() const { return LegacyShape(3); }
  inline int LegacyShape(int index) const {
    CHECK_LE(num_axes(), 4)
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;
    CHECK_LT(index, 4);
    CHECK_GE(index, -4);
    if (index &gt;= num_axes() || index &lt; -num_axes()) {
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse
      // indexing) -- this special case simulates the one-padding used to fill
      // extraneous axes of legacy blobs.
      return 1;
    }
    return shape(index);
  }
  //下面的是计算偏移量的函数
  inline int offset(const int n, const int c = 0, const int h = 0,
      const int w = 0) const {
    CHECK_GE(n, 0);
    CHECK_LE(n, num());
    CHECK_GE(channels(), 0);
    CHECK_LE(c, channels());
    CHECK_GE(height(), 0);
    CHECK_LE(h, height());
    CHECK_GE(width(), 0);
    CHECK_LE(w, width());
    return ((n * channels() + c) * height() + h) * width() + w;
  }

  inline int offset(const vector&lt;int&gt;&amp; indices) const {
    CHECK_LE(indices.size(), num_axes());
    int offset = 0;
    for (int i = 0; i &lt; num_axes(); ++i) {
      offset *= shape(i);
      if (indices.size() &gt; i) {
        CHECK_GE(indices[i], 0);
        CHECK_LT(indices[i], shape(i));
        offset += indices[i];
      }
    }
    return offset;
  }
  //按值拷贝Blob到当前Blob
  void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false);
  
  //下面几个函数是存取器(getter/setter)
  inline Dtype data_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_data()[offset(n, c, h, w)];
  }

  inline Dtype diff_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_diff()[offset(n, c, h, w)];
  }

  inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_data()[offset(index)];
  }

  inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_diff()[offset(index)];
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;
  }
  
  //只读访问cpu_date
  const Dtype* cpu_data() const;
  //设置cpu_date
  void set_cpu_data(Dtype* data);
  const int* gpu_shape() const;
  //只读访问gpu_date
  const Dtype* gpu_data() const;
  //设置gpu_date
  void set_gpu_data(Dtype* data);
  //只读访问cpu_diff
  const Dtype* cpu_diff() const;
  //只读访问gpu_diff
  const Dtype* gpu_diff() const;
  //下面四个是读写访问数据
  Dtype* mutable_cpu_data();
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  void Update();    //Blob更新运算，可简单理解为data与diff的merge过程
  //反序列化函数，从BlobProto中恢复个Blob对象
  void FromProto(const BlobProto&amp; proto, bool reshape = true);
  //序列化函数，将内存中的Blob对象保存到BlobProto中
  void ToProto(BlobProto* proto, bool write_diff = false) const;

  /// @brief Compute the sum of absolute values (L1 norm) of the data.
  Dtype asum_data() const;
  /// @brief Compute the sum of absolute values (L1 norm) of the diff.
  Dtype asum_diff() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the data.
  Dtype sumsq_data() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the diff.
  Dtype sumsq_diff() const;

/// @brief Scale the blob data by a constant factor.
  void scale_data(Dtype scale_factor);
  /// @brief Scale the blob diff by a constant factor.
  void scale_diff(Dtype scale_factor);
 // 共享另一个 Blob 的 diff
  void ShareData(const Blob&amp; other);
  void ShareDiff(const Blob&amp; other);
  
  protected:
  shared_ptr&lt;SyncedMemory&gt; data_;   //存放指向data的指针
  shared_ptr&lt;SyncedMemory&gt; diff_;   //存放指向diff的指针
  shared_ptr&lt;SyncedMemory&gt; shape_data_; 
  vector&lt;int&gt; shape_;   //形状信息
  int count_;   //存放有效元素数目信息
  int capacity_;    //存放Blob容器的容量信息

  DISABLE_COPY_AND_ASSIGN(Blob);    //禁用拷贝构造函数、陚值运算符重载
};  // class Blob

</code></pre>

<p><strong>注意到Caffe类中成员变量名都带有后缀，这样在函数实现中容易区分临时变量和类成员变量。</strong></p>

<p>打幵<code>include/caffe/syncedmem.hpp</code>，査看该类的用法:</p>

<pre><code class="language-c++">#ifndef CAFFE_SYNCEDMEM_HPP_
#define CAFFE_SYNCEDMEM_HPP_

#include &lt;cstdlib&gt;

#ifdef USE_MKL
  #include &quot;mkl.h&quot;
#endif

#include &quot;caffe/common.hpp&quot;

namespace caffe {

//如果在GPU模式，且CUDA使能，那么主机内存会以页锁定内存方式分配（使用cudaMallocHostU函数。对f-单GPU的性能提升不明显，但多GPU会非常明显)
inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) {
#ifndef CPU_ONLY
  if (Caffe::mode() == Caffe::GPU) {
    CUDA_CHECK(cudaMallocHost(ptr, size));
    *use_cuda = true;
    return;
  }
#endif
#ifdef USE_MKL
  *ptr = mkl_malloc(size ? size:1, 64);
#else
  *ptr = malloc(size);
#endif
  *use_cuda = false;
  CHECK(*ptr) &lt;&lt; &quot;host allocation of size &quot; &lt;&lt; size &lt;&lt; &quot; failed&quot;;
}
// 与CaffeMallocHost对应
inline void CaffeFreeHost(void* ptr, bool use_cuda) {
#ifndef CPU_ONLY
  if (use_cuda) {
    CUDA_CHECK(cudaFreeHost(ptr));
    return;
  }
#endif
#ifdef USE_MKL
  mkl_free(ptr);
#else
  free(ptr);
#endif
}

//该类负责存储分配以及主机和设备间同步
class SyncedMemory {
 public:
 //构造函数
  SyncedMemory();
  //显式构造函数
  explicit SyncedMemory(size_t size);
  //析构函数
  ~SyncedMemory();
  const void* cpu_data();       //只读获取cpu data
  void set_cpu_data(void* data);    //设置cpu data
  const void* gpu_data();       //只读获取gpu data
  void set_gpu_data(void* data);    //设置gpu data
  void* mutable_cpu_data();     // 读写获取 cpu data
  void* mutable_gpu_data();     // 读写获取 gpu data
  //状态机变量，表示4种状态：术初始化、CPU数据奋效、GPU数据有效、己同步
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  //获得当前状态机变量值
  SyncedHead head() { return head_; }
  //获得当前存储空间尺寸
  size_t size() { return size_; }

#ifndef CPU_ONLY
  void async_gpu_push(const cudaStream_t&amp; stream);
#endif

 private:
  void check_device();

  void to_cpu();    //数据同步至CPU
  void to_gpu();    //数据同步至GPU
  void* cpu_ptr_;   //位于CPU的数据指针
  void* gpu_ptr_;   //位于GPU的数据指针
  size_t size_;     //存储空间大小
  SyncedHead head_; //状态机变量
  bool own_cpu_data_;   //标志是否拥有CPU数据所有权（否，即从别的对象共享)
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;   ////标志是否拥有GPU数据所有权
  int device_;      //设备号

  DISABLE_COPY_AND_ASSIGN(SyncedMemory);
};  // class SyncedMemory

}  // namespace caffe

#endif  // CAFFE_SYNCEDMEM_HPP_

</code></pre>

<p>Blob类实现的源码位于<code>src/caffe/blob.cpp</code>中，内容如下:</p>

<pre><code class="language-c++">
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {
//变维函数，将（num, channels, height, width}参数转换为vector&lt;int&gt;，然后调用重载的变维函数void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape)
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
//真正变维函数
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes); //保证vector维度&lt;=kMaxBlobAxes
  count_ = 1;   //用于计算元素总数=num * channels * height * width 
  shape_.resize(shape.size());  //成员变量维度也被重罝
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);  // 保证每维度尺寸都&gt;=0
    if (count_ != 0) {
    //证count_不溢出
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];     //count_累乘
    shape_[i] = shape[i];   //为成员变量赋值
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {     //如果新的count_大于当前己分f配空间容量
    capacity_ = count_;         //扩容，重新分配data_和dif f_空间
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

//void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) 和void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)与上面类似. 

//构造函数
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  // 调用Reshape之前必须初始化capacity_，否则会导致不可预期结果
  : capacity_(0) {
  Reshape(num, channels, height, width);
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(shape);
}

template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}
//只读获取cpu date指针
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);     //保证data_不为 NULL
  return (const Dtype*)data_-&gt;cpu_data();
}
//修改cpu data指针
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}
//只读获取cpu_diff指针
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}
//只读获取gpu_diff指针
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}
//读写访问cpu data指针
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}
//读写访问gpu data指针
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}
//与上面相同
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}
//共享另一个Blob的data指针
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}
//共享另一个Blob的diff指针
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}
//Update()函数用于网络参数Blob的更新。其中int和unsigned int类型处理并未实现
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  // We will perform update based on where the data is located.data在哪里我们就在那里更新
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:       //data位于cpu端
    // 执行CPU计算
        caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:   //data位于GPU端,或者CPU/GPU已经同步
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // 执行 CPU 上的计算，data_[i】=data_[i] - diff_[i], i = 0,1,2,…，count_-1
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;     //编泽时打开了CPU_ONLY选项，那么GPU模式禁用
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
  }
}
//计算data_的L1-范数,其中int和unsigned int类型处理并未实现
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());  //执行CPU上的asum计算
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
//同上,计算diff_的L1范数
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
  return 0;
}
//计算data_的L2-范数
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);  //执行 CPU上的dot计算
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//同上,计算diff_的L2-范数
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//对data_进行幅度缩放
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:   //执行CPU上的计算
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}
//对diff_进行缩放,同理
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
}
//判断形状是否相同
template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    // Using deprecated 4D Blob dimensions --
    // shape is (num, channels, height, width).
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.
    // methods as these index from the beginning of the blob shape, where legacy parameter blobs were indexed from the end of the blob shape (e.g., bias Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).
    //输入的维度若使用过时的维度信息（num, channels,height, width)，则需要转换为新的vector参数,代码使用了C++中的“懒”逻辑
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  //直接对比
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}
//从另一个Blob对象拷贝data (可选diff),必要时进行变维
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);      //如果要变维,则执行这个
    } else {    //两个blob形状不同,则报错
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:      //GPU模式
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:      //CPU模式
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

//从BlobProto中加载一个Blob,适用于从磁盘载入之前导出的Blob
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {        //从BlobProto对象中获得所需各个维度信息
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      // Using deprecated 4D Blob dimensions --
      // shape is (num, channels, height, width).
      shape.resize(4);
      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }
    Reshape(shape);     //Blob按照维度信息进行变维
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
  }
  // copy data 加载数据
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {   // 如果之前保存的是double类型 data
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);   //加载double date
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);  //否则加载float data
    }
  }
  if (proto.double_diff_size() &gt; 0) {   // 如果之前保存的是 double 类型 diff
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}
//将Blob中的data(可选diff)导出到BlobProto结构体.便于存储到磁盘文件中
template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();     //重置proto的维度,保证与blob相同
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();   //清除data
  proto-&gt;clear_double_diff();   //清除diff
  const double* data_vec = cpu_data();  //将data导出到proto
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {         //  若有write_diff的需求
    const double* diff_vec = cpu_diff();    //将diff导出到proto
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}
//同上,只不过类型为float
template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}
//实例化Blob   类模板（float, double)
INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;

}  // namespace caffe

</code></pre>

<p><strong>到此,我们就了解了Caffe一些基本的数据结构.后面就应该学习Layer层中对数据的一些处理.</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Caffe%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html'>Caffe 数据结构</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14969765744787.html">
                
                  <h1>caffe数据结构</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Blob</a>
</li>
<li>
<a href="#toc_1">Blob的基本用法</a>
</li>
</ul>


<p>一个CNN网络是由多个Layer堆叠而成的.如图所示:<br/>
<img src="media/14966518170972/14966520513325.jpg" alt=""/></p>

<p>caffe按照我们设计的图纸(prototxt),用Blob这些砖块建成一层层(Layer)楼房,最后通过方法SGD方法(Solver)进行简装修(Train),精装修(Finetune)实现的.我们这里就是学习这些基本概念.</p>

<h2 id="toc_0">Blob</h2>

<p>Caffe使用称为Blob的4维数组用于存储和交换数据.Blob提供了统一的存储器接口,持有一批图像或其它数据,权值,权值更新值. 其它机器学习框架也有类似的数据结构.</p>

<p><strong>Blob在内存中为4维数组,分别为<code>(width_,height_,channels_,num_)</code>,width_和height_表示图像的宽和高,channel_表示颜色通道RGB,num_表示第几帧,用于存储数据或权值(data)和权值增量(diff),在进行网路计算时,每层的输入,输出都需要Blob对象缓冲.Blob是Caffe的基本存储单元.</strong></p>

<h2 id="toc_1">Blob的基本用法</h2>

<p>Blob是一个模板类,所以创建对象时需要制定模板参数.我们这里写一个简单的测试程序<code>blob_demo.cpp</code>将它放在<code>caffe</code>的安装目录下:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>上面代码首先创建了整型Blob对象a,打印其维度信息,然后调用其<code>Reshape()</code>方法,再次打印其维度信息.</p>

<p>使用如下命令来编译上面的文件.</p>

<pre><code>g++ -o app blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe
</code></pre>

<p>生成了可执行程序<code>app</code></p>

<p>这个时候运行<code>app</code>的话可能会遇到下面这个错误:<br/>
<img src="media/14966518170972/14968048615065.jpg" alt=""/><br/>
这个因为<code>app</code>没有链接到这个动态库文件,执行下边这个命令链接:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app
</code></pre>

<p><code>/usr/local/Cellar/caffe/build/lib/</code>为<code>@rpath/libcaffe.so.1.0.0</code>动态库的路径.</p>

<p>执行后,再次运行会遇到错误:<br/>
<img src="media/14966518170972/14968049981988.jpg" alt=""/></p>

<p>与上面类似,这是因为没有链接到<code>@rpath/libhdf5_hl.10.dylib</code><br/>
执行下面这个命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/lib/libcaffe.so.1.0.0
</code></pre>

<p>其中<code>/Users/liangzhonghao/anaconda2/lib</code>包含这个库文件.</p>

<p>再次执行app,终于成功了!<br/>
<img src="media/14966518170972/14968051024802.jpg" alt=""/></p>

<p>创建了Blob对象之后,我们可以通过<code>mutable_cpu[gpu]_data[diff]</code>函数来修改其内部数值:</p>

<p>代码为:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    for(int i=0;i&lt;a.count();i++){
        p[i]=i;
    }
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    return 0;
}
</code></pre>

<p>跟上面一样继续编译和执行,这里按照上面的命令继续来编译的话,遇到了一个错误:<br/>
<img src="media/14966518170972/14968061663863.jpg" alt=""/></p>

<p>之后换成下边的命令执行后成功:</p>

<pre><code>g++ -o app2 blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe -lglog -lboost_system -lprotobuf
</code></pre>

<p>差别在于,后边加上了<code>-lglog -lboost_system -lprotobuf</code>命令,具体作用后续将研究(暂时不理解),继续运行后,又出现了错误:<br/>
<img src="media/14966518170972/14968063024883.jpg" alt=""/></p>

<p>同样是动态库的连接问题:<br/>
运行命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app2
</code></pre>

<p>执行命令,然后运行<code>app2</code>.得到输出:<br/>
<img src="media/14966518170972/14968063915684.jpg" alt=""/></p>

<p><strong>可见,Blob下标的访问与c/c++高维数组几乎一致,而Blob好处在于可以直接同步CPU/GPU上的数据.</strong></p>

<p>Blob还支持计算所有元素的绝对值之和(L1-范数),平方和(L2-范数):</p>

<pre><code>cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
</code></pre>

<p>输出结果为:</p>

<pre><code>ASUM = 276
SUMSQ = 4324

</code></pre>

<p>除了data,我们还可以改diff部分,与data的操作基本一致:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //将data初始化为1,2,3....
        q[i]= a.count()-1-i;   //将diff初始化为23,22,21,...
    }
    
    a.Update();         //执行update操作,将diff与data融合,这也是CNN权值更新步骤的最终实施者
   
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
    
    return 0;
}

</code></pre>

<p>然后执行以下命令编译,链接库文件:</p>

<pre><code>g++ -o app blob_demo_diff.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe  -lglog -lboost_system -lprotobuf

install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/LZHcaffe/./app
</code></pre>

<p><img src="media/14969765744787/14969953318311.jpg" alt=""/></p>

<p>运行.app,结果为:<br/>
<img src="media/14969765744787/14969953744774.jpg" alt=""/></p>

<p>上面表明,在<code>Update()</code>函数中,实现了<code>data = data -diff</code>操作,这个主要是在CNN权值更新时会用到,后面继续学习.</p>

<p>将Blob内部值保存到硬盘,或者冲硬盘载入到内存,可以分别通过<code>ToProto(),FromProto()</code>实现:</p>

<pre><code class="language-c++">
#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
#include&lt;caffe/util/io.hpp&gt;   //需要包含这个头文件
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //将data初始化为1,2,3....
        q[i]= a.count()-1-i;   //将diff初始化为23,22,21,...
    }
    
    a.Update();         //执行update操作,将diff与data融合,这也是CNN权值更新步骤的最终实施者
   
    BlobProto bp;          //构造一个BlobProto对象
    a.ToProto(&amp;bp,true);    //将a序列化,连同diff(默认不带)
    WriteProtoToBinaryFile(bp,&quot;a.blob&quot;);     //写入磁盘文件&quot;a.blob&quot;
    BlobProto bp2;           //构造一个新的BlobProto对象
    ReadProtoFromBinaryFileOrDie(&quot;a.blob&quot;,&amp;bp2);    //读取磁盘文件
    Blob&lt;float&gt; b;          //新建一个Blob对象b
    b.FromProto(bp2,true);  //从序列化对象bp2中克隆b(连同形状)
    
    for(int u=0;u&lt;b.num();u++){
        for(int v=0;v&lt;b.channels();v++){
            for(int w=0;w&lt;b.height();w++){
                for(int x=0;x&lt;b.width();x++){
                    cout&lt;&lt;&quot;b[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;b.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;b.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;b.sumsq_data()&lt;&lt;endl;
    
    
    return 0;
}

</code></pre>

<p>编译,连接库文件后(注意编译时末尾加入<code>&quot;-lglog -lboost_system -lprotobuf&quot;</code>选项),输出如下:</p>

<p><img src="media/14969765744787/14969964533804.jpg" alt=""/></p>

<p>可以发现与上面没有差别,只是在文件夹中多了一个<code>Blob.a</code>文件,<strong>所以<code>BlobProto</code>对象实现了硬盘与内存之间的数据通信.可以帮助保存中间权值和数据</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14964511552671.html">
                
                  <h1>激活函数</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>深度神经网络之所以具有丰富的表达能力，除了有深层次的网络之外，还有一个重要因素即非线性处理单元,称为激活函数（Activation Function)或挤压函数（Squashing Function).<strong>所以我们必须要关注怎么在caffe中实现这些函数.</strong></p>

<p>下图是一个神经元模型.\(\varphi(.)\)为激活函数.主要作用是将上一层的输入线性组合结果\(u_k\)动态范围压缩到特定值域(例如[-1,1]).一般来说具备非线性处理单元的深度神经网络(大于等于3层),理论上可以逼近任意函数.<br/>
<img src="media/14964511552671/14964521741597.jpg" alt=""/></p>

<p>其中几个常用的激活函数如下:<br/>
1.Sigmoid函数,值域为(0,1)<br/>
\[<br/>
\varphi(x) = \frac{1}{1+e^{-ax}}<br/>
\]<br/>
<img src="media/14964511552671/14964523348860.jpg" alt=""/></p>

<p>2.tanh函数,值域为(-1,1):<br/>
\[<br/>
\varphi(x) = \frac{1-e^{-2x}}{1+e^{-2x}}<br/>
\]<br/>
<img src="media/14964511552671/14964526906602.jpg" alt=""/></p>

<p>3.ReLu(Rectified Linear Unit，规整化线性单元)函数,值域为\([0,+ \infty)\),是一种非饱和激活函数.<br/>
\[<br/>
\varphi(x) = max(0,x)<br/>
\]<br/>
<img src="media/14964511552671/14964530576767.jpg" alt=""/></p>

<p>远不止上面这些激活函数,随着发展,陆续又出现了很多激活函数.这里不多介绍.后面还要自学很多这类相关知识.</p>

<p>神经网络中最大的问题是梯度消失问题（Gradient Vanishing Problem),这在使用 <code>Sigmoid、tanh</code>等饱和激活函数情况下尤为严重(神经网络进行误差反向传播时，各层都要乘以激活函数的一阶导数\(G=e\cdot \varphi&#39;(x) \cdot x\)),梯度每传递一层都会衰减一次,网络层数较多时,梯度G就会不停的衰减至消失),使得训练网络时收敛极慢,而ReLU这类非饱和激活函数收敛速度就快很多.所以学习网络模型中一般都会选用类似ReLu这种死活函数.</p>

<p>接下来我们学习在caffe用代码实现对应层的计算,包括前向传播计算和反向传播计算.Caffe的所有与激活函数相关的Layer类声明在<code>include/caffe/layers</code>文件夹中分别为<code>sigmoid_layer.hpp,relu_layer.hpp,tanh_layer.hpp</code>,我们将它们统称为<strong>非线性层</strong>,我们重点关注<code>ReLULayer,SigmoidLayer和TanHLayer</code>这三类.</p>

<p>在前面我们测试的LeNet-5模型中使用了ReLu层,我们在<code>example/mnist/lenet_train_test.prototxt</code>中找到描述:</p>

<pre><code>layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
</code></pre>

<p>与卷积层、全连接层最大的不同,就是没有权值相关的参数，描述相对简单。另外两种层没有实际样例，怎么办呢？这时按照我们的Caffe源码阅读方法论.从<code>src/caffe/proto/caffe.proto</code>中获得灵感。</p>

<pre><code class="language-c++">// ReLU层参数
message ReLUParameter {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  // Leaky ReLU参数，我们暂不关心
  optional float negative_slope = 1 [default = 0];
  enum Engine {     //计算引擎选择
    DEFAULT = 0;
    CAFFE = 1;      // Caffe 实现
    CUDNN = 2;      // CUDNN 实现
  }
  optional Engine engine = 2 [default = DEFAULT];
}
</code></pre>

<pre><code class="language-c++">// Sigmoid层参数
message SigmoidParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}

</code></pre>

<pre><code class="language-c++">//  tanh 层参数
message TanHParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}
</code></pre>

<p>非线性层的共同特点就是对前一层blob中的数值逐一进行非线性变换，并放回原blob中。激活函数的类声明如下:</p>

<pre><code class="language-c++">namespace caffe {
//非线性层的鼻祖NeuronLayer，派生于Layer类，特点是输出blob(y)与输入blob(x)尺寸相同

/**
 * @brief An interface for layers that take one blob as input (@f$ x @f$)
 *        and produce one equally-sized blob as output (@f$ y @f$), where
 *        each element of the output depends only on the corresponding input
 *        element.
 */
template &lt;typename Dtype&gt;
class NeuronLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit NeuronLayer(const LayerParameter&amp; param)
     : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline int ExactNumBottomBlobs() const { return 1; }
  virtual inline int ExactNumTopBlobs() const { return 1; }
};

}  // namespace caffe

#endif  // CAFFE_NEURON_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// ReLULayer，派生于NeuronLayer，实现了ReLu激活函数计算

/**
 * @brief Rectified Linear Unit non-linearity @f$ y = \max(0, x) @f$.
 *        The simple max is fast to compute, and the function does not saturate.
 */
template &lt;typename Dtype&gt;
class ReLULayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
 
  /**
   * @param param provides ReLUParameter relu_param,
   *     with ReLULayer options:
   *   - negative_slope (\b optional, default 0).
   *     the value @f$ \nu @f$ by which negative values are multiplied.
   */
  explicit ReLULayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;ReLU&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \max(0, x)
   *      @f$ by default.  If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed outputs are @f$ y = \max(0, x) + \nu \min(0, x) @f$.
   */
   //前向传波函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the ReLU inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            0 &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$ if propagate_down[0], by default.
   *      If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed gradients are @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            \nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$.
   */
   
   //反向传波函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_RELU_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// SigmoidLayer,派生于NeuronLayer，实现了Sigmoid激活函数的计算
/**
 * @brief Sigmoid function non-linearity @f$
 *         y = (1 + \exp(-x))^{-1}
 *     @f$, a classic choice in neural networks.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class SigmoidLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
  explicit SigmoidLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;Sigmoid&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = (1 + \exp(-x))^{-1}
   *      @f$
   */
   
   //前向传播函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y} y (1 - y)
   *      @f$ if propagate_down[0]
   */
   
   //反向传播函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_SIGMOID_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// TanHLayer，派生于NeuronLayer，实现了tanh激活函数计算
/**
 * @brief TanH hyperbolic tangent non-linearity @f$
 *         y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
 *     @f$, popular in auto-encoders.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class TanHLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
  explicit TanHLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;TanH&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
   *      @f$
   */
   
   //前向传播函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y}
   *              \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
   *            = \frac{\partial E}{\partial y} (1 - y^2)
   *      @f$ if propagate_down[0]
   */
   
   //反向传播函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_TANH_LAYER_HPP_
</code></pre>

<p>上面类的声明比较简单,各自声明了Forward和Backward函数.下面对这些函数的实现进行解析.我们首先看下<code>src/caffe/layers/relu_layer.cpp</code>中前向传播函数的实现代码。</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // (只读) 获得输人blob的data指针
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  // (读写）获得输出blob的data指针
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  //获得输入blob元素个数
  const int count = bottom[0]-&gt;count();
  // Leaky ReLU参数，从layer_param中获得，默认为0，即普通ReLU
  Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
  //执行ReLU操作我们姑且认为negative_slop值为0,不考虑Leaky ReLU
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = std::max(bottom_data[i], Dtype(0))
        + negative_slope * std::min(bottom_data[i], Dtype(0));
  }
}
</code></pre>

<p>不出所料，用一层for循环就搞定了,下面我们来看<strong>反向传播函数</strong>的实现代码.</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // 如果需要做反向传播计算
  if (propagate_down[0]) {
    //(只读）获得前一层的data指针
    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
    //(只读) 获得后一层的diff指针
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    //(读写) 获得前一层的diff指针
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    //获得要参计算的元素总数
    const int count = bottom[0]-&gt;count();
    // Leaky ReLU参数，姑且认为是0
    Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
    for (int i = 0; i &lt; count; ++i) {
    // ReLU的导函数就是（bottom_data[i] &gt; 0)，根据求导链式法则，后一层的误差乘以导函数得到前一层的误差
      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)
          + negative_slope * (bottom_data[i] &lt;= 0));
    }
  }
}
</code></pre>

<p>到这里可以看到ReLu计算非常简单(目前如此)</p>

<p>其它激活函数源码,之后也许用的比较少,这里不做多的介绍.</p>

<p>所以,非线性层虽然公式表示较为复杂,但代码实现都非常简洁、直观，只要掌握了基本求导技巧，同样可以推导出非线性层其他类的反向传播公式.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/3</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffe框架学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14957648679184.html">
                
                  <h1>UDP编程</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>TCP是建立可靠连接，并且通信双方都可以以流的形式发送数据。相对TCP，UDP则是<strong>面向无连接</strong>的协议。</p>

<p><font color=red>使用UDP协议时，不需要建立连接，只需要知道对方的IP地址和端口号，就可以直接发数据包。但是，能不能到达就不知道了。</p>

<p>虽然用UDP传输数据不可靠，但它的优点是和TCP比，速度快，对于不要求可靠到达的数据，就可以使用UDP协议。</font></p>

<p>我们来看看如何通过UDP协议传输数据。和TCP类似，使用<mark>UDP的通信双方也分为客户端和服务器</mark>。服务器首先需要绑定端口：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# 绑定端口:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>创建Socket时，<code>SOCK_DGRAM</code>指定了这个Socket的类型是UDP。绑定端口和TCP一样，但是不需要调用<code>listen()</code>方法，而是直接接收来自任何客户端的数据：</p>

<pre><code class="language-py">print &#39;Bind UDP on 9999...&#39;
while True:
    # 接收数据:
    data, addr = s.recvfrom(1024)
    print &#39;Received from %s:%s.&#39; % addr
    s.sendto(&#39;Hello, %s!&#39; % data, addr)
</code></pre>

<p><code>recvfrom()</code>方法返回数据和客户端的地址与端口，这样，服务器收到数据后，直接调用<code>sendto()</code>就可以把数据用UDP发给客户端。</p>

<p>注意这里省掉了多线程，因为这个例子很简单。</p>

<p>客户端使用UDP时，首先仍然创建基于UDP的Socket，然后，不需要调用<code>connect()</code>，直接通过<code>sendto()</code>给服务器发数据：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # 发送数据:
    s.sendto(data, (&#39;127.0.0.1&#39;, 9999))
    # 接收数据:
    print s.recv(1024)
s.close()
</code></pre>

<p>客户端:<br/>
<img src="media/14957648679184/14957678758780.jpg" alt=""/><br/>
服务器:<br/>
<img src="media/14957648679184/14957678951579.jpg" alt=""/><br/>
客户端从服务器接收数据仍然调用<code>recv()</code>方法。</p>

<h2 id="toc_0">小结</h2>

<p>UDP的使用与TCP类似，但是不需要建立连接。此外，服务器绑定UDP端口和TCP端口互不冲突，也就是说，UDP的9999端口与TCP的9999端口可以各自绑定。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/26</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%BB%83%E4%B9%A0.html'>Python练习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14957594179111.html">
                
                  <h1>网络编程</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">TCP/IP</a>
</li>
<li>
<a href="#toc_1">TCP编程</a>
<ul>
<li>
<a href="#toc_2">客户端</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">服务器</a>
</li>
<li>
<a href="#toc_4">小结</a>
</li>
</ul>


<h2 id="toc_0">TCP/IP</h2>

<p>现在的网络编程基本都是在一个统一的通用写一下来进行的,<code>互联网协议簇（Internet Protocol Suite）</code>就是通用协议标准。Internet是由inter和net两个单词组合起来的，原意就是连接“网络”的网络，有了Internet，任何私有网络，只要支持这个协议，就可以联入互联网。</p>

<p>因为互联网协议包含了上百种协议标准，但是最重要的两个协议是TCP和IP协议，所以，大家把互联网的协议简称<strong>TCP/IP</strong>协议。</p>

<p>通信的时候，双方必须知道对方的标识，好比发邮件必须知道对方的邮件地址。互联网上每个计算机的唯一标识就是IP地址，类似123.123.123.123。如果一台计算机同时接入到两个或更多的网络，比如路由器，它就会有两个或多个IP地址，所以，IP地址对应的实际上是计算机的网络接口，通常是网卡。</p>

<p><strong>IP协议负责把数据从一台计算机通过网络发送到另一台计算机。数据被分割成一小块一小块，然后通过IP包发送出去。</strong>由于互联网链路复杂，两台计算机之间经常有多条线路，因此，<strong>路由器就负责决定如何把一个IP包转发出去。IP包的特点是按块发送，途径多个路由，但不保证能到达，也不保证顺序到达。</strong></p>

<p><font color=red>TCP协议则是建立在IP协议之上的。TCP协议负责在两台计算机之间建立可靠连接，保证数据包按顺序到达。TCP协议会通过握手建立连接，然后，对每个IP包编号，确保对方按顺序收到，如果包丢掉了，就自动重发。</font></p>

<p>许多常用的更高级的协议都是建立在TCP协议基础上的，比如用于浏览器的HTTP协议、发送邮件的SMTP协议等。</p>

<p>一个IP包除了包含要传输的数据外，还包含源IP地址和目标IP地址，源端口和目标端口。</p>

<p>端口有什么作用？在两台计算机通信时，只发IP地址是不够的，因为同一台计算机上跑着多个网络程序。一个IP包来了之后，到底是交给浏览器还是QQ，就需要<strong>端口号</strong>来区分。每个网络程序都向操作系统申请唯一的端口号，这样，两个进程在两台计算机之间建立网络连接就需要各自的IP地址和各自的端口号。(在进行tomcat设置的时候,类似也是需要设置端口号.)</p>

<p>一个进程也可能同时与多个计算机建立链接，因此它会申请很多端口。</p>

<h2 id="toc_1">TCP编程</h2>

<p>Socket是网络编程的一个抽象概念。通常我们用一个Socket表示“打开了一个网络链接”，而打开一个Socket需要知道目标计算机的IP地址和端口号，再指定协议类型即可。</p>

<h3 id="toc_2">客户端</h3>

<p>大多数连接都是可靠的TCP连接。创建TCP连接时，主动发起连接的叫客户端，被动响应连接的叫服务器。</p>

<p>我们要创建一个基于TCP连接的Socket，可以这样做：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import socket

s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

s = connect((&#39;www.sina.com.cn&#39;,80))
</code></pre>

<p>创建Socket时，<code>AF_INET</code>指定使用<code>IPv4</code>协议，如果要用更先进的<code>IPv6</code>，就指定为<code>AF_INET6</code>。<code>SOCK_STREAM</code>指定使用面向流的TCP协议，这样，一个<code>Socket</code>对象就创建成功，但是还没有建立连接。</p>

<p>客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号。新浪网站的IP地址可以用域名<code>www.sina.com.cn</code>自动转换到IP地址，但是怎么知道新浪服务器的端口号呢？</p>

<p>答案是作为服务器，提供什么样的服务，端口号就必须固定下来。由于我们想要访问网页，因此新浪提供网页服务的服务器必须把端口号固定在<code>80</code>端口，因为<code>80</code>端口是Web服务的标准端口。其他服务都有对应的标准端口号，例如SMTP服务是25端口，FTP服务是21端口，等等。端口号小于1024的是Internet标准服务的端口，端口号大于1024的，可以任意使用。</p>

<p>因此，我们连接新浪服务器的代码如下：</p>

<pre><code class="language-py">s.connect((&#39;www.sina.com.cn&#39;, 80))
</code></pre>

<p><strong>注意参数是一个tuple，包含地址和端口号。</strong></p>

<p>建立TCP连接后，我们就可以向新浪服务器发送请求，要求返回首页的内容：</p>

<pre><code class="language-py">s.send(&#39;GET / HTTP/1.1\r\nHost: www.sina.com.cn\r\nConnection: close\r\n\r\n&#39;)
</code></pre>

<p>TCP连接创建的是双向通道，双方都可以同时给对方发数据。但是谁先发谁后发，怎么协调，要根据具体的协议来决定。例如，HTTP协议规定客户端必须先发请求给服务器，服务器收到后才发数据给客户端。</p>

<p>发送的文本格式必须符合HTTP标准，如果格式没问题，接下来就可以接收新浪服务器返回的数据了：</p>

<pre><code class="language-py"># 接收数据:
buffer = []
while True:
    # 每次最多接收1k字节:
    d = s.recv(1024)
    if d:
        buffer.append(d)
    else:
        break
data = &#39;&#39;.join(buffer)
</code></pre>

<p>接收数据时，调用<code>recv(max)</code>方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到<code>recv()</code>返回空数据，表示接收完毕，退出循环。</p>

<p>当我们接收完数据后，调用<code>close()</code>方法关闭<code>Socket</code>，这样，一次完整的网络通信就结束了：</p>

<pre><code class="language-py"># 关闭连接:
s.close()
</code></pre>

<p>接收到的数据包括<code>HTTP头</code>和<code>网页本身</code>，我们只需要把HTTP头和网页分离一下，把<code>HTTP头</code>打印出来，网页内容保存到文件：</p>

<pre><code class="language-py">header, html = data.split(&#39;\r\n\r\n&#39;, 1)
print header
# 把接收的数据写入文件:
with open(&#39;sina.html&#39;, &#39;wb&#39;) as f:
    f.write(html)
</code></pre>

<p>现在，只需要在浏览器中打开这个<code>sina.html</code>文件，就可以看到新浪的首页了。</p>

<h2 id="toc_3">服务器</h2>

<p>和客户端编程相比，服务器编程就要复杂一些。</p>

<p>服务器进程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了。</p>

<p>所以，服务器会打开固定端口（比如80）监听，每来一个客户端连接，就创建该Socket连接。由于服务器会有大量来自客户端的连接，所以，服务器要能够区分一个Socket连接是和哪个客户端绑定的。一个Socket依赖4项：服务器地址、服务器端口、客户端地址、客户端端口来唯一确定一个Socket。</p>

<p>但是服务器还需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。</p>

<p>我们来编写一个简单的服务器程序，它接收客户端连接，把客户端发过来的字符串加上<code>Hello</code>再发回去。</p>

<p>首先，创建一个基于IPv4和TCP协议的Socket：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
</code></pre>

<p>然后，我们要绑定监听的地址和端口。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用<code>0.0.0.0</code>绑定到所有的网络地址，还可以用<code>127.0.0.1</code>绑定到本机地址。<code>127.0.0.1</code>是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。</p>

<p>端口号需要预先指定。因为我们写的这个服务不是标准服务，所以用<code>9999</code>这个端口号。请注意，小于<code>1024</code>的端口号必须要有管理员权限才能绑定：</p>

<pre><code class="language-py"># 监听端口:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>紧接着，调用listen()方法开始监听端口，传入的参数指定等待连接的最大数量：<br/>
<code>py<br/>
s.listen(5)<br/>
print &#39;Waiting for connection...&#39;<br/>
</code></p>

<p>接下来，服务器程序通过一个永久循环来接受来自客户端的连接，<code>accept()</code>会等待并返回一个客户端的连接:</p>

<pre><code class="language-py">while True:
    # 接受一个新连接:
    sock, addr = s.accept()
    # 创建新线程来处理TCP连接:
    t = threading.Thread(target=tcplink, args=(sock, addr))
    t.start()
</code></pre>

<p>每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接：</p>

<pre><code class="language-py">def tcplink(sock, addr):
    print &#39;Accept new connection from %s:%s...&#39; % addr
    sock.send(&#39;Welcome!&#39;)
    while True:
        data = sock.recv(1024)
        time.sleep(1)
        if data == &#39;exit&#39; or not data:
            break
        sock.send(&#39;Hello, %s!&#39; % data)
    sock.close()
    print &#39;Connection from %s:%s closed.&#39; % addr
</code></pre>

<p>连接建立后，服务器首先发一条欢迎消息，然后等待客户端数据，并加上<code>Hello</code>再发送给客户端。如果客户端发送了<code>exit</code>字符串，就直接关闭连接。</p>

<p>要测试这个服务器程序，我们还需要编写一个客户端程序：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# 建立连接:
s.connect((&#39;127.0.0.1&#39;, 9999))
# 接收欢迎消息:
print s.recv(1024)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # 发送数据:
    s.send(data)
    print s.recv(1024)
s.send(&#39;exit&#39;)
s.close()
</code></pre>

<p>我们需要打开两个命令行窗口，一个运行服务器程序，另一个运行客户端程序，就可以看到效果了：<br/>
(先运行服务器端程序)<br/>
执行了两次客户端程序:<br/>
<img src="media/14957594179111/14957645115702.jpg" alt=""/><br/>
服务端:<br/>
<img src="media/14957594179111/14957645546966.jpg" alt=""/><br/>
需要注意的是，客户端程序运行完毕就退出了，而服务器程序会永远运行下去，必须按Ctrl+C退出程序。</p>

<h2 id="toc_4">小结</h2>

<p>用TCP协议进行Socket编程在Python中十分简单，对于客户端，要主动连接服务器的IP和指定端口，对于服务器，要首先监听指定端口，然后，对每一个新的连接，创建一个线程或进程来处理。通常，服务器程序会无限运行下去。</p>

<p>同一个端口，被一个Socket绑定了以后，就不能被别的Socket绑定了。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/26</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%BB%83%E4%B9%A0.html'>Python练习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14957005904776.html">
                
                  <h1>图形界面</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>Python支持多种图形界面的第三方库，包括：<br/>
* Tk<br/>
* wxWidgets<br/>
* Qt<br/>
* GTK<br/>
等等.</p>

<p>但是Python自带的库是支持Tk的Tkinter，使用Tkinter，无需安装任何包，就可以直接使用。本章简单介绍如何使用Tkinter进行GUI编程。</p>

<h2 id="toc_0">Tkinter</h2>

<p>我们来梳理一下概念：</p>

<p>我们编写的Python代码会调用内置的Tkinter，Tkinter封装了访问Tk的接口；</p>

<p>Tk是一个图形库，支持多个操作系统，使用Tcl语言开发；</p>

<p>Tk会调用操作系统提供的本地GUI接口，完成最终的GUI。</p>

<p>所以，我们的代码只需要调用Tkinter提供的接口就可以了。</p>

<h2 id="toc_1">第一个GUI程序</h2>

<p>使用Tkinter十分简单，我们来编写一个GUI版本的“Hello, world!”。</p>

<p>第一步是导入Tkinter包的所有内容：</p>

<pre><code class="language-py">from Tkinter import *
</code></pre>

<p>第二步是从<code>Frame</code>派生一个<code>Application</code>类，这是所有Widget的父容器：</p>

<pre><code class="language-py">
class Applicition(Frame):
    def __init__(self, master=None):
        Frame.__init__(self,master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.helloLabel = Label(self,text = &#39;Hello,world&#39;)
        self.helloLabel.pack()
        self.quitButton = Button(self, text = &#39;Quit&#39;,command = self.quit)
        self.quitButton.pack()

</code></pre>

<p>在GUI中，每个Button、Label、输入框等，都是一个Widget。Frame则是可以容纳其他Widget的Widget，所有的Widget组合起来就是一棵树。</p>

<p><code>pack()</code>方法把Widget加入到父容器中，并实现布局。<code>pack()</code>是最简单的布局，<code>grid()</code>可以实现更复杂的布局。</p>

<p>在<code>createWidgets()</code>方法中，我们创建一个<code>Label</code>和一个<code>Button</code>，当Button被点击时，触发<code>self.quit()</code>使程序退出。</p>

<p>第三步，实例化<code>Application</code>，并启动消息循环：</p>

<pre><code class="language-py">app = Application()
# 设置窗口标题:
app.master.title(&#39;Hello World&#39;)
# 主消息循环:
app.mainloop()
</code></pre>

<p>GUI程序的主线程负责监听来自操作系统的消息，并依次处理每一条消息。因此，如果消息处理非常耗时，就需要在新线程中处理。</p>

<p>运行这个GUI程序，可以看到下面的窗口：<br/>
<img src="media/14957005904776/14957017137688.jpg" alt=""/><br/>
点击“Quit”按钮或者窗口的“x”结束程序。</p>

<h2 id="toc_2">输入文本</h2>

<p>我们再对这个GUI程序改进一下，加入一个文本框，让用户可以输入文本，然后点按钮后，弹出消息对话框。</p>

<pre><code class="language-py">from Tkinter import *
import tkMessageBox


class Applicition(Frame):

    def __init__(self, master=None):
        Frame.__init__(self, master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.nameInput = Entry(self)
        self.nameInput.pack()
        self.alertButton = Button(self, text=&#39;Hello&#39;, command=self.hello)
        self.alertButton.pack()

    def hello(self):
        name = self.nameInput.get() or &#39;world&#39;
        tkMessageBox.showinfo(&#39;Message&#39;, &#39;Hello,%s&#39; % name)

app = Applicition()
app.master.title(&#39;Hello World&#39;)
app.mainloop()

</code></pre>

<p><img src="media/14957005904776/14957022189285.jpg" alt=""/></p>

<p>当用户点击按钮时，触发<code>hello()</code>，通过<code>self.nameInput.get()</code>获得用户输入的文本后，使用<code>tkMessageBox.showinfo()</code>可以弹出消息对话框。</p>

<h2 id="toc_3">小结</h2>

<p>Python内置的Tkinter可以满足基本的GUI程序的要求，如果是非常复杂的GUI程序，建议用操作系统原生支持的语言和库来编写。</p>

<p>源码参考：<a href="https://github.com/michaelliao/learn-python/tree/master/gui">https://github.com/michaelliao/learn-python/tree/master/gui</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%BB%83%E4%B9%A0.html'>Python练习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_23.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_25.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="/asset/img/logn.png" /></div>
            
                <h1>LZH007</h1>
                <div class="site-des">LZH的技术杂事小博客~</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/lockxmonk" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lzhabc007@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="MAC%20OS.html"><strong>MAC OS</strong></a>
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>统计学习方法</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Python练习</strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>图像去雾技术</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html"><strong>算法学习</strong></a>
        
            <a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html"><strong>常见面试问题</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15129821006078.html">Sort Characters By Frequency</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15129806433168.html">First Unique Character in a String</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15129796221707.html">Minimum Moves to Equal Array Elements II</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15129788527282.html">Minimum Moves to Equal Array Elements</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15129760305793.html">Range Addition II</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
          <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1265629731'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s22.cnzz.com/z_stat.php%3Fid%3D1265629731%26online%3D1' type='text/javascript'%3E%3C/script%3E"));</script>    
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2017
Powered by <a target="_blank" href="https://lockxmonk.github.io/index.html">LZH</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
