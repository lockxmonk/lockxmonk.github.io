<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="MAC%20OS.html">MAC OS</a></li>
        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">æœºå™¨å­¦ä¹ </a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Pythonç»ƒä¹ </a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">å›¾åƒå»é›¾æŠ€æœ¯</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14980252836745.html">
                
                  <h1>MACä¸‹openBlasçš„å®‰è£…</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>è¿™é‡Œæˆ‘ä»¬å®‰è£…openblasä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•:</p>

<ol>
<li>é€šè¿‡gitä»£ç åˆ°æœ¬åœ°å¹¶å®‰è£…</li>
<li>é€šè¿‡brewæ¥å®‰è£…</li>
</ol>

<h2 id="toc_0">gitåˆ°æœ¬åœ°ç¼–è¯‘å®‰è£….</h2>

<ol>
<li>gitä»£ç åˆ°æœ¬åœ°å¹¶å®‰è£…</li>
</ol>

<pre><code>git clone https://github.com/xianyi/OpenBLAS.git
cd OpenBLAS
make -j4
make install

</code></pre>

<ol>
<li>ä¿®æ”¹Caffeçš„<code>Makefile.config</code></li>
</ol>

<pre><code>BLAS := open
BLAS_INCLUDE :=  /opt/OpenBLAS/include
BLAS_LIB := /opt/OpenBLAS/lib
</code></pre>

<ol>
<li>caffeé‡æ–°make</li>
</ol>

<pre><code>make clean
make pycaffe
make all -j4
make test &amp;&amp; runtest
</code></pre>

<h2 id="toc_1">ä½¿ç”¨brewè¿›è¡Œå®‰è£…</h2>

<ol>
<li><code>brew uninstall openblas; brew install --fresh -vd openblas.</code>è¿è¡Œä¸Šé¢å‘½ä»¤å®‰è£…openblas</li>
<li>æ›´æ”¹<code>Makefile.config</code></li>
</ol>

<pre><code>
# Homebrew puts openblas in a directory that is not on the standard search path
BLAS_INCLUDE := $(shell brew --prefix openblas)/include
BLAS_LIB := $(shell brew --prefix openblas)/lib

</code></pre>

<ol>
<li>é‡æ–°ç¼–è¯‘</li>
</ol>

<p><strong>å¦‚æœæœ‰äº›äººé‡åˆ°äº†è¿™ç§é”™è¯¯</strong></p>

<pre><code>In file included from src/caffe/util/blocking_queue.cpp:5:
In file included from ./include/caffe/layers/base_data_layer.hpp:9:
In file included from ./include/caffe/layer.hpp:12:
In file included from ./include/caffe/util/math_functions.hpp:11:
./include/caffe/util/mkl_alternate.hpp:14:10: fatal error: &#39;cblas.h&#39; file not found
#include &lt;cblas.h&gt;
         ^
1 error generated.
make: *** [.build_release/src/caffe/util/blocking_queue.o] Error 1
</code></pre>

<p>å¯ä»¥è¯•è¯•è¿™ä¸ªå‘½ä»¤:</p>

<pre><code>cmake -DCMAKE_CXX_FLAGS=-I/usr/local/opt/openblas/include ..
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14980135145677.html">
                
                  <h1>Caffeä¸­çš„Net</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Netçš„åŸºæœ¬ç”¨æ³•</a>
</li>
</ul>


<p>Netåœ¨Caffeä¸­ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„CNNæ¨¡å‹ï¼Œå®ƒåŒ…å«è‹¥å¹²Layerå®ä¾‹ã€‚å‰é¢æˆ‘ä»¬å·²ç»åœ¨ç¬¬5å¤©å†…å®¹ä¸­çœ‹åˆ°ç”¨ProtoBufferæ–‡æœ¬æ–‡ä»¶ï¼ˆprototxt)æè¿°çš„ç»å…¸ç½‘ç»œç»“æ„å¦‚LeNetã€AlexNet,è¿™äº›ç»“æ„åæ˜ åœ¨Caffeä»£ç å®ç°ä¸Šå°±æ˜¯ä¸€ä¸ªNetå¯¹è±¡ã€‚<strong>Netå…¶å®æ˜¯ç›¸å¯¹Blobã€ Layeræ›´ä¸ºå¤æ‚çš„è®¾è®¡ï¼Œéœ€è¦æ²‰ä½æ°”</strong>ã€‚</p>

<h2 id="toc_0">Netçš„åŸºæœ¬ç”¨æ³•</h2>

<p>Netæ˜¯ä¸€å¼ å›¾çº¸ï¼Œå¯¹åº”çš„æè¿°æ–‡ä»¶ä¸º<code>*.prototxt</code>,æˆ‘ä»¬é€‰æ‹©Caffeè‡ªå¸¦çš„CaffeNetæ¨¡å‹æè¿°æ–‡ä»¶,ä½äº<code>models/bvlc_reference_caffenet/deploy.prototxt</code>ã€‚å°†è¯¥æ–‡ä»¶æ‹·è´åˆ°å½“å‰å·¥ä½œç›®å½•ä¸‹ã€‚</p>

<p>ç¼–å†™æµ‹è¯•ä»£ç ä¸º:</p>

<pre><code class="language-c++">#include &lt;vector&gt;
#include &lt;iostream&gt;
#include &lt;caffe/net.hpp&gt;
using namespace caffe;
using namespace std;

int main(void)
{
    std::string proto(&quot;deploy.prototxt&quot;);
    Net&lt;float&gt; nn(proto,caffe::TEST);
    vector&lt;string&gt; bn = nn.blob_names();    // è·å– Net ä¸­æ‰€æœ‰ Blob å¯¹è±¡å
    vector&lt;string&gt; ln = nn.layer_names();   // è·å– Net ä¸­æ‰€æœ‰ Layer å¯¹è±¡å
    for (int i = 0; i &lt; bn.size(); i++)
    {
        cout&lt;&lt;&quot;Blob #&quot;&lt;&lt;i&lt;&lt;&quot; : &quot;&lt;&lt;bn[i]&lt;&lt;endl;
    }
    for (int i = 0; i &lt; ln.size(); i++)
    {
        cout&lt;&lt;&quot;layer #&quot;&lt;&lt;i&lt;&lt;&quot; : &quot;&lt;&lt;ln[i]&lt;&lt;endl;
    }
    return 0;
}
</code></pre>

<p>ç¼–è¯‘(<strong>æ³¨æ„è¿™é‡Œæˆ‘ä»¬éœ€è¦å®‰è£…openblas,å…·ä½“è¿‡ç¨‹ä¸å†è¿™é‡Œè®²è¿°</strong>):</p>

<pre><code>g++ -o netapp net_demo.cpp -I /usr/local/Cellar/caffe/include -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/build/lib -I /usr/local/Cellar/openblas/0.2.19_1/include  -lcaffe -lglog -lboost_system -lprotobuf
</code></pre>

<p><strong>æ³¨æ„åˆ°è¿™é‡Œæœ‰ä¸€æ®µ<code>-I /usr/local/Cellar/openblas/0.2.19_1/include</code>è¿™æ˜¯ä¸ºäº†è¿æ¥åˆ°æœ¬åœ°çš„<code>blas</code>åº“</strong></p>

<p>è¿è¡Œ ./netapp :</p>

<p>å‘ç°åˆæŠ¥é”™äº†:<br/>
<img src="media/14980135145677/14981000832074.jpg" alt=""/><br/>
è¿è¡Œ<code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/LZHcaffe/./netapp</code>å‘½ä»¤,è¿æ¥åº“æ–‡ä»¶.</p>

<p>è¿è¡ŒæˆåŠŸå,è¾“å‡ºä¸º:</p>

<pre><code>...çœç•¥ä¸Šé¢éƒ¨åˆ†,ä¹‹å‰å·²ç»è§è¿‡äº†
I0622 11:03:24.688719 3012948928 net.cpp:255] Network initialization done.
Blob #0 : data
Blob #1 : conv1
Blob #2 : pool1
Blob #3 : norm1
Blob #4 : conv2
Blob #5 : pool2
Blob #6 : norm2
Blob #7 : conv3
Blob #8 : conv4
Blob #9 : conv5
Blob #10 : pool5
Blob #11 : fc6
Blob #12 : fc7
Blob #13 : fc8
Blob #14 : prob
layer #0 : data
layer #1 : conv1
layer #2 : relu1
layer #3 : pool1
layer #4 : norm1
layer #5 : conv2
layer #6 : relu2
layer #7 : pool2
layer #8 : norm2
layer #9 : conv3
layer #10 : relu3
layer #11 : conv4
layer #12 : relu4
layer #13 : conv5
layer #14 : relu5
layer #15 : pool5
layer #16 : fc6
layer #17 : relu6
layer #18 : drop6
layer #19 : fc7
layer #20 : relu7
layer #21 : drop7
layer #22 : fc8
layer #23 : prob

</code></pre>

<p>é€šè¿‡ä¸Šé¢çš„ç®€å•ä¾‹å­,æˆ‘ä»¬å‘ç°Netä¸­æ—¢åŒ…æ‹¬Layerå¯¹è±¡,æœ‰åŒ…æ‹¬Blobå¯¹è±¡.å…¶ä¸­Blobå¯¹è±¡ç”¨äºå­˜æ”¾æ¯ä¸ªLayerè¾“å…¥/è¾“å‡ºä¸­é—´ç»“æœ.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14974936829967.html">
                
                  <h1>Caffeä¸­Layerçš„å­¦ä¹ </h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>Layeræ˜¯Caffeçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œè‡³å°‘æœ‰ä¸€ä¸ªè¾“å…¥Blob (Bottom Blob)å’Œä¸€ä¸ªè¾“å‡ºBlob (Top Blob),éƒ¨åˆ†Layerå¸¦æœ‰æƒå€¼(Weight)å’Œåç½®é¡¹ï¼ˆBias),<strong>æœ‰ä¸¤ä¸ªè¿ç®—æ–¹å‘</strong>ï¼šå‰å‘ä¼ æ’­ï¼ˆForward)å’Œåå‘ä¼ æ’­ï¼ˆBackward)ï¼Œå…¶ä¸­å‰å‘ä¼ æ’­è®¡ç®—ä¼šå¯¹è¾“å…¥Blobè¿›è¡ŒæŸç§å¤„ç†ï¼ˆå­˜æƒå€¼å’Œåç½®é¡¹çš„Layerä¼šåˆ©ç”¨è¿™äº›å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼‰,å¾—åˆ°è¾“å‡ºBlob;è€Œåå‘ä¼ æ’­è®¡ç®—åˆ™å¯¹è¾“å‡ºBlobçš„diffè¿›è¡ŒæŸç§å¤„ç†ï¼Œå¾—åˆ°è¾“å…¥Blobçš„diff(æœ‰æƒå€¼å’Œåç½®é¡¹çš„Layerå¯èƒ½ä¹Ÿä¼šè®¡ç®—æƒå€¼Blobã€åç½®é¡¹Blobçš„diff)ã€‚</p>

<h2 id="toc_0">layerä¸­çš„æ•°æ®ç»“æ„æè¿°</h2>

<p>æˆ‘ä»¬å¯ä»¥æœç´¢caffeä¸­å…³äº<code>message LayerParameter</code>çš„ç±»,æ¥äº†è§£.<br/>
å¦‚æœä½ ä¸€å¼€å§‹æ‰¾ä¸åˆ°è¿™ä¸ªç±»åœ¨é‚£ä¸ªæ–‡ä»¶æè¿°,å¯ä»¥ç”¨ä¸‹é¢è¿™ä¸ªå‘½ä»¤å»æœç´¢:</p>

<pre><code>âœ  caffe git:(master) âœ— grep -n -H -R &quot;message LayerParameter&quot; *
</code></pre>

<p><img src="media/14974936829967/14974948024735.jpg" alt=""/><br/>
å¾—åˆ°å®ƒçš„è·¯å¾„.</p>

<p>æˆ‘ä»¬å‘ç°æ˜¯åœ¨<code>src/caffe/proto/caffe.proto</code>è¿™ä¸ªè·¯å¾„ä¸­.å› ä¸ºcaffeä½¿ç”¨<code>google_protobuf</code>æ•°æ®ç±»å‹æ¥å£°æ˜layer.å…³äº<code>google_protobuf</code>çš„ç›¸å…³å†…å®¹,ä¹‹åå¯ä»¥ç ”ç©¶ä¸€ä¸‹.</p>

<p>è¿™é‡Œæˆ‘ä»¬çœ‹ä¸€ä¸‹æºç :</p>

<pre><code class="language-protobuf">//æ³¨æ„ï¼šå¦‚æœä½ å¢åŠ äº†1ä¸ªæ–°çš„LayerParameteråŸŸï¼Œä¸€å®šè®°å¾—æ›´æ–°ä¸€ä¸ªå¯ç”¨ID
// LayerParameter ä¸‹ä¸€ä¸ªlayer-specific ID: 147 (last added: recurrent_param)
message LayerParameter {
  optional string name = 1; // the layer name
  optional string type = 2; // the layer type
  repeated string bottom = 3; // è¾“å…¥Blob(bottom Blob)çš„åç§°
  repeated string top = 4; // è¾“å‡ºBlob(Top Blob)çš„åç§°

  // å½“å‰è®¡ç®—é˜¶æ®µï¼ˆTRAIN æˆ– TEST)
    optional Phase phase = 10;

  // ä¸ºæ¯ä¸ªTop Blobåˆ†é…å¯¹æŸå¤±å‡½æ•°çš„æƒé‡ï¼Œæ¯ä¸ªLayeréƒ½æœ‰é»˜è®¤å€¼ï¼Œè¦ä¹ˆä¸º0,è¡¨ç¤ºä¸å‚ä¸ç›®æ ‡å‡½æ•°è®¡ç®—ï¼šè¦ä¹ˆä¸º1ï¼Œè¡¨ç¤ºå‚ä¸æŸå¤±å‡½æ•°è®¡ç®—
  repeated float loss_weight = 5;

  // æŒ‡å®šè®­ç»ƒå‚æ•°ï¼ˆä¾‹å¦‚ç›¸å¯¹å…¨å±€å­¦ä¹ å¸¸æ•°çš„ç¼©æ”¾å› å­ï¼Œä»¥åŠç”¨äºæƒå€¼å…±äº« çš„åç§°æˆ–å…¶ä»–è®¾ç½®)
  repeated ParamSpec param = 6;

  // æ‰¿è½½äº†è¯¥å±‚æ•°å€¼å‚æ•°çš„Blob
  repeated BlobProto blobs = 7;
  //æ˜¯å¦å¯¹Bottom Blobè¿›è¡Œåå‘ä¼ æ’­è¿‡ç¨‹ã€‚è¯¥å­—æ®µçš„ç»´åº¦åº”ä¸ Bottom Blobä¸ªæ•°ä¸€è‡´
  // Specifies whether to backpropagate to each bottom. If unspecified,
  // Caffe will automatically infer whether each input needs backpropagation
  // to compute parameter gradients. If set to true for some inputs,
  // backpropagation to those inputs is forced; if set false for some inputs,
  // backpropagation to those inputs is skipped.
  //
  // The size must be either 0 or equal to the number of bottoms.
  repeated bool propagate_down = 11;

 //æ§åˆ¶æŸä¸ªå±‚åœ¨æŸä¸ªæ—¶åˆ»æ˜¯å¦åŒ…å«åœ¨ç½‘ç»œä¸­ï¼ŒåŸºäºå½“å‰NetStateã€‚ä½ å¯ä»¥ä¸ºincludeæˆ–exclude(ä¸è¦åŒæ—¶ï¼‰æŒ‡å®šéé›¶å€¼ã€‚å¦‚æœæ²¡æœ‰ä»»ä½•è§„åˆ™ï¼Œé‚£ä¹ˆè¯¥å±‚ä¸€ç›´åŒ…å«åœ¨ç½‘ç»œä¸­ï¼šå¦‚æœå½“å‰NetStateæ»¡è¶³äº†ä»»ä½•1ä¸ªæŒ‡å®šè§„åˆ™ï¼Œè€¶ä¹ˆè¯¥å±‚ä¼šè¢«åŒ…å«æˆ–æ’æ–¥
  // Rules controlling whether and when a layer is included in the network,
  // based on the current NetState.  You may specify a non-zero number of rules
  // to include OR exclude, but not both.  If no include or exclude rules are
  // specified, the layer is always included.  If the current NetState meets
  // ANY (i.e., one or more) of the specified rules, the layer is
  // included/excluded.
  repeated NetStateRule include = 8;
  repeated NetStateRule exclude = 9;

  // Parameters for data pre-processing.æ•°æ®é¢„å¤„ç†å‚æ•°
  optional TransformationParameter transform_param = 100;

  // Parameters shared by loss layers.æ‰€æœ‰æŸå¤±å±‚å…±äº«çš„å‚æ•°
  optional LossParameter loss_param = 101;
  
  
  //ç‰¹å®šç±»å‹å±‚çš„å‚æ•°ã€‚æ³¨æ„ä¸€äº›å±‚å®ç°æ—¶å¯èƒ½æœ‰å¤šäºä¸€ç§çš„è®¡ç®—å¼•æ“ï¼Œè¿™äº›å±‚åŒ…æ‹¬ä¸€ä¸ªå¼•æ“ç±»å‹å’Œå¼•æ“å‚æ•°æ¥é€‰æ‹©å®ç°.é»˜è®¤å¼•æ“æ˜¯åœ¨ç¼–è¯‘é˜¶æ®µç”±å¼•æ“å¼€å…³è®¾ç½®çš„
  // Layer type-specific parameters.
  //
  // Note: certain layers may have more than one computational engine
  // for their implementation. These layers include an Engine type and
  // engine parameter for selecting the implementation.
  // The default for the engine is set by the ENGINE switch at compile-time.
  optional AccuracyParameter accuracy_param = 102;
  optional ArgMaxParameter argmax_param = 103;
  optional BatchNormParameter batch_norm_param = 139;
  optional BiasParameter bias_param = 141;
  optional ConcatParameter concat_param = 104;
  optional ContrastiveLossParameter contrastive_loss_param = 105;
  optional ConvolutionParameter convolution_param = 106;
  optional CropParameter crop_param = 144;
  optional DataParameter data_param = 107;
  optional DropoutParameter dropout_param = 108;
  optional DummyDataParameter dummy_data_param = 109;
  optional EltwiseParameter eltwise_param = 110;
  optional ELUParameter elu_param = 140;
  optional EmbedParameter embed_param = 137;
  optional ExpParameter exp_param = 111;
  optional FlattenParameter flatten_param = 135;
  optional HDF5DataParameter hdf5_data_param = 112;
  optional HDF5OutputParameter hdf5_output_param = 113;
  optional HingeLossParameter hinge_loss_param = 114;
  optional ImageDataParameter image_data_param = 115;
  optional InfogainLossParameter infogain_loss_param = 116;
  optional InnerProductParameter inner_product_param = 117;
  optional InputParameter input_param = 143;
  optional LogParameter log_param = 134;
  optional LRNParameter lrn_param = 118;
  optional MemoryDataParameter memory_data_param = 119;
  optional MVNParameter mvn_param = 120;
  optional ParameterParameter parameter_param = 145;
  optional PoolingParameter pooling_param = 121;
  optional PowerParameter power_param = 122;
  optional PReLUParameter prelu_param = 131;
  optional PythonParameter python_param = 130;
  optional RecurrentParameter recurrent_param = 146;
  optional ReductionParameter reduction_param = 136;
  optional ReLUParameter relu_param = 123;
  optional ReshapeParameter reshape_param = 133;
  optional ScaleParameter scale_param = 142;
  optional SigmoidParameter sigmoid_param = 124;
  optional SoftmaxParameter softmax_param = 125;
  optional SPPParameter spp_param = 132;
  optional SliceParameter slice_param = 126;
  optional TanHParameter tanh_param = 127;
  optional ThresholdParameter threshold_param = 128;
  optional TileParameter tile_param = 138;
  optional WindowDataParameter window_data_param = 129;
}

</code></pre>

<h2 id="toc_1">Layeræ˜¯æ€ä¹ˆç‚¼æˆçš„</h2>

<p>Layerå¤´æ–‡ä»¶ä½äº<code>include/caffe/layer.hpp</code>ä¸­ï¼Œæˆ‘ä»¬æ¥è§£æä¸€ä¸‹: </p>

<pre><code class="language-c++">#ifndef CAFFE_LAYER_H_
#define CAFFE_LAYER_H_

#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/layer_factory.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

/**
 Forward declare boost::thread instead of including boost/thread.hpp
 to avoid a boost/NVCC issues (#1009, #1010) on OSX.
 */
namespace boost { class mutex; }

namespace caffe {

/**
 * @brief An interface for the units of computation which can be composed into a
 *        Net.
 *
 * Layer%s must implement a Forward function, in which they take their input
 * (bottom) Blob%s (if any) and compute their output Blob%s (if any).
 * They may also implement a Backward function, in which they compute the error
 * gradients with respect to their input Blob%s, given the error gradients with
 * their output Blob%s.
 */
template &lt;typename Dtype&gt;
class Layer {
 public:
  /**
   * You should not implement your own constructor. Any set up code should go
   * to SetUp(), where the dimensions of the bottom blobs are provided to the
   * layer.
   */
   //æ˜¾å¼æ„é€ å‡½æ•°ï¼Œä»LayerParameterå¯¹è±¡ä¸­åŠ è½½é…ç½®
  explicit Layer(const LayerParameter&amp; param)
    : layer_param_(param) {
      // Set phase(è®­ç»ƒ/é¢„æµ‹) and copy blobs (if there are any).
      phase_ = param.phase();
      if (layer_param_.blobs_size() &gt; 0) {
        //æŒ‰ layer_param_è®¾ç½®æœ¬èº«Blobå¯¹è±¡ä¸ªæ•°ï¼Œå¹¶ä¾æ¬¡å°†æ¯ä¸ªBlobå¯¹è±¡å°ºå¯¸è°ƒæ•´ä¸ºä¸layer_param_ä¸­çš„Blobå°ºå¯¸ä¸€è‡´
        blobs_.resize(layer_param_.blobs_size());
        for (int i = 0; i &lt; layer_param_.blobs_size(); ++i) {
          blobs_[i].reset(new Blob&lt;Dtype&gt;());
          blobs_[i]-&gt;FromProto(layer_param_.blobs(i));
        }
      }
    }
    //ææ„å‡½æ•°
  virtual ~Layer() {}

  /**
   * @brief Implements common layer setup functionality.
   *
   * @param bottom the preshaped input blobs
   * @param top
   *     the allocated but unshaped output blobs, to be shaped by Reshape
   *
   * Checks that the number of bottom and top blobs is correct.
   * Calls LayerSetUp to do special layer setup for individual layer types,
   * followed by Reshape to set up sizes of top blobs and internal buffers.
   * Sets up the loss weight multiplier blobs for any non-zero loss weights.
   * This method may not be overridden.
   */
   
   //é…ç½®å‡½æ•°,å®ç°å¸¸ç”¨å±‚é…ç½®æ¥å£ï¼Œä¸å¯è¢«è¦†ç›–
  void SetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    CheckBlobCounts(bottom, top);   //æ£€æŸ¥Blob
    LayerSetUp(bottom, top);        //  ä¸å±‚ç±»å‹ç›¸å…³çš„é…ç½®è¿‡ç¨‹
    Reshape(bottom, top);       //å¯¹Top Blobå˜å½¢
    SetLossWeights(top);        //è®¾ç½®æŸå¤±æƒå€¼å› å­Blob
  }

  /**
   * @brief Does layer-specific setup: your layer should implement this function
   *        as well as Reshape.
   *
   * @param bottom
   *     the preshaped input blobs, whose data fields store the input data for
   *     this layer
   * @param top
   *     the allocated but unshaped output blobs
   *
   * This method should do one-time layer specific setup. This includes reading
   * and processing relevent parameters from the &lt;code&gt;layer_param_&lt;/code&gt;.
   * Setting up the shapes of top blobs and internal buffers should be done in
   * &lt;code&gt;Reshape&lt;/code&gt;, which will be called before the forward pass to
   * adjust the top blob sizes.
   */
   
   //å±‚é…ç½®ï¼ˆè™šï¼‰å‡½æ•°ï¼Œåšç‰¹å®šç±»å‹å±‚ç›¸å…³çš„é…ç½®ï¼Œç”±è¯¥ç±»å‹å±‚è‡ªå·±å®ç°
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {}
   
   //å˜å½¢ï¼ˆçº¯è™šï¼‰å‡½æ•°ï¼Œä¿®æ”¹Top Blobä»¥åŠå†…éƒ¨Blobç¼“å†²åŒºçš„å½¢çŠ¶
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = 0;

 
   //å‰å‘ä¼ æ’­å‡½æ•°,ç»™å®šBottom Blob,è®¡ç®—Top Blobå’Œloss,è¿”å›å€¼ä¸ºå½“å‰å±‚loss
   //è¯¥å‡½æ•°ä¼šè°ƒç”¨ç›¸åº”è®¾è£•åŒ…è£…é—²æ•°ï¼Œå¦‚Forward_cpuæˆ–Forward_gpuæ¥å®ç°çœŸæ­£çš„è®¡ç®—è¿‡ç¨‹ã€‚å¦‚æœè¯¥å±‚æœ‰ä»»æ„éé›¶loss_weightså‚æ•°ï¼Œé‚£ä¹ˆåŒ…è£…å‡½æ•°ä¼šè®¡ç®—å¹¶è¿”å›loss
   //æ´¾ç”Ÿç±»åº”è¯¥å®ç°Forward_cpuå’ŒForward_gpu (å¯é€‰ã€‰
  inline Dtype Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  //åå‘ä¼ æ’­å‡½æ•°ï¼Œç»™å®šTop Blobè¯¯å·®æ¢¯åº¦ï¼Œæ±ç®—Bottom Blobè¯¯å·®æ¢¯åº¦
  //å‚æ•°è¯´æ˜:
  // top-Top Blobï¼Œå…¶diffåŸŸåŒ…å«æ¥è‡ªä¸Šä¸€å±‚çš„è¯¯å·®æ¢¯åº¦
  // propagate_down -- å¤šè·¯å¹µå…³ï¼Œä¸Bottom BlobçŸ¢é‡ç»´åº¦ç›¸é—®ï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºæ˜¯å¦å°†è¯¯å·®æ¢¯åº¦ä¼ é€’åˆ°å¯¹åº”çš„ Bottom Blob
  // bottomâ€”Bottom Blobï¼Œå…¶diffåŸŸéœ€è¦ç”±è¯¥å‡½æ•°è®¡ç®—å¾—åˆ°
  // è¯¥å‡½æ•°ä¼šè°ƒç”¨ç›¸åº”è®¾å¤‡åŒ…è£…å‡½æ•°ï¼Œå¦‚Backward_cpuæˆ–Backward_gpuæ¥å®ç°çœŸæ­£çš„è®¡ç®—è¿‡ç¨‹ï¼Œç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°
  inline void Backward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);

  //è¿”å›Layerå†…éƒ¨å¯è®­ç»ƒçš„æƒå€¼ã€åç½®é¡¹Blobå‘é‡
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() {
    return blobs_;
  }

  //è¿”å›Layeråˆå§‹åŒ–å‚æ•°ï¼ˆç”±ProtoBufferæä¾›)
  const LayerParameter&amp; layer_param() const { return layer_param_; }

  //å°†Layeråˆå§‹åŒ–å‚æ•°å†™å…¥ProtoBufferç¼“å†²åŒº
  virtual void ToProto(LayerParameter* param, bool write_diff = false);

  //è¿”å›ä¸æŸä¸ªTop Blobç›¸å…³çš„æ ‡é‡losså€¼
  inline Dtype loss(const int top_index) const {
    return (loss_.size() &gt; top_index) ? loss_[top_index] : Dtype(0);
  }

  //è®¾ç½®ä¸æŸä¸ªTop Blobç›¸å…³çš„æ ‡é‡losså€¼
  inline void set_loss(const int top_index, const Dtype value) {
    if (loss_.size() &lt;= top_index) {
      loss_.resize(top_index + 1, Dtype(0));
    }
    loss_[top_index] = value;
  }

  //è¿”å›å±‚ç±»å‹å­—ç¬¦ä¸²,ä¾¿äºè¯†åˆ¥,ç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°
  virtual inline const char* type() const { return &quot;&quot;; }

 //è¿”å›è¯¥Layeréœ€è¦çš„è¾“å…¥Blobæ•°ç›®.-1è¡¨ç¤ºä¸å…³å¿ƒã€‚ç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°
  virtual inline int ExactNumBottomBlobs() const { return -1; }

  virtual inline int MinBottomBlobs() const { return -1; }
  
  virtual inline int MaxBottomBlobs() const { return -1; }
  //è¿”å›è¯¥Layeréœ€è¦çš„è¾“å‡ºBlobæ•°ç›®.-1è¡¨ç¤ºä¸å…³å¿ƒã€‚ç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°
  virtual inline int ExactNumTopBlobs() const { return -1; }
 
  virtual inline int MinTopBlobs() const { return -1; }
  
  virtual inline int MaxTopBlobs() const { return -1; }
  
  //è¿”å›è¯¥Layeræ˜¯å¦æœ‰ç›¸åŒçš„è¾“å…¥/è¾“å‡ºBlobï¼Œç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°
  virtual inline bool EqualNumBottomTopBlobs() const { return false; }

  //è¿”å›æ˜¯å¦å…è®¸åŒ¿åTop Blob,å³ç”±è¯¥Layerè‡ªåŠ¨åˆ›å»ºã€‚è‹¥ä¸ºçœŸï¼Œåœ¨Net::Init()å‡½æ•°ä¸­ä¼šåˆ›å»ºè¶³å¤Ÿå¤šçš„åŒ¿åTop Blobæ¥æ»¡è¶³è¯¥ Layer ExactNumTopBlobs()ã€MinTopBlobs()éœ€æ±‚
  virtual inline bool AutoTopBlobs() const { return false; }

  //è¿”å›æŸäº›Bottom Blobè¶³å¦å…è®¸å¼ºåˆ¶åå‘ä¼ æ’­ï¼Œå¦‚æœAllowForceBackward(i) === false,å°†ä¼šå¿½ç•¥ force_backward è®¾å®š
  virtual inline bool AllowForceBackward(const int bottom_index) const {
    return true;
  }

  //æŒ‡å®šè¯¥Layeræ˜¯å¦è®¡ç®—ç›¸å¯¹æƒå€¼æˆ–åç½®é¡¹çš„æ¢¯åº¦ï¼Œå…·ä½“ç›¸å¯¹è°ç”±param_idæŒ‡å®š
  inline bool param_propagate_down(const int param_id) {
    return (param_propagate_down_.size() &gt; param_id) ?
        param_propagate_down_[param_id] : false;
  }
  
  //è®¾ç½®è¯¥Layeræ˜¯å¦è®¡ç®—ç›¸å¯¹æƒå€¼æˆ–åç½®é¡¹çš„æ¢¯åº¦ï¼Œå…·ä½“ç›¸å¯¹è°ç”±param_idæŒ‡å®š
  inline void set_param_propagate_down(const int param_id, const bool value) {
    if (param_propagate_down_.size() &lt;= param_id) {
      param_propagate_down_.resize(param_id + 1, true);
    }
    param_propagate_down_[param_id] = value;
  }


 protected:
  /** The protobuf that stores the layer parameters */
  LayerParameter layer_param_;
  /** å½“å‰æ‰€å¤„é˜¶æ®µ: TRAIN or TEST */
  Phase phase_;
  /** The vector that stores the learnable parameters as a set of blobs. */
  //Layer å†…éƒ¨æƒå€¼æˆ–åç½®é¡¹ï¼Œä»¥ Blob æ–¹å¼ç»„ç»‡
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;
  /** Vector indicating whether to compute the diff of each param blob. */
  //æ ‡å¿—ä½ï¼Œæ˜¯å¦è®¡ç®—å¯¹åº”å‚æ•°çš„è¯¯å·®æ¢¯åº¦
  vector&lt;bool&gt; param_propagate_down_;

  //æ ‡å¿—ä½ï¼Œåœ¨ç›®æ ‡å‡½æ•°ä¸­ï¼Œæ˜¯å¦æ¯ä¸ªTop Blobéƒ½æœ‰éé›¶æƒé‡
  vector&lt;Dtype&gt; loss_;

//ä¸‹é¢4ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬ä¼šåœ¨å„ä¸ªLayeræ´¾ç”Ÿç±»ä¸­ç»å¸¸çœ‹åˆ°

  /** @brief Using the CPU device, compute the layer output. */
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = 0;
  /**
   * @brief Using the GPU device, compute the layer output.
   *        Fall back to Forward_cpu() if unavailable.
   */
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // LOG(WARNING) &lt;&lt; &quot;Using CPU code as backup.&quot;;
    return Forward_cpu(bottom, top);
  }

  /**
   * @brief Using the CPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   */
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) = 0;
  /**
   * @brief Using the GPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   *        Fall back to Backward_cpu() if unavailable.
   */
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // LOG(WARNING) &lt;&lt; &quot;Using CPU code as backup.&quot;;
    Backward_cpu(top, propagate_down, bottom);
  }

  /**
   * Called by the parent Layer&#39;s SetUp to check that the number of bottom
   * and top Blobs provided as input match the expected numbers specified by
   * the {ExactNum,Min,Max}{Bottom,Top}Blobs() functions.
   */
   //æ ¡éªŒè¾“å…¥/è¾“å‡ºBlobæ•°ç›®æ˜¯å¦æ»¡è¶³Layerè¦æ±‚
  virtual void CheckBlobCounts(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
                               const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    if (ExactNumBottomBlobs() &gt;= 0) {
      CHECK_EQ(ExactNumBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes &quot; &lt;&lt; ExactNumBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (MinBottomBlobs() &gt;= 0) {
      CHECK_LE(MinBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes at least &quot; &lt;&lt; MinBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (MaxBottomBlobs() &gt;= 0) {
      CHECK_GE(MaxBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes at most &quot; &lt;&lt; MaxBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (ExactNumTopBlobs() &gt;= 0) {
      CHECK_EQ(ExactNumTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces &quot; &lt;&lt; ExactNumTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (MinTopBlobs() &gt;= 0) {
      CHECK_LE(MinTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces at least &quot; &lt;&lt; MinTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (MaxTopBlobs() &gt;= 0) {
      CHECK_GE(MaxTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces at most &quot; &lt;&lt; MaxTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (EqualNumBottomTopBlobs()) {
      CHECK_EQ(bottom.size(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces one top blob as output for each &quot;
          &lt;&lt; &quot;bottom blob input.&quot;;
    }
  }

  /**
   * Called by SetUp to initialize the weights associated with any top blobs in
   * the loss function. Store non-zero loss weights in the diff blob.
   */
   //è¯¥å‡½æ•°åœ¨Layerçš„Setupå‡½æ•°ä¸­è¢«è°ƒç”¨ï¼Œä¸»è¦ç›®çš„æ˜¯åˆå§‹åŒ–ä¸Top Blobç›¸å…³çš„lossæƒé‡ï¼Œæ”¾åˆ°Top Blobçš„diffåŸŸï¼Œå®é™…ç”±Forward()è®¡ç®—losså‡½æ•°
   //loss_weight == 0,è¡¨ç¤ºå½“å‰å±‚ä¸å‚ä¸losså‡½æ•°æ±ç®—ï¼Œå¤§éƒ¨åˆ†Layerå±äºè¿™ä¸€ç±»
   //loss_weight ==1,è¡¨ç¤ºå½“å‰å±‚å‚ä¸losså‡½æ•°æ±ç®—ï¼ŒæŸå¤±å±‚ï¼ˆLossLayer) å±äºè¿™ä¸€ç±»
  inline void SetLossWeights(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  //ä»ProtoBufferå¯¹è±¡ä¸­è·å¾—Layerå‚æ•°ï¼Œè¿™é‡Œéœ€è¦ç”¨loss_weightå‚æ•°
    const int num_loss_weights = layer_param_.loss_weight_size();
    //å¦‚æœ ProtoBufferä¸­å­˜åœ¨è‡³å°‘ä¸€ä¸ªloss_weightå‚æ•°,loss_weightå‚æ•°ä¸ªæ•°åº”å½“ä¸Top Blobæ•°ç›®ç›¸åŒï¼Œæˆ–è€…ä¸è¦loss_weightå‚æ•°
    if (num_loss_weights) {
      CHECK_EQ(top.size(), num_loss_weights) &lt;&lt; &quot;loss_weight must be &quot;
          &quot;unspecified or specified once per top blob.&quot;;
    //éå†æ¯ä¸ªTop Blob
      for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
      // ä» ProtoBuffer å¯¹è±¡æ‹¿åˆ° loss_weight å®é™…å€¼(0 æˆ–è€…1)
        const Dtype loss_weight = layer_param_.loss_weight(top_id);
        //è‹¥ä¸º0,è·³è¿‡
        if (loss_weight == Dtype(0)) { continue; }\
        //è‹¥ä¸ä¸º0,åˆ™å¯¹ç½‘ç»œè¿›è¡Œç›¸å…³è®¾ç½®
        this-&gt;set_loss(top_id, loss_weight);    //æœ¬åœ°è®°å½•loss_weightå€¼
        const int count = top[top_id]-&gt;count();
        Dtype* loss_multiplier = top[top_id]-&gt;mutable_cpu_diff();
        //å°†loss_weightå€¼å…¥ Top Blob çš„diffåŸŸï¼Œä¼ é€’åˆ°å…¶ä»–éœ€è€ä½¿ç”¨çš„åœ°ä¸€æ–¹ï¼Œå®ç°è¿œç¨‹åŒæ­¥
        caffe_set(count, loss_weight, loss_multiplier);
      }
    }
  }

 private:
 //ç¦ç”¨æ‹·è´æ„é€ å‡½æ•°å’Œè³¦å€¼è¿ç®—å‡½æ•° 
  DISABLE_COPY_AND_ASSIGN(Layer);
};  // class Layer

// Forward and backward wrappers. You should implement the cpu and
// gpu specific implementations instead, and should not change these
// functions.
//ä½¿ç”¨æ—¶åªéœ€åœ¨æ´¾ç”Ÿç±»ä¸­æ”¹å†™ Forward_cpuã€Forward_gpuã€Backward_cpuã€Backward_gpu
template &lt;typename Dtype&gt;
inline Dtype Layer&lt;Dtype&gt;::Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  Dtype loss = 0;
  Reshape(bottom, top);
  switch (Caffe::mode()) {      //åˆ¤æ–­è®¡ç®—è®¾å¤‡
  case Caffe::CPU:      //åœ¨CPUä¸Šæ‰§è¡ŒForwardè®¡ç®—
    Forward_cpu(bottom, top);   //è°ƒç”¨CPUç‰ˆæœ¬çš„ Forwardå‡½æ•°
    //è¿˜æ²¡å®Œï¼Œè¦è®¡ç®—loss (å¦‚æœæœ‰çš„è¯)
    for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
      if (!this-&gt;loss(top_id)) { continue; }
      const int count = top[top_id]-&gt;count();
      // è‹¥ä¸º LossLayerï¼Œåˆ™å·²ç»é€šè¿‡Forwardå‡½æ•°è®¡ç®—å‡ºå…¨å±€æŸå¤±å‡½æ•°ï¼Œæ”¾åœ¨Top Blob dataåŸŸ
      const Dtype* data = top[top_id]-&gt;cpu_data();
      // è‹¥loss_weightä¸ä¸º0,åˆ™å·±ç»åœ¨SetLossWeightså‡½æ•°ä¸­å°†lossæƒé‡æ”¾åœ¨Top Blob diffåŸŸ
      const Dtype* loss_weights = top[top_id]-&gt;cpu_diff();
      // è®¡ç®—åŠ æƒåçš„lossä¹‹å’Œï¼Œå¾—åˆ°æ ‡é‡losså€¼
      loss += caffe_cpu_dot(count, data, loss_weights);
    }
    break;
  case Caffe::GPU:
    Forward_gpu(bottom, top);
#ifndef CPU_ONLY
    for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
      if (!this-&gt;loss(top_id)) { continue; }
      const int count = top[top_id]-&gt;count();
      const Dtype* data = top[top_id]-&gt;gpu_data();
      const Dtype* loss_weights = top[top_id]-&gt;gpu_diff();
      Dtype blob_loss = 0;
      caffe_gpu_dot(count, data, loss_weights, &amp;blob_loss);
      loss += blob_loss;
    }
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
  return loss;
}
//åå‘ä¼ æ’­å‡½æ•°,ç›´æ¥è°ƒç”¨å¯¹åº”è®¾å¤‡å‡½æ•°
template &lt;typename Dtype&gt;
inline void Layer&lt;Dtype&gt;::Backward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  switch (Caffe::mode()) {
  case Caffe::CPU:
    Backward_cpu(top, propagate_down, bottom);
    break;
  case Caffe::GPU:
    Backward_gpu(top, propagate_down, bottom);
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

//å°†å±‚é…ç½®å‚æ•°åºåˆ—åŒ–ä¸ºProtoBuffer
template &lt;typename Dtype&gt;
void Layer&lt;Dtype&gt;::ToProto(LayerParameter* param, bool write_diff) {
  param-&gt;Clear();
  param-&gt;CopyFrom(layer_param_);
  param-&gt;clear_blobs();
  for (int i = 0; i &lt; blobs_.size(); ++i) { //æƒå€¼å’Œåç½®é¡¹ä¹Ÿä¼šä¿å­˜
    blobs_[i]-&gt;ToProto(param-&gt;add_blobs(), write_diff);
  }
}

}  // namespace caffe

#endif  // CAFFE_LAYER_H_

</code></pre>

<p><code>Layer</code>æºæ–‡ä»¶ä½äº<code>src/caffe/layer.cpp</code>ä¸­:</p>

<pre><code class="language-c++">#include &quot;caffe/layer.hpp&quot;

namespace caffe {

INSTANTIATE_CLASS(Layer);

}  // namespace caffe

</code></pre>

<p>å¯è§Layerå¤§éƒ¨åˆ†å‡½æ•°å¹¶æ²¡æœ‰å®ç°ï¼Œåªæœ‰è™šå‡½æ•°ï¼ŒçœŸæ­£çš„å®ç°éƒ½åœ¨æ´¾ç”Ÿç±»ä¸­ã€‚å…·ä½“ä»£ç å¯ä»¥è¿›ä¸€æ­¥é˜…è¯» <code>src/caffe/ä¸¨ayers/*.cpp</code>ã€‚</p>

<p>åœ¨ä½¿ç”¨ Layer ä¹‹å‰ï¼Œéœ€è¦å…ˆåŒ…å«å¤´æ–‡ä»¶<code>#include &lt;caffe/layer.hpp&gt;</code>ï¼Œå†é€šè¿‡<code>using namespace caffe;</code>ä½¿ç”¨å‘½åç©ºé—´caffeã€‚å¦‚æœä»£ç ä¸­è¯•å›¾åˆ›å»ºLayerå¯¹è±¡ï¼Œç¼–è¯‘æ—¶ä¼šæŠ¥é”™ï¼š</p>

<p><code>error: cannot declare variable &#39;a^ to be of abstract type &#39;caffeï¼š:Layer&lt;float&gt;</code></p>

<p>è¿™æ˜¯å› ä¸ºLayerç±»æ˜¯ä¸€ä¸ªè™šåŸºç±»ï¼Œä¸èƒ½ç›´æ¥åˆ›å»ºå¯¹è±¡ã€‚å…³äºè™šåŸºç±»,è¿™é‡Œä¸å†è¿‡å¤šè¯´æ˜.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/15</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14974281629389.html">
                
                  <h1>æ€æ ·ç§»é™¤OSXRESERVEDåˆ†åŒº(å¦‚æœBootCamp Assistantåœ¨å®‰è£…ä¹‹åæ²¡æœ‰æˆåŠŸåˆ é™¤è¿™ä¸ªåˆ†åŒº)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ol>
<li>æ‰“å¼€ç£ç›˜å·¥å…·</li>
<li>ç‚¹å‡»ä¸»ç‰©ç†ç£ç›˜(ä¸æ˜¯ä¸‹è¾¹çš„åˆ†åŒº)</li>
<li>ç‚¹å‡»åˆ†åŒºæŒ‰é’®</li>
<li>When you see the pie chart, click on the OSXRESERVED partition in the pie chart</li>
<li>ç‚¹å‡»<code>-</code>æ ‡å¿—</li>
<li>click apply</li>
</ol>

<p>Thats it! </p>

<p><strong>DON&#39;T use the ERASE Button in Disk Utility! That will cause problems and it won&#39;t reallocate space back.</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/14</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MAC%20OS.html'>MAC OS</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14974276192148.html">
                
                  <h1>é‡æ–°åˆ©ç”¨boot campå®‰è£…win10</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>ç”±äºåŸæ¥ç»™win10åˆ†çš„ç£ç›˜å®¹é‡å¤ªå°,è€Œmacåˆä¸èƒ½åœ¨åç»­åŠ¨æ€ç»™winåˆ†åŒºå¢åŠ ç£ç›˜å®¹é‡,æ‰€ä»¥åªèƒ½é‡æ–°å®‰è£…åˆ†åŒºäº†.</p>

<ol>
<li>æ‰“å¼€macç³»ç»Ÿå·¥å…·ä¸­çš„ç£ç›˜<code>ç£ç›˜å·¥å…·</code>é€‰æ‹©æ€»çš„ç£ç›˜,ç‚¹å‡»ä¸Šæ–¹çš„<code>åˆ†åŒº</code>æŒ‰é’®,å°†ç³»ç»Ÿä¸­å…¶å®ƒåˆ†åŒºåˆ æ‰(é€šè¿‡ç‚¹å‡»é¥¼å›¾ä¸­çš„å¯¹åº”åŒºåŸŸ,ç‚¹å‡»ä¸‹æ–¹çš„<code>-</code>å·),å°±å¯ä»¥å°†åˆ†åŒºåˆ æ‰,åˆå¹¶åˆ°macç³»ç»Ÿç£ç›˜.å…¶ä»–åˆ†åŒºæ¯”å¦‚ä¹‹å‰å®‰è£…winäº§ç”Ÿçš„<code>OSXRESERVED Partition</code> </li>
<li>æ‰“å¼€bootcamp,ç‚¹å‡»æ¢å¤æŒ‰é’®,å°†ä¹‹å‰çš„winç³»ç»ŸæŠ¹æ‰,ä¹‹åå°±ä¼šå‘ç°,ä½ çš„macç³»ç»Ÿç£ç›˜å˜å›æ¥äº†.</li>
<li>é‡å¯ä¸€ä¸‹,ä¹‹åæ‰“å¼€bootcamp,ç‚¹å‡»ä¸‹æ–¹çš„<code>ç»§ç»­</code>æŒ‰é’®,é€‰æ‹©ç³»ç»Ÿisoé•œåƒ,ä¸ºwinç³»ç»Ÿé€‰æ‹©åˆ†åŒºå¤§å°(è¿™å›åˆ†å¤šäº›ğŸ˜“).</li>
<li>ç‚¹å‡»ç¡®å®š,ç­‰å¾…ä¸‹è½½Windowsæ”¯æŒè½¯ä»¶.
<img src="media/14974276192148/14974281373736.jpg" alt=""/></li>
<li>å®‰è£…ä¹‹å,åœ¨OSRESEVERåˆ†åŒºä¸­,çš„bootcampæ–‡ä»¶å¤¹ä¸­æ‰“å¼€å®‰è£…é©±åŠ¨çš„ç¨‹åº.</li>
</ol>

<p>åˆ°æ­¤win10 åº”è¯¥å°±å®‰è£…å¥½äº†~</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/14</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MAC%20OS.html'>MAC OS</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14970763342670.html">
                
                  <h1>caffeæ•°æ®ç»“æ„æè¿°</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>æ‰“å¼€caffeç›®å½•ä¸‹çš„<code>src/caffe/proto/caffe.proto</code>æ–‡ä»¶,é¦–å…ˆè®²çš„å°±æ˜¯Blobçš„æè¿°.</p>

<pre><code class="language-proto">// è¯¥ç»“æ„æè¿°äº† Blobçš„å½¢çŠ¶ä¿¡æ¯
message BlobShape {
  repeated int64 dim = 1 [packed = true];  //åªåŒ…æ‹¬è‹¥å¹²int64ç±»å‹å€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºBlobæ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚packedè¡¨ç¤ºè¿™äº›å€¼åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒï¼Œæ²¡æœ‰ç©ºæ´
}

//è¯¥ç»“æ„æè¿°Blobåœ¨ç£ç›˜ä¸­åºåˆ—åŒ–åçš„å½¢æ€
message BlobProto {
  optional BlobShape shape = 7;    //å¯é€‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªBlobShapeå¯¹è±¡
  repeated float data = 5 [packed = true]; // //åŒ…æ‹¬è‹¥åƒæµ®ç‚¹å…ƒç´ ï¼Œå­˜å‚¨æ•°æ®æˆ–æƒå€¼ï¼Œå…ƒç´ æ•°ç›®ç”±shapeæˆ–ï¼ˆnum, channels, height, width)ç¡®å®šï¼Œè¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒ.
  repeated float diff = 6 [packed = true];  ////åŒ…æ‹¬è‹¥å¹²æµ®ç‚¹å…ƒç´ ï¼Œç”¨äºå­˜å‚¨å¢é‡ä¿¡æ¯ï¼Œç»´åº¦ä¸data æ•°ç»„ä¸€è‡´
  repeated double double_data = 8 [packed = true];  // ä¸ dataå¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸ºdouble
  repeated double double_diff = 9 [packed = true];  // ä¸ diff å¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸º double

  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}
</code></pre>

<p><strong>è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨protobufferä¸»è¦æ˜¯å› ä¸ºå®ƒå…·æœ‰å¾ˆå¥½çš„å¥å£®æ€§,å°†ç¼–ç¨‹æœ€å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹åŠ ä»¥éšè—ï¼Œè®©æœºå™¨è‡ªåŠ¨å¤„ç†.</strong></p>

<h2 id="toc_0">Blobçš„æ„æˆ</h2>

<p>Blobæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»,å£°æ˜åœ¨<code>include/caffe/blob.hppä¸­</code>,é‡Œé¢å°è£…äº†ä¸€äº›åŸºæœ¬çš„Layer,Net,Solverç­‰,è¿˜æœ‰syncedmemç±»:</p>

<pre><code class="language-C++">
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;//ç”±protocç”Ÿæˆçš„å¤´æ–‡ä»¶ï¼Œå£°æ˜äº† BlobProtoã€BlobShapeç­‰éµå¾ªcaffe.protoåè®®çš„æ•°æ®ç»“æ„ å¯ä»¥åœ¨src/caffe/protoæ–‡ä»¶ä¸‹è¿è¡Œprotoc caffe.proto --cpp_out=./å‘½ä»¤ç”Ÿæˆè¯¥å¤´æ–‡ä»¶.
#include &quot;caffe/syncedmem.hpp&quot;  //CPU/GPUå…±äº«å†…å­˜ç±»ï¼Œç”¨äºæ•°æ®åŒæ­¥

const int kMaxBlobAxes = 32;    //Blobæœ€å¤§ç»´æ•°ç›®
template &lt;typename Dtype&gt;
class Blob {    //ç±»å£°æ˜
 public:
    //é»˜è®¤æ„é€ å‡½æ•°
  Blob()
       : data_(), diff_(), count_(0), capacity_(0) {}
    //æ˜¾å¼æ„é€ å‡½æ•°
  explicit Blob(const int num, const int channels, const int height, const int width);
  explicit Blob(const vector&lt;int&gt;&amp; shape);

 //å˜å½¢å‡½æ•°ï¼ŒæŠ¥æ®è¾“å…¥å‚æ•°é‡æ–°è®¾ç½®å½“å‰Blobå½¢çŠ¶,å¿…è¦æ—¶é‡æ–°åˆ†é…å†…å­˜
  void Reshape(const int num, const int channels, const int height,
      const int width);
  
  void Reshape(const vector&lt;int&gt;&amp; shape);
  void Reshape(const BlobShape&amp; shape);
  void ReshapeLike(const Blob&amp; other);
  //å¾—åˆ°Blobå½¢çŠ¶å­—ç¬¦ä¸²ç”¨äºæ‰“å°log,è§Caffeè¿è¡Œlog,ç±»ä¼¼&quot;Top shape: 100 1 28 28 (78400)&quot;
  inline string shape_string() const {
    ostringstream stream;
    for (int i = 0; i &lt; shape_.size(); ++i) {
      stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;
    }
    stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;
    return stream.str();
  }
  //è¿”å›Blobå½¢çŠ¶
  inline const vector&lt;int&gt;&amp; shape() const { return shape_; }
    //è¿”å›æŸ1ç»´åº¦çš„å°ºå¯¸
  inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  //è¿”å›ç»´åº¦æ•°ç›®
  inline int num_axes() const { return shape_.size(); }
  //è¿”å›Blobä¸­å…ƒç´ æ€»æ•°
  inline int count() const { return count_; }
    //è¿”å›Blobä¸­æŸå‡ ç»´å­é›†çš„å…ƒç´ æ€»æ•°
    inline int count(int start_axis, int end_axis) const {
    CHECK_LE(start_axis, end_axis); //ä¿è¯ start_axis &lt;= end_axis
    CHECK_GE(start_axis, 0);  // ä¿è¯ start_axis &gt;= 0
    CHECK_GE(end_axis, 0);      // ä¿è¯ end_axis &gt;= 0
    CHECK_LE(start_axis, num_axes()); //ä¿è¯start_axis    &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    CHECK_LE(end_axis, num_axes()); //ä¿è¯end_axis &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    int count = 1;
    for (int i = start_axis; i &lt; end_axis; ++i) {
      count *= shape(i);
    }
    return count;
  }
  //è®¡ç®—ä»æŸä¸€ç»´åº¦å¼€å§‹çš„å…ƒç´ æ€»æ•°
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
  //è½¬æ¢åæ ‡è½´ç´¢å¼•[-N,N)ä¸ºæ™®é€šç´¢å¼•[0,N)
  inline int CanonicalAxisIndex(int axis_index) const {
    CHECK_GE(axis_index, -num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    CHECK_LT(axis_index, num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    if (axis_index &lt; 0) {
    //è´Ÿç´¢å¼•è¡¨ç¤ºä»åå‘å‰è®¿é—®ï¼Œ-1è¡¨ç¤ºæœ€åä¸€ä¸ªä¸ªå…ƒç´ ï¼Œæ™®é€šç´¢å¼•å€¼ä¸º N-1:åŒç†ï¼Œ-2 =&gt; N-2, -3 =&gt; N-3,â€¦
      return axis_index + num_axes();
    }
    return axis_index;
  }
  //è·å–æŸä¸€ç»´çš„å°ºå¯¸
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.
  inline int num() const { return LegacyShape(0); }
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.
  inline int channels() const { return LegacyShape(1); }
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.
  inline int height() const { return LegacyShape(2); }
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.
  inline int width() const { return LegacyShape(3); }
  inline int LegacyShape(int index) const {
    CHECK_LE(num_axes(), 4)
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;
    CHECK_LT(index, 4);
    CHECK_GE(index, -4);
    if (index &gt;= num_axes() || index &lt; -num_axes()) {
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse
      // indexing) -- this special case simulates the one-padding used to fill
      // extraneous axes of legacy blobs.
      return 1;
    }
    return shape(index);
  }
  //ä¸‹é¢çš„æ˜¯è®¡ç®—åç§»é‡çš„å‡½æ•°
  inline int offset(const int n, const int c = 0, const int h = 0,
      const int w = 0) const {
    CHECK_GE(n, 0);
    CHECK_LE(n, num());
    CHECK_GE(channels(), 0);
    CHECK_LE(c, channels());
    CHECK_GE(height(), 0);
    CHECK_LE(h, height());
    CHECK_GE(width(), 0);
    CHECK_LE(w, width());
    return ((n * channels() + c) * height() + h) * width() + w;
  }

  inline int offset(const vector&lt;int&gt;&amp; indices) const {
    CHECK_LE(indices.size(), num_axes());
    int offset = 0;
    for (int i = 0; i &lt; num_axes(); ++i) {
      offset *= shape(i);
      if (indices.size() &gt; i) {
        CHECK_GE(indices[i], 0);
        CHECK_LT(indices[i], shape(i));
        offset += indices[i];
      }
    }
    return offset;
  }
  //æŒ‰å€¼æ‹·è´Blobåˆ°å½“å‰Blob
  void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false);
  
  //ä¸‹é¢å‡ ä¸ªå‡½æ•°æ˜¯å­˜å–å™¨(getter/setter)
  inline Dtype data_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_data()[offset(n, c, h, w)];
  }

  inline Dtype diff_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_diff()[offset(n, c, h, w)];
  }

  inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_data()[offset(index)];
  }

  inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_diff()[offset(index)];
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;
  }
  
  //åªè¯»è®¿é—®cpu_date
  const Dtype* cpu_data() const;
  //è®¾ç½®cpu_date
  void set_cpu_data(Dtype* data);
  const int* gpu_shape() const;
  //åªè¯»è®¿é—®gpu_date
  const Dtype* gpu_data() const;
  //è®¾ç½®gpu_date
  void set_gpu_data(Dtype* data);
  //åªè¯»è®¿é—®cpu_diff
  const Dtype* cpu_diff() const;
  //åªè¯»è®¿é—®gpu_diff
  const Dtype* gpu_diff() const;
  //ä¸‹é¢å››ä¸ªæ˜¯è¯»å†™è®¿é—®æ•°æ®
  Dtype* mutable_cpu_data();
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  void Update();    //Blobæ›´æ–°è¿ç®—ï¼Œå¯ç®€å•ç†è§£ä¸ºdataä¸diffçš„mergeè¿‡ç¨‹
  //ååºåˆ—åŒ–å‡½æ•°ï¼Œä»BlobProtoä¸­æ¢å¤ä¸ªBlobå¯¹è±¡
  void FromProto(const BlobProto&amp; proto, bool reshape = true);
  //åºåˆ—åŒ–å‡½æ•°ï¼Œå°†å†…å­˜ä¸­çš„Blobå¯¹è±¡ä¿å­˜åˆ°BlobProtoä¸­
  void ToProto(BlobProto* proto, bool write_diff = false) const;

  /// @brief Compute the sum of absolute values (L1 norm) of the data.
  Dtype asum_data() const;
  /// @brief Compute the sum of absolute values (L1 norm) of the diff.
  Dtype asum_diff() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the data.
  Dtype sumsq_data() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the diff.
  Dtype sumsq_diff() const;

/// @brief Scale the blob data by a constant factor.
  void scale_data(Dtype scale_factor);
  /// @brief Scale the blob diff by a constant factor.
  void scale_diff(Dtype scale_factor);
 // å…±äº«å¦ä¸€ä¸ª Blob çš„ diff
  void ShareData(const Blob&amp; other);
  void ShareDiff(const Blob&amp; other);
  
  protected:
  shared_ptr&lt;SyncedMemory&gt; data_;   //å­˜æ”¾æŒ‡å‘dataçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; diff_;   //å­˜æ”¾æŒ‡å‘diffçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; shape_data_; 
  vector&lt;int&gt; shape_;   //å½¢çŠ¶ä¿¡æ¯
  int count_;   //å­˜æ”¾æœ‰æ•ˆå…ƒç´ æ•°ç›®ä¿¡æ¯
  int capacity_;    //å­˜æ”¾Blobå®¹å™¨çš„å®¹é‡ä¿¡æ¯

  DISABLE_COPY_AND_ASSIGN(Blob);    //ç¦ç”¨æ‹·è´æ„é€ å‡½æ•°ã€é™šå€¼è¿ç®—ç¬¦é‡è½½
};  // class Blob

</code></pre>

<p><strong>æ³¨æ„åˆ°Caffeç±»ä¸­æˆå‘˜å˜é‡åéƒ½å¸¦æœ‰åç¼€ï¼Œè¿™æ ·åœ¨å‡½æ•°å®ç°ä¸­å®¹æ˜“åŒºåˆ†ä¸´æ—¶å˜é‡å’Œç±»æˆå‘˜å˜é‡ã€‚</strong></p>

<p>æ‰“å¹µ<code>include/caffe/syncedmem.hpp</code>ï¼ŒæŸ»çœ‹è¯¥ç±»çš„ç”¨æ³•:</p>

<pre><code class="language-c++">#ifndef CAFFE_SYNCEDMEM_HPP_
#define CAFFE_SYNCEDMEM_HPP_

#include &lt;cstdlib&gt;

#ifdef USE_MKL
  #include &quot;mkl.h&quot;
#endif

#include &quot;caffe/common.hpp&quot;

namespace caffe {

//å¦‚æœåœ¨GPUæ¨¡å¼ï¼Œä¸”CUDAä½¿èƒ½ï¼Œé‚£ä¹ˆä¸»æœºå†…å­˜ä¼šä»¥é¡µé”å®šå†…å­˜æ–¹å¼åˆ†é…ï¼ˆä½¿ç”¨cudaMallocHostUå‡½æ•°ã€‚å¯¹f-å•GPUçš„æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œä½†å¤šGPUä¼šéå¸¸æ˜æ˜¾)
inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) {
#ifndef CPU_ONLY
  if (Caffe::mode() == Caffe::GPU) {
    CUDA_CHECK(cudaMallocHost(ptr, size));
    *use_cuda = true;
    return;
  }
#endif
#ifdef USE_MKL
  *ptr = mkl_malloc(size ? size:1, 64);
#else
  *ptr = malloc(size);
#endif
  *use_cuda = false;
  CHECK(*ptr) &lt;&lt; &quot;host allocation of size &quot; &lt;&lt; size &lt;&lt; &quot; failed&quot;;
}
// ä¸CaffeMallocHostå¯¹åº”
inline void CaffeFreeHost(void* ptr, bool use_cuda) {
#ifndef CPU_ONLY
  if (use_cuda) {
    CUDA_CHECK(cudaFreeHost(ptr));
    return;
  }
#endif
#ifdef USE_MKL
  mkl_free(ptr);
#else
  free(ptr);
#endif
}

//è¯¥ç±»è´Ÿè´£å­˜å‚¨åˆ†é…ä»¥åŠä¸»æœºå’Œè®¾å¤‡é—´åŒæ­¥
class SyncedMemory {
 public:
 //æ„é€ å‡½æ•°
  SyncedMemory();
  //æ˜¾å¼æ„é€ å‡½æ•°
  explicit SyncedMemory(size_t size);
  //ææ„å‡½æ•°
  ~SyncedMemory();
  const void* cpu_data();       //åªè¯»è·å–cpu data
  void set_cpu_data(void* data);    //è®¾ç½®cpu data
  const void* gpu_data();       //åªè¯»è·å–gpu data
  void set_gpu_data(void* data);    //è®¾ç½®gpu data
  void* mutable_cpu_data();     // è¯»å†™è·å– cpu data
  void* mutable_gpu_data();     // è¯»å†™è·å– gpu data
  //çŠ¶æ€æœºå˜é‡ï¼Œè¡¨ç¤º4ç§çŠ¶æ€ï¼šæœ¯åˆå§‹åŒ–ã€CPUæ•°æ®å¥‹æ•ˆã€GPUæ•°æ®æœ‰æ•ˆã€å·±åŒæ­¥
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  //è·å¾—å½“å‰çŠ¶æ€æœºå˜é‡å€¼
  SyncedHead head() { return head_; }
  //è·å¾—å½“å‰å­˜å‚¨ç©ºé—´å°ºå¯¸
  size_t size() { return size_; }

#ifndef CPU_ONLY
  void async_gpu_push(const cudaStream_t&amp; stream);
#endif

 private:
  void check_device();

  void to_cpu();    //æ•°æ®åŒæ­¥è‡³CPU
  void to_gpu();    //æ•°æ®åŒæ­¥è‡³GPU
  void* cpu_ptr_;   //ä½äºCPUçš„æ•°æ®æŒ‡é’ˆ
  void* gpu_ptr_;   //ä½äºGPUçš„æ•°æ®æŒ‡é’ˆ
  size_t size_;     //å­˜å‚¨ç©ºé—´å¤§å°
  SyncedHead head_; //çŠ¶æ€æœºå˜é‡
  bool own_cpu_data_;   //æ ‡å¿—æ˜¯å¦æ‹¥æœ‰CPUæ•°æ®æ‰€æœ‰æƒï¼ˆå¦ï¼Œå³ä»åˆ«çš„å¯¹è±¡å…±äº«)
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;   ////æ ‡å¿—æ˜¯å¦æ‹¥æœ‰GPUæ•°æ®æ‰€æœ‰æƒ
  int device_;      //è®¾å¤‡å·

  DISABLE_COPY_AND_ASSIGN(SyncedMemory);
};  // class SyncedMemory

}  // namespace caffe

#endif  // CAFFE_SYNCEDMEM_HPP_

</code></pre>

<p>Blobç±»å®ç°çš„æºç ä½äº<code>src/caffe/blob.cpp</code>ä¸­ï¼Œå†…å®¹å¦‚ä¸‹:</p>

<pre><code class="language-c++">
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {
//å˜ç»´å‡½æ•°ï¼Œå°†ï¼ˆnum, channels, height, width}å‚æ•°è½¬æ¢ä¸ºvector&lt;int&gt;ï¼Œç„¶åè°ƒç”¨é‡è½½çš„å˜ç»´å‡½æ•°void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape)
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
//çœŸæ­£å˜ç»´å‡½æ•°
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes); //ä¿è¯vectorç»´åº¦&lt;=kMaxBlobAxes
  count_ = 1;   //ç”¨äºè®¡ç®—å…ƒç´ æ€»æ•°=num * channels * height * width 
  shape_.resize(shape.size());  //æˆå‘˜å˜é‡ç»´åº¦ä¹Ÿè¢«é‡ç½
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);  // ä¿è¯æ¯ç»´åº¦å°ºå¯¸éƒ½&gt;=0
    if (count_ != 0) {
    //è¯count_ä¸æº¢å‡º
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];     //count_ç´¯ä¹˜
    shape_[i] = shape[i];   //ä¸ºæˆå‘˜å˜é‡èµ‹å€¼
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {     //å¦‚æœæ–°çš„count_å¤§äºå½“å‰å·±åˆ†fé…ç©ºé—´å®¹é‡
    capacity_ = count_;         //æ‰©å®¹ï¼Œé‡æ–°åˆ†é…data_å’Œdif f_ç©ºé—´
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

//void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) å’Œvoid Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)ä¸ä¸Šé¢ç±»ä¼¼. 

//æ„é€ å‡½æ•°
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  // è°ƒç”¨Reshapeä¹‹å‰å¿…é¡»åˆå§‹åŒ–capacity_ï¼Œå¦åˆ™ä¼šå¯¼è‡´ä¸å¯é¢„æœŸç»“æœ
  : capacity_(0) {
  Reshape(num, channels, height, width);
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(shape);
}

template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}
//åªè¯»è·å–cpu dateæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);     //ä¿è¯data_ä¸ä¸º NULL
  return (const Dtype*)data_-&gt;cpu_data();
}
//ä¿®æ”¹cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}
//åªè¯»è·å–cpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}
//åªè¯»è·å–gpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}
//è¯»å†™è®¿é—®cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}
//è¯»å†™è®¿é—®gpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}
//ä¸ä¸Šé¢ç›¸åŒ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}
//Update()å‡½æ•°ç”¨äºç½‘ç»œå‚æ•°Blobçš„æ›´æ–°ã€‚å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  // We will perform update based on where the data is located.dataåœ¨å“ªé‡Œæˆ‘ä»¬å°±åœ¨é‚£é‡Œæ›´æ–°
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:       //dataä½äºcpuç«¯
    // æ‰§è¡ŒCPUè®¡ç®—
        caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:   //dataä½äºGPUç«¯,æˆ–è€…CPU/GPUå·²ç»åŒæ­¥
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // æ‰§è¡Œ CPU ä¸Šçš„è®¡ç®—ï¼Œdata_[iã€‘=data_[i] - diff_[i], i = 0,1,2,â€¦ï¼Œcount_-1
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;     //ç¼–æ³½æ—¶æ‰“å¼€äº†CPU_ONLYé€‰é¡¹ï¼Œé‚£ä¹ˆGPUæ¨¡å¼ç¦ç”¨
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
  }
}
//è®¡ç®—data_çš„L1-èŒƒæ•°,å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());  //æ‰§è¡ŒCPUä¸Šçš„asumè®¡ç®—
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
//åŒä¸Š,è®¡ç®—diff_çš„L1èŒƒæ•°
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
  return 0;
}
//è®¡ç®—data_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);  //æ‰§è¡Œ CPUä¸Šçš„dotè®¡ç®—
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//åŒä¸Š,è®¡ç®—diff_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//å¯¹data_è¿›è¡Œå¹…åº¦ç¼©æ”¾
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:   //æ‰§è¡ŒCPUä¸Šçš„è®¡ç®—
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}
//å¯¹diff_è¿›è¡Œç¼©æ”¾,åŒç†
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
}
//åˆ¤æ–­å½¢çŠ¶æ˜¯å¦ç›¸åŒ
template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    // Using deprecated 4D Blob dimensions --
    // shape is (num, channels, height, width).
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.
    // methods as these index from the beginning of the blob shape, where legacy parameter blobs were indexed from the end of the blob shape (e.g., bias Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).
    //è¾“å…¥çš„ç»´åº¦è‹¥ä½¿ç”¨è¿‡æ—¶çš„ç»´åº¦ä¿¡æ¯ï¼ˆnum, channels,height, width)ï¼Œåˆ™éœ€è¦è½¬æ¢ä¸ºæ–°çš„vectorå‚æ•°,ä»£ç ä½¿ç”¨äº†C++ä¸­çš„â€œæ‡’â€é€»è¾‘
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  //ç›´æ¥å¯¹æ¯”
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}
//ä»å¦ä¸€ä¸ªBlobå¯¹è±¡æ‹·è´data (å¯é€‰diff),å¿…è¦æ—¶è¿›è¡Œå˜ç»´
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);      //å¦‚æœè¦å˜ç»´,åˆ™æ‰§è¡Œè¿™ä¸ª
    } else {    //ä¸¤ä¸ªblobå½¢çŠ¶ä¸åŒ,åˆ™æŠ¥é”™
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:      //GPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:      //CPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

//ä»BlobProtoä¸­åŠ è½½ä¸€ä¸ªBlob,é€‚ç”¨äºä»ç£ç›˜è½½å…¥ä¹‹å‰å¯¼å‡ºçš„Blob
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {        //ä»BlobProtoå¯¹è±¡ä¸­è·å¾—æ‰€éœ€å„ä¸ªç»´åº¦ä¿¡æ¯
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      // Using deprecated 4D Blob dimensions --
      // shape is (num, channels, height, width).
      shape.resize(4);
      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }
    Reshape(shape);     //BlobæŒ‰ç…§ç»´åº¦ä¿¡æ¯è¿›è¡Œå˜ç»´
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
  }
  // copy data åŠ è½½æ•°æ®
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯doubleç±»å‹ data
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);   //åŠ è½½double date
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);  //å¦åˆ™åŠ è½½float data
    }
  }
  if (proto.double_diff_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯ double ç±»å‹ diff
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}
//å°†Blobä¸­çš„data(å¯é€‰diff)å¯¼å‡ºåˆ°BlobProtoç»“æ„ä½“.ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜æ–‡ä»¶ä¸­
template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();     //é‡ç½®protoçš„ç»´åº¦,ä¿è¯ä¸blobç›¸åŒ
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();   //æ¸…é™¤data
  proto-&gt;clear_double_diff();   //æ¸…é™¤diff
  const double* data_vec = cpu_data();  //å°†dataå¯¼å‡ºåˆ°proto
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {         //  è‹¥æœ‰write_diffçš„éœ€æ±‚
    const double* diff_vec = cpu_diff();    //å°†diffå¯¼å‡ºåˆ°proto
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}
//åŒä¸Š,åªä¸è¿‡ç±»å‹ä¸ºfloat
template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}
//å®ä¾‹åŒ–Blob   ç±»æ¨¡æ¿ï¼ˆfloat, double)
INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;

}  // namespace caffe

</code></pre>

<p><strong>åˆ°æ­¤,æˆ‘ä»¬å°±äº†è§£äº†Caffeä¸€äº›åŸºæœ¬çš„æ•°æ®ç»“æ„.åé¢å°±åº”è¯¥å­¦ä¹ Layerå±‚ä¸­å¯¹æ•°æ®çš„ä¸€äº›å¤„ç†.</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>LZH007</h1>
                <div class="site-des">LZHçš„æŠ€æœ¯æ‚äº‹å°åšå®¢~</div>
                <div class="social">











  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="MAC%20OS.html"><strong>MAC OS</strong></a>
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>æœºå™¨å­¦ä¹ </strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Pythonç»ƒä¹ </strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>å›¾åƒå»é›¾æŠ€æœ¯</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="14980252836745.html">MACä¸‹openBlasçš„å®‰è£…</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14980135145677.html">Caffeä¸­çš„Net</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14974936829967.html">Caffeä¸­Layerçš„å­¦ä¹ </a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14974281629389.html">æ€æ ·ç§»é™¤OSXRESERVEDåˆ†åŒº(å¦‚æœBootCamp Assistantåœ¨å®‰è£…ä¹‹åæ²¡æœ‰æˆåŠŸåˆ é™¤è¿™ä¸ªåˆ†åŒº)</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14974276192148.html">é‡æ–°åˆ©ç”¨boot campå®‰è£…win10</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
