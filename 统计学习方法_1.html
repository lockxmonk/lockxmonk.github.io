<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  统计学习方法 - LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">统计学习方法</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Python练习</a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">图像去雾技术</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14897993109270.html">
                
                  <h1>决策树</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">决策树模型与学习</a>
</li>
<li>
<a href="#toc_1">决策树与if-then规则</a>
</li>
<li>
<a href="#toc_2">决策树与条件概率分布</a>
</li>
<li>
<a href="#toc_3">特征选择</a>
<ul>
<li>
<a href="#toc_4">特征选择问题</a>
</li>
<li>
<a href="#toc_5">信息增益</a>
</li>
<li>
<a href="#toc_6">信息增益比</a>
</li>
</ul>
</li>
</ul>


<blockquote>
<p>决策树（decision tree)是一种基本的分类与回归方法。本章主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。这些决策树学习的思想主要来源于由Quinlan在1986年提出的ID3算法和1993年提出的C4。5算法,以及由Breiman等人在1984年提出的CART算法。</p>
</blockquote>

<h2 id="toc_0">决策树模型与学习</h2>

<p>定义（决策树）：分类决策树模型是一种描述对实例进行分类的树形结 构。决策树由结点（node)和有向边（directed edge)组成。结点有两种类型：内部结点（internal node)和叶结点（leafnode)。内部结点表示一个特征或属性，叶结点表示一个类。</p>

<p>用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p>

<p>图5.1是一个决策树的示意图。图中圆和方框分别表示内部结点和叶结点。<br/>
<img src="media/14897993109270/14897996064452.jpg" alt=""/></p>

<h2 id="toc_1">决策树与if-then规则</h2>

<p>可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</p>

<h2 id="toc_2">决策树与条件概率分布</h2>

<p>决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition)上。将特征空间划分为互不相交的单元（cell)或区域（region),并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量， Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>

<p>图5.2 (a)示意地表示了特征空间的一个划分。图中的大正方形表示特征空间。这个大正方形被若干个小矩形分割，每个小矩形表示一个单元。特征空间划分上的单元构成了一个集合，X取值为单元的集合。为简单起见，假设只有两类:正类和负类，即Y取值为+1和-1。小矩形中的数字表示单元的类。图5.2(b)示意地表示特征空间划分确定时，特征（单元）给定条件下类的条件概率分布。 图5.2 (b)中条件概率分布对应于图5.2 (a)的划分。当某个单元c的条件概率满足\(P(Y=+1|X= C)&gt;0.5\)时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例。图5.2(c)为对应于图5.2 (b)中条件概率分布的决策树。<br/>
<img src="media/14897993109270/14898004011784.jpg" alt=""/></p>

<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该 不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p>

<p>决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。<br/>
当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal)的.<br/>
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。<mark>开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去：如果还有子集 不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。</mark></p>

<p>以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即<mark>可能发生过拟合现象</mark>。我们需要对己生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。<mark>具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点</mark>。</p>

<p>如果特征数童很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。</p>

<p>可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。</p>

<p>决策树学习常用的算法有ID3、C4.5与CART,下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。</p>

<h2 id="toc_3">特征选择</h2>

<h3 id="toc_4">特征选择问题</h3>

<p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。</p>

<p>首先通过一个例子来说明特征选择问题：</p>

<p><strong>例5.1：表5.1是一个由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性)：第1个特征是年龄，有3个可能值：青年，中年，老年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子，有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一 般。表的最后一列是类别，是否同意贷款，取2个值：是，否.</strong></p>

<p><img src="media/14897993109270/14898020066279.jpg" alt=""/></p>

<p>希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p>

<p>特征选择是决定用哪个特征来划分特征空间。</p>

<p>图5.3表示从表5.1数据学习到的两个可能的决策树，分别由两个不同特征的根结点构成。图5.3(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图5.3(b)所示的根结点的特征是有工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特 征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益 (information gain〉就能够很好地表示这一直观的准则.</p>

<h3 id="toc_5">信息增益</h3>

<p>为了便于说明，先给出熵与条件熵的定义。<br/>
在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为：<br/>
\(P(X=x_i) = P_i  , i=1,2,3...n\)<br/>
则随机变量X的熵定义为：<br/>
\(H(X) = -\sum^{n}_{i=1}{p_ilogp_i}\)<br/>
在上式中，若\(p_i=0\),则定义\(olog0=0\)。通常，上式中的对数以2为底或者以e为底（自然对数），这时熵的单位分别称作比特(bit)或纳特(nat).由定义可 知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作开\(H_{(p)}\),即<br/>
<img src="media/14897993109270/14898028331151.jpg" alt=""/><br/>
熵越大，随机变量的不确定性就越大。从定义可验证：<br/>
<img src="media/14897993109270/14898033210936.jpg" alt=""/></p>

<p>设有随机变量(X,Y),其联合概率为：<br/>
<img src="media/14897993109270/14898040283563.jpg" alt=""/><br/>
条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy) H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望<br/>
<img src="media/14897993109270/14898042638745.jpg" alt=""/><br/>
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵 (empirical entropy)和经验条件熵（empirical conditional entropy )。此时，如果有0概率，令0log0=0。<br/>
信息增益（information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>

<p>信息增益的定义如下所示：<br/>
<img src="media/14897993109270/14898059454405.jpg" alt=""/></p>

<p>决策树学习应用信息增益准则选择特征。<mark>给定训练数据集D和特征A,经验 熵H(D)表示对数据集D进行分类的不确定性。而经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益</mark>，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。<br/>
根据信息增益准则的特征选择方法是：对训练数据集（或子集）D,计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>

<p>下面讲述信息增益的算法：<br/>
<img src="media/14897993109270/14898066826354.jpg" alt=""/><br/>
<img src="media/14897993109270/14898066953482.jpg" alt=""/></p>

<p>下面举例：<br/>
<img src="media/14897993109270/14898226986668.jpg" alt=""/><br/>
<img src="media/14897993109270/14898227425071.jpg" alt=""/></p>

<h3 id="toc_6">信息增益比</h3>

<p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问題困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比（information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。<br/>
定义（信息增益比）：特征A对训练数据集D的信息增益比\(g_R(D,A)\)定义为其信息增益\(g(D,A)\)与训练数据集D的经验熵H(D)之比：<br/>
<img src="media/14897993109270/14898241117010.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14893694433396.html">
                
                  <h1>朴素贝叶斯法</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">朴素贝叶斯的学习与分类</a>
<ul>
<li>
<a href="#toc_1">基本方法</a>
</li>
<li>
<a href="#toc_2">后验概率最大化的含义</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">朴素贝叶斯发的参数估计</a>
<ul>
<li>
<a href="#toc_4">极大似然估计</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">学习与分类算法</a>
</li>
<li>
<a href="#toc_6">贝叶斯估计</a>
</li>
</ul>


<p>朴素贝叶斯（naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布：然后基于此模型，对给定的输入:x，利用贝叶斯定理求出后验概率最大的输出:y。朴素贝叶斯法实现简单，学习与预测的效率都很髙，是一种常用的方法.</p>

<h2 id="toc_0">朴素贝叶斯的学习与分类</h2>

<h3 id="toc_1">基本方法</h3>

<p>设输入空间\(\chi \subseteq R^n\)为n维向量的集合，输出空间为类标记集合\(\mathcal{Y}={c_1,c_2,...c_k}\)。输入为特征向量\(x\in \chi\),输出为类标记\(y\in mathcal{Y}\)。X是定义在输入空间\(\chi\)上的随机向量，Y是定义在输出空间\(\mathcal{Y}\)上的随机变量。P(X,Y)是X和Y的联合概率分布。训练数据集：<br/>
\(T = {(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)<br/>
由P(X,Y)独立同分布产生。</p>

<p>朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y)。具体的，学习以下先验概率分布及条件概率分布。先验概率分布：<br/>
\(P(Y = c_k), k=1,2,3...K\)</p>

<p>条件概率分布：<br/>
\(P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,...K\)<br/>
于是学习到联合概率分布P(X,Y)。</p>

<p>条件概率分布P(X=x|Y=\(c_k\))有指数级数量的参数，其估计实际是不可行的，事实上，假设\(x^{(j)}可取值有S_j个，j=1,2,3...n,Y可取值为K个，那么参数个数为K\prod_{j=1}^n{S_j}\)</p>

<p>朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：<br/>
<img src="media/14893694433396/14897352338918.jpg" alt=""/></p>

<p>朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴 素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。<br/>
朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布\(P(Y=C_k|X=x)\),将后验概率最大的类作为x的类输出。后验概率计算根据贝 叶斯定理进行：<br/>
<img src="media/14893694433396/14897352467770.jpg" alt=""/></p>

<h3 id="toc_2">后验概率最大化的含义</h3>

<p>朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。假设选择0-1损失函数：<br/>
<img src="media/14893694433396/14897368963380.jpg" alt=""/><br/>
式中f(X)是分类决策函数。这时，期望风险函数为：<br/>
<img src="media/14893694433396/14897369463646.jpg" alt=""/><br/>
期望是对联合分布P(X,Y)取的，由此取条件期望：<br/>
<img src="media/14893694433396/14897370507779.jpg" alt=""/><br/>
为了是期望风险最小化，只需要对\(X=x\)逐个极小化，由此得到：<br/>
<img src="media/14893694433396/14897371045644.jpg" alt=""/><br/>
这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：<br/>
<img src="media/14893694433396/14897380067414.jpg" alt=""/><br/>
即朴素贝叶斯法所采用的原理。</p>

<h2 id="toc_3">朴素贝叶斯发的参数估计</h2>

<h3 id="toc_4">极大似然估计</h3>

<p>在朴素贝叶斯法中，学习意味着估计\(P(Y=c_k)\)和\(P(X^{(j)}=x^{(j)} |Y=c_k)\)。可以应用极大似然估计法估计相应的概率。先验概率\(P(Y=c_k)\)的极大似然估计是:<br/>
<img src="media/14893694433396/14897383598198.jpg" alt=""/><br/>
设第j个特征\(x^{(j)}\)可能取值的几何为{\(a_{j1},a_{j2}....a_{jS_j}\)},条件概率\(P(X^{j}=a_{jl}|Y=c_k)\)的极大似然估计是：<br/>
<img src="media/14893694433396/14897397226736.jpg" alt=""/></p>

<h2 id="toc_5">学习与分类算法</h2>

<p>下面给出朴素贝叶斯的学习与分类方法。</p>

<p>算法(朴素贝叶斯算法)：<br/>
<img src="media/14893694433396/14897403452985.jpg" alt=""/></p>

<p>下面用一个例题来解释说明上述算法：<br/>
<img src="media/14893694433396/14897405687808.jpg" alt=""/><br/>
<img src="media/14893694433396/14897405876725.jpg" alt=""/></p>

<h2 id="toc_6">贝叶斯估计</h2>

<p>用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差，解决这一问题的方法是采用贝叶斯估计。具 体地，条件概率的贝叶斯估计是：<br/>
<img src="media/14893694433396/14897409238342.jpg" alt=""/></p>

<p>这次再用上述例题来举例，如下所示：<br/>
<img src="media/14893694433396/14897415528528.jpg" alt=""/></p>

<blockquote>
<p><strong>总结：朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。</strong></p>
</blockquote>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14891936534415.html">
                
                  <h1>k近邻法</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">k近邻算法</a>
</li>
<li>
<a href="#toc_1">k近邻模型</a>
<ul>
<li>
<a href="#toc_2">模型</a>
</li>
<li>
<a href="#toc_3">距离度量</a>
</li>
<li>
<a href="#toc_4">k值的选择</a>
</li>
<li>
<a href="#toc_5">分类决策规则</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">k近邻法的实现：kd树</a>
<ul>
<li>
<a href="#toc_7">构造kd树</a>
</li>
<li>
<a href="#toc_8">搜索kd树</a>
</li>
</ul>
</li>
</ul>


<p>k近邻法（k-nearestneighbor,k-NN)是一种基本分类与回归方法。本书只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间 的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别己定。分类时，对新的实例，根据其k个最近邻的训练实例的类别， 通过多数表决等方式进行预测。因此，k近邻法不具有显式的学习过程。近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型“，k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。k近邻法1968年由Cover和Hart提出。</p>

<h2 id="toc_0">k近邻算法</h2>

<p>k近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类，下面叙述k邻近算法。<br/>
<img src="media/14891936534415/14891944741165.jpg" alt=""/><br/>
<img src="media/14891936534415/14891946432255.jpg" alt=""/></p>

<h2 id="toc_1">k近邻模型</h2>

<p>k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。</p>

<h3 id="toc_2">模型</h3>

<p>k近邻算法中，当训练集、距离度量（如欧氏距离）、k值及分类局侧规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。</p>

<p>特征空间中，对每个训练实例点\(x_i\),距离该点比其他点更近的所有点组成一个区域，叫作单元（cell).每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例\(x_i\)的类\(y_i\),作为其单元中所有点的类标记（class label)。这样，每个单元的实例点的类别是确定的。下图是二维特征空间划分的一个例子。<br/>
<img src="media/14891936534415/14891961608422.jpg" alt=""/></p>

<h3 id="toc_3">距离度量</h3>

<p>特征空间中两个实例点的距离是两个实例点相似程度的反映.k近邻模型的特征空间一般是n维实数向量空间\(R^n\)。使用的距离是欧氏距离，但也可以是其他距离，如更一般的\(L_p\)距离（\(L_p -distance\))或Minkowski距离（Minkowski distance)。<br/>
关于特征空间中两坐标点的距离定义为：<br/>
<img src="media/14891936534415/14891974598310.jpg" alt=""/></p>

<p>不同的距离定义，所求出的最近邻点是不同的。<br/>
<img src="media/14891936534415/14891979526147.jpg" alt=""/></p>

<h3 id="toc_4">k值的选择</h3>

<p>k值的选择会对k近邻法的结果产生重大影响。</p>

<p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error)会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error) 会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错.换句话说，值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>

<p>如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例 较远的（不相似的）训练实例也会对预测起作用，使预测发生错误.k值的增大就意味着整体的模型变得简单。</p>

<p>如果k=N,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。</p>

<p>在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p>

<h3 id="toc_5">分类决策规则</h3>

<p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</p>

<p>多数表决规则（majority voting rule)有如下解释：如果分类的损失函数为0-1损失函数，分类函数为：<br/>
<img src="media/14891936534415/14891994327344.jpg" alt=""/></p>

<p>那么误分类的概率为：<br/>
<img src="media/14891936534415/14891994564311.jpg" alt=""/><br/>
对给定的实例\(x \in \chi\),其中最邻近的k个训练实例点构成集合\(N_k(x)\),如果涵盖\(N_k(x)\)的区域的类别是\(C_j\),那么误分类率是：<br/>
<img src="media/14891936534415/14891997004694.jpg" alt=""/><br/>
要使误分类率最小即经验风险最小，就要使\(\sum_{x_i\in{N_k(x)}}I(y_i=c_j)\)最大，<mark>所以多数表决规则等价于风险最小化</mark>。</p>

<h2 id="toc_6">k近邻法的实现：kd树</h2>

<p>实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻捜索。这点在特征空间的维数大及训练数据容量大时尤其必要。</p>

<p>k近邻法最简单的实现方法是线性扫描（linear  scan)。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。</p>

<p>为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的kd树(kd tree)方法.</p>

<h3 id="toc_7">构造kd树</h3>

<p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition)。构造kd树相 当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。</p>

<p>构造kd树的方法如下：构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；通过下面的递归方法，不断地对k维空间进行切分，生成子结 点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）：这时，实例被分到两个子区域.这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。</p>

<p>通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数（median)为切分点，这样得到的kd树是平衡的。<mark>注意，平衡的kd树搜索时的效率未必是最优的</mark>。<br/>
下面给出构造kd树的算法：<br/>
<img src="media/14891936534415/14892010553846.jpg" alt=""/><br/>
<img src="media/14891936534415/14892019093068.jpg" alt=""/></p>

<p>下面给出一个例子，来反应上述算法的用途：<br/>
<strong>（中位数：一组数据按大小顺序排列起来，处在中间位置的一个数或两个数的平均值。）</strong><br/>
<img src="media/14891936534415/14892021282433.jpg" alt=""/><br/>
<img src="media/14891936534415/14892121527765.jpg" alt=""/></p>

<h3 id="toc_8">搜索kd树</h3>

<p>下面介绍如何利用kd树进行k近邻搜索。可以看到，利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。这里以最近邻为例加以叙述，同样的方法可以应用到k近邻。</p>

<p>给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点：然后从该叶结点出发，依次回退到父结点；不断査找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提髙。</p>

<p>包含目标点的叶结点对应包含目标点的最小超矩形区域。以此叶结点的实例点作为当前最近点。目标点的最近邻一定在以目标点为中心并通过当前最近点的 超球体的内部（参阅图3.5)。然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点，将此点作为新的当前最近点。算法转到更上一级的父结点，继续上述过程。如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。<br/>
下面叙述用kd树的最近邻搜索算法：</p>

<pre><code>算法3.3 (用kd树的最近邻搜索）

输入：己构造的kd树；目标点X；
输出：x的最近邻。
(1) 在kd树中找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移 动到右子结点。直到子结点为叶结点为止.
(2) 以此叶结点为”当前最近点“
(3) 递归地向上回退，在每个结点进行以下操作：
(a) 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”
(b) 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检査另一子结点对应
的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。
如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索：
如果不相交，向上回退。
(4)当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。
    
如果实例点是随机分布的，kd树搜索的平均计算复杂度是O(logN)，这里N是训练实例数.kd树更适用于训练实例数远大于空间维数时的k近邻搜索。当空 间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。
</code></pre>

<p>下面通过一个例题来说明搜索方法：</p>

<p><img src="media/14891936534415/14892141583109.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/11</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14891071598954.html">
                
                  <h1>感知机算法的收敛性</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">感知机学习算法的对偶形式</a>
</li>
</ul>


<p>算法的收敛性主要是证明，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限 次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p>

<p>将偏置b并入权重向量w，记做\(\hat{w} = (w^T,b)^T\)同样也将输入向量加以扩充，加进常数1，记做：\(\hat{x} = (x^T,1)^T\),这样，\(\hat{x}\in R^{n+1}\)，\(\hat{w}\in R^{n+1}\) 显然，\(\hat{w}*\hat{x}=w*x+b\)。<br/>
<img src="media/14891071598954/14891094628135.jpg" alt=""/></p>

<p>上述定理主要说明误分类的次数<code>k</code>是有上界的，算法具有收敛性。</p>

<p>下面是该定理的证明过程：<br/>
<img src="media/14891071598954/14891100385544.jpg" alt=""/><br/>
<img src="media/14891071598954/14891100637572.jpg" alt=""/><br/>
<img src="media/14891071598954/14891105642852.jpg" alt=""/><br/>
<img src="media/14891071598954/14891105771778.jpg" alt=""/></p>

<blockquote>
<p>所以定理表明，误分类的次数<code>k</code>是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。但是感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。这就是线性支持向量机的想法。当训练集线性不可分时，感知机学习算法不收敛，迭代结果 就会发生震荡。</p>
</blockquote>

<h2 id="toc_0">感知机学习算法的对偶形式</h2>

<p>现在考虑感知机学习算法的对偶形式。感知机学习算法的原始形式和对偶形式与支持向量机学习算法的原始形式和对偶形式相对应。</p>

<p>对偶形式的基本思想是，将w和b表示为实例\(x_{i}\)和标记\(y_i\)的线性组合形式，通过求解其系数而求得w和b。不失一般性,在算法2.1中可假设初始值\(w_0,b_0\)均为0，对误分类点\((x_i,y_i)\)通过：<br/>
\(w\leftarrow w+ηy_ix_i\)<br/>
\(b\leftarrow b+ηy_i\)<br/>
逐步修改w，b，设修改n次，则w,b关于\((x_i,y_i)\)的增量分别是\(α_ix_iy_i和α_iy_i\)这里\(α=n_iη\)。这样，从学习过程不难看出，最后学习到的w,b可以分别表示为：<img src="media/14891071598954/14891128523346.jpg" alt=""/><br/>
这里，\(\alpha_i\geq0,i=1,2,...N,当\eta=1时\)，表示第i个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响最大。</p>

<p>下面对照原始形式来叙述感知机学习算法的对偶形式。<br/>
<img src="media/14891071598954/14891134427824.jpg" alt=""/><br/>
<img src="media/14891071598954/14891134670827.jpg" alt=""/><br/>
对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。<br/>
\(G = [x_i * x_j]_{N*N}\)</p>

<p>同样根据该算法有例题如下：<br/>
<img src="media/14891071598954/14891137936409.jpg" alt=""/><br/>
<img src="media/14891071598954/14891170220533.jpg" alt=""/><br/>
对照例2.1，结果一致，迭代步骤也是互相对应的。<br/>
与原始形式一样，感知机学习算法的对偶形式迭代是收敛的。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14890437581129.html">
                
                  <h1>感知机</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">感知机模型</a>
</li>
<li>
<a href="#toc_1">感知机的学习策略</a>
<ul>
<li>
<a href="#toc_2">数据集的线性可分性</a>
</li>
<li>
<a href="#toc_3">感知机学习策略</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">感知机学习算法</a>
<ul>
<li>
<a href="#toc_5">感知机学习算法的原始形式</a>
</li>
</ul>
</li>
</ul>


<p>感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别（一般为+1和-1），感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面。感知机也是神经网络与支持向量机的基础。</p>

<h2 id="toc_0">感知机模型</h2>

<p>感知机的定义为：<br/>
<img src="media/14890437581129/14890447646559.jpg" alt=""/></p>

<p>感知机有如下的几何解释：<br/>
线性方程：w*x + b = 0<br/>
对应于特征空间R中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点就被划分为正，负两类，如下图所示：<br/>
<img src="media/14890437581129/14890449623708.jpg" alt=""/></p>

<p>所以求感知机的模型，也就是去求得模型参数w，b。感知机预测，通过学习得到的感知机模型，对于新输入实例给出其对应的输出类别。</p>

<h2 id="toc_1">感知机的学习策略</h2>

<h3 id="toc_2">数据集的线性可分性</h3>

<p><img src="media/14890437581129/14890453486666.jpg" alt=""/></p>

<h3 id="toc_3">感知机学习策略</h3>

<p>如果训练数据集是线性可分的，那么我们需要确定一个学习策略，也就是定义（经验）损失函数并将损失函数极小化。</p>

<p>损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数w,b的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。为此，首先写出输入控件R中任一点x到平面S的距离：<img src="media/14890437581129/14890456220821.jpg" alt=""/><br/>
这里，||w||是w的2范数。<br/>
其次，对于五分类的数据（x，y）来说，<br/>
<code>-y(w*x+b)&gt;0</code>成立。因为当<code>w*x+b&gt;0</code>时，<code>y=-1</code>，而当<code>w*x+b&lt;0</code>时<code>y=+1</code>，因此，误分类点x到超平面S的距离是：<br/>
<img src="media/14890437581129/14890460147136.jpg" alt=""/><br/>
这样，假设超平面S的误分类点集合为M，那么所有的误分类点到超平面S的总距离为：<br/>
<img src="media/14890437581129/14890462066356.jpg" alt=""/></p>

<p>所以，感知机的损失函数定义为：<img src="media/14890437581129/14890462888502.jpg" alt=""/></p>

<p>显然，损失函数是非负的，如果没有误分类点，那么损失函数为0，而误分类点越少，误分类点离超平面越近，损失函数值越小。一个特定的样本点的损失函数：在误分类时是参数w，b的线性函数，在正确分类时是0，因此给定训练数据集T，损失函数是w，b的连续可导函数。<br/>
总的来说我们的感知机学习策略就是在假设空间中选取使损失函数式最小的模型参数w，b。</p>

<h2 id="toc_4">感知机学习算法</h2>

<p>我们的策略已经明确，就是求解损失函数式的最优化，我们这里最优化的方法是随机梯度下降法。</p>

<h3 id="toc_5">感知机学习算法的原始形式</h3>

<p>感知机学习算法是对以下最优化问题的算法，给定一个训练集：<img src="media/14890437581129/14890473616479.jpg" alt=""/><br/>
感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent)。首先，任意选取一个超平面然后用梯度下降法不断地极小化目标函数<code>（2.5）</code>。极小化过程中不是一次使M中所有误分类点的梯度下降， 而是一次随机选取一个误分类点使其梯度下降。<br/>
假设误分类点集合M是固定的，那么损失函数的梯度由<img src="media/14890437581129/14890475151426.jpg" alt=""/><br/>
给出。<br/>
随机选取一个误分类点<code>(x,y)</code>，对w,b进行更新：<img src="media/14890437581129/14890475562013.jpg" alt=""/><br/>
式中η(0&lt;η≤1)是步长，在统计学习中又称为学习率，这样，通过迭代可以期待损失函数不断减小，直到为0，综上所述，得到如下算法：<img src="media/14890437581129/14890483224015.jpg" alt=""/><br/>
这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p>

<p>上述算法是感知机学习的基本算法，对应于后面的对偶形式，称为原始形式。感知机学习算法简单且易于实现。</p>

<p>该算法使用方法如下：<br/>
<img src="media/14890437581129/14890503525005.jpg" alt=""/><br/>
<img src="media/14890437581129/14890503748369.jpg" alt=""/><br/>
<img src="media/14890437581129/14890503942221.jpg" alt=""/><br/>
这是在计算中误分类点先后取<code>x1,x3,x3,x3,x1,x3,x3</code>得到的分离超平面和感 知机模型，如果在计算中误分类点依次取<code>x1,x3,x3,x3,x2,x3,x3,x3,x1,x3,x3</code>那么得到的分离超平面是2x<sup>1</sup>+x<sup>2</sup>-5=0</p>

<p>可见，感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="统计学习方法.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>LZH007</h1>
                <div class="site-des">LZH的技术杂事小博客~</div>
                <div class="social">











  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>统计学习方法</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Python练习</strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>图像去雾技术</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="14963894105351.html">提交测试</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957648679184.html">UDP编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957594179111.html">网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14957005904776.html">图形界面</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14956930097908.html">HTMLParser</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
