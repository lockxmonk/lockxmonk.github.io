<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="MAC%20OS.html">MAC OS</a></li>
        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">统计学习方法</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Python练习</a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">图像去雾技术</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html">算法学习</a></li>
        
            <li><a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html">常见面试问题</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14920445399813.html">
                
                  <h1>模型学习的最优化算法（接上一文）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">改进的迭代尺度法</a>
</li>
</ul>


<p>逻辑斯谛回归模型、最大嫡模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解.从最优化的观点看，这时的目标函数具有很好的性质.<font color=red>它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解.</font>常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法.牛顿法或拟牛顿法一般收敛速度更快.</p>

<p>这次主要学习基于改进的迭代尺度法与拟牛顿法的最大熵模型学习算法。（还有梯度下降法，这次不过多学习）</p>

<h2 id="toc_0">改进的迭代尺度法</h2>

<p>改进的迭代尺度法（improved iterative scaling，IIS)是一种最大熵模型学习的最优化算法.</p>

<p>已知最大熵模型为：<br/>
<img src="media/14920445399813/14920447866369.jpg" alt=""/><br/>
对数似然函数为：<br/>
<img src="media/14920445399813/14920449425684.jpg" alt=""/><br/>
目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值\(\hat{w}\)</p>

<p>改进的迭代尺度算法（iis）的想法是：假设最大熵模型当前的参数向量是\(w=(w_1,w_2,...,w_n)^T\),我们 希望找到一个新的参数向量\(w+\delta = (w_1+\delta_1,w_2+\delta_2,....,w_n+\delta_n)^T\)，使得模型的对数似然函数值增大.如果能有这样一种参数向量更新的方法\(\tau:w \rightarrow w+\delta\),那么就 可以重复使用这一方法，直至找到对数似然函数的最大值.</p>

<p>对于给定的经验分布\(\tilde P(x,y)\),模型参数从\(w到w+\delta\)，对数似然函数的改变量是：<br/>
<img src="media/14920445399813/14920457816508.jpg" alt=""/></p>

<p>如果能找到适当的\(\delta使下届A(\delta | w)\)提高，那么对数似然函数也会提高。然而，函数\(A(\delta | w)\)中的\(\delta\)是一个向量，含有多个变量，不容易同时优化，IIS试图以此只优化其中一个变量\(\delta_i\)，而固定其他变量\(\delta_j,i  \neq j\).</p>

<p>为达到这一目的，IIS进一步降低下界\(A(\delta | w)\),具体的，IIS引进一个量\(f^{\#}(x,y)\):<br/>
<img src="media/14920445399813/14920481227290.jpg" alt=""/><br/>
因为\(f_i\)是一个二值函数，故\(f^{\#}(x,y)\)表示所有特征在（x，y）出现的次数，这样，\(A(\delta | w)\)可以改写为：</p>

<p><img src="media/14920445399813/14920483320007.jpg" alt=""/></p>

<p><strong>下面给出IIS算法：</strong><br/>
<img src="media/14920445399813/14920487060621.jpg" alt=""/><br/>
<img src="media/14920445399813/14920491294532.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14917858710123.html">
                
                  <h1>logistic regression(逻辑斯蒂回归)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">逻辑斯蒂回归模型</a>
<ul>
<li>
<a href="#toc_1">逻辑斯蒂分布</a>
</li>
<li>
<a href="#toc_2">二项逻辑斯蒂回归模型</a>
</li>
<li>
<a href="#toc_3">模型参数估计</a>
</li>
<li>
<a href="#toc_4">多项了逻辑斯蒂回归</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_5">最大熵模型</a>
<ul>
<li>
<a href="#toc_6">最大熵原理</a>
<ul>
<li>
<a href="#toc_7">最大熵模型的定义</a>
</li>
<li>
<a href="#toc_8">最大熵模型的学习</a>
</li>
<li>
<a href="#toc_9">极大似然估计</a>
</li>
</ul>
</li>
</ul>


<p>简介：逻辑斯谛回归（logistic regression)是统计学习中的经典分类方法.最大熵是 概率模型学习的一个准则，将其推广到分类问题得到最大熵模型(maximum entropy model).逻辑斯谛回归模型与最大熵模型都属于对数线性模型.</p>

<h2 id="toc_0">逻辑斯蒂回归模型</h2>

<h3 id="toc_1">逻辑斯蒂分布</h3>

<p><img src="media/14917858710123/14918919297549.jpg" alt=""/></p>

<h3 id="toc_2">二项逻辑斯蒂回归模型</h3>

<p>二项逻辑斯诗回归模型（binomial logistic regression model)是一种分类模型，由条件概率分布\(P(Y|X)\)表示，形式为参数化的逻辑斯谛分布.这里，随机变量X取值为实数，随机变量Y取值为1或0.我们通过监督学习的方法来估计模型参数.</p>

<p><img src="media/14917858710123/14918930389977.jpg" alt=""/><br/>
现在考査逻辑斯谛回归模型的特点.一个事件的几率（odds)是指该事件发生的概率与该事件不发生的概率的比值.如果事件发生的概率是p,那么该亊件的几率是\(\frac{1}{1-p}\) ,该事件的对数几率(log odds)或logit函数是<br/>
<img src="media/14917858710123/14918936472837.jpg" alt=""/><br/>
<font color = red>这就是说，在逻辑斯谛回归模型中，输出y = l的对数几率是输入x的线性函数.或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型.</font></p>

<p>换一个角度看，考虑对输入x进行分类的线性函数\(w*x\),其值域为实数域.注意，这里\(x\in R^{n+1},w \in R^{n+1}\).通过逻辑斯谛回归模型定义式(6.5)可以将线性函数\(w*x\)转换为概率：<br/>
<img src="media/14917858710123/14918951209571.jpg" alt=""/></p>

<p><strong>这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0(如图6.1所示).这样的模型就是逻辑斯谛回归模型.</strong></p>

<h3 id="toc_3">模型参数估计</h3>

<p>逻辑斯谛回归模型学习时，对于给定的训练数据集\(T=\{{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}}\),其中\(x_i\in R^n,y_i \in \{{0,1\}}\)，<strong>可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型.</strong><br/>
<img src="media/14917858710123/14918969386189.jpg" alt=""/></p>

<h3 id="toc_4">多项了逻辑斯蒂回归</h3>

<p>上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类.可以将其推广为多项逻辑斯缔回归模型(multi-nominal logistic regression model)，用于多类分类.假设离散型随机变量Y的取值集合是{1,2...K}，那么多项逻辑斯谛回归模型是:<br/>
<img src="media/14917858710123/14918973306162.jpg" alt=""/><br/>
二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯讳回归.</p>

<h1 id="toc_5">最大熵模型</h1>

<p>最大熵模型（maximum entropy model)由最大熵原理推导实现.这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。</p>

<h2 id="toc_6">最大熵原理</h2>

<p>最大熵原理是概率模型学习的一个准则.<font color=red>最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型.通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型.</font></p>

<p>假设离散随机变量X的概率分布是p(X),则其熵是：<br/>
<img src="media/14917858710123/14919006601164.jpg" alt=""/><br/>
直观地，<font color=red>最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件.在没有更多信息的情况下，那些不确定的部分都是“等可能的”.最大熵原理通过熵的最大化来表示等可能性.</font>“等可能”不容易操作，而熵则是一个可优化的数值指标.</p>

<p>首先，可以通过一个简单的例子来了解一下最大熵原理.<br/>
<img src="media/14917858710123/14919019478267.jpg" alt=""/></p>

<p>图6.2提供了用最大熵原理进行概率模型选择的几何解释.概率模型集合P可由欧氏空间中的单纯形(simplex)表示，如左图的三角形(2-单纯形).一个点代表一个模型，整个单纯形代表模型集合.右图上的一条直线对应于一个约束 条件，直线的交集对应于满足所有约束条件的模型集合.一般地，这样的模型仍有无穷多个.学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则给出最优模型选择的一个准则.<br/>
<img src="media/14917858710123/14919593499530.jpg" alt=""/></p>

<h3 id="toc_7">最大熵模型的定义</h3>

<p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型. </p>

<p>假设分类模型是一个条件概率分布\(P(Y|X),X \in \mathcal{X} \subseteq R^n\)表示输入，\(Y \in \mathcal{Y}\)表示输出，\(\mathcal{X}\)和\(\mathcal{Y}\)分别是输入和输出的集合.这个模型表示的是对于给定的输入\(\mathcal{X}\)，以条作概率\(P(Y|X)\)输出Y.<br/>
给定一个训练数据集<br/>
\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)<br/>
学习的目标是用最大熵原理选择最好的分类模型。</p>

<p>首先考虑模型应该满足的条件，给定训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布，分别以\(\tilde{P}(X,Y)和\tilde{P}(X)\)表示。这里：<br/>
<img src="media/14917858710123/14919609290680.jpg" alt=""/><br/>
 用特征函数\(f(x,y)描述输入x和输出y之间的某一个事实\)。其定义是：<br/>
 <img src="media/14917858710123/14919611561526.jpg" alt=""/><br/>
特征函数f(x,y)关于经验分布\(\tilde{P}(X,Y)\)的期望值，用\(E_\tilde{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919613926643.jpg" alt=""/><br/>
特征函数f(x,y)关于模型P(Y|X)与经验分布\(\tilde{P}(X)\)的期望值，用\(E_{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919617357953.jpg" alt=""/></p>

<p>最大熵模型的定义为：<br/>
<img src="media/14917858710123/14919625972087.jpg" alt=""/></p>

<h3 id="toc_8">最大熵模型的学习</h3>

<p>最大熵模型的学习过程就是求解最大熵模型的过程.最大熵模型的学习可以形式化为约束最优化问题.<br/>
对于给定的训练数据集\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)以及特征函数\(f_i(x,y),i=1,2,...n\),最大熵模型的学习等价于约束最优化问题：<br/>
<img src="media/14917858710123/14919630413332.jpg" alt=""/><br/>
求解约束最优化问题(6.14)〜(6.16)，所得出的解，就是最大熵模型学习的解.下面给出具体推导.<br/>
这里，将约束最优化的原始问题转换为无约束最优化的对偶问题.通过求解对偶问题求解原始问题.<br/>
<img src="media/14917858710123/14919633775267.jpg" alt=""/><br/>
<img src="media/14917858710123/14919633957280.jpg" alt=""/></p>

<p>下面举例来说明最大熵模型的计算：</p>

<p><strong>例：假设随机变量X有5个取值{A,B,C,D,E}，要估计各个值的概率P(A),P(B),P(C),P(D),P(E).</strong></p>

<p><img src="media/14917858710123/14919642672585.jpg" alt=""/><br/>
<img src="media/14917858710123/14919642856866.jpg" alt=""/><br/>
<img src="media/14917858710123/14919643007948.jpg" alt=""/></p>

<h3 id="toc_9">极大似然估计</h3>

<p>从以上最大熵模型学习中可以看出，最大熵模型是由式(6.22)、式(6.23)表示的 条件概率分布.下面证明对偶函数的极大化等价于最大嫡模型的极大似然估计.</p>

<p>己知训练数据的经验概率分布\(\tilde{P}(X,Y)\),条件概率分布\(P(Y|X)\)的对数似然 函数表示为:<br/>
<img src="media/14917858710123/14919659054476.jpg" alt=""/><br/>
当提哦啊煎概率分布P(y|x)是最大熵模型(6.22)和(6.23)时,对数似然函数\(L_ \tilde{p}(P_w)\)为：<br/>
<img src="media/14917858710123/14919660761710.jpg" alt=""/><br/>
比较算式(6.26)和式(6.27),可得：<br/>
<img src="media/14917858710123/14920442862315.jpg" alt=""/></p>

<p>既然对偶函数\(\psi(w)\)等价于对数似然函数\(L_{\tilde p}(P_w)\) ,于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实.<br/>
这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题.</p>

<p>可以将最大熵模型写成更一般的形式.<br/>
<img src="media/14917858710123/14920444597468.jpg" alt=""/><br/>
最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型(log linear model).模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14908575557810.html">
                
                  <h1>决策树生成--CART算法</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">CART算法</a>
<ul>
<li>
<a href="#toc_1">CART生成</a>
<ul>
<li>
<a href="#toc_2">1.回归树的生成</a>
</li>
<li>
<a href="#toc_3">2.分类树的生成</a>
</li>
<li>
<a href="#toc_4">CART剪枝</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">CART算法</h2>

<p>分类与回归树(classification and regression tree, CART)模型由 Breiman等人在1984年提出，是应用广泛的决策树学习方法.CART同样由特征选择、树 的生成及剪枝组成，<mark>既可以用于分类也可以用于回归.</mark>以下将用于分类与回归的树统称为决策树.</p>

<p>CART是在给定输入随机变量X的条件下输出随机变量Y的条件概率分布的学习方法.CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支.这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布.</p>

<p><strong>CART算法由以下两步组成：</strong></p>

<p>1.决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大：<br/>
2.决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准.</p>

<h3 id="toc_1">CART生成</h3>

<p>决策树的生成就是递归地构建二叉决策树的过程.对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树.</p>

<h4 id="toc_2">1.回归树的生成</h4>

<p>假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集:<br/>
    \(D={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\)<br/>
考虑如何生成回归树。</p>

<p>一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上 的输出值.假设己将输入空间划分为M个单元\(R_1,R_2,R_3...R_M\),并且在每个单元\(R_M\)上 有一个固定的输出值\(c_m\)，于是回归树模型可表示为:<br/>
<img src="media/14908575557810/14909500378632.jpg" alt=""/></p>

<p>当输入空间的划分确定时，可以用平方误差\(\sum_{x_i\in R}(y_i-f(x_i))^2\)来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值.易知，单元\(R_m\)上的\(c_m\)的最优值\(\hat c_m\)是\(R_m\)上的所有输入实例\(x_i\)对应的输出\(y_i\)的均值，即:<br/>
<img src="media/14908575557810/14909504677321.jpg" alt=""/></p>

<p>问题是怎样对输入空间进行划分,这里采用启发式的方法，选择第j个变量\(x^{(j)}\)和它取的值s，作为切分变量（splitting variable)和切分点（splitting point),并定义两个区域：<br/>
<img src="media/14908575557810/14909505605933.jpg" alt=""/><br/>
然后寻找最优切分变量j和最优切分点s，最优地，求解：<br/>
<img src="media/14908575557810/14909506372664.jpg" alt=""/><br/>
对固定输入变量j可以找到最优切分点s:<br/>
<img src="media/14908575557810/14909506657025.jpg" alt=""/><br/>
便利所有输入变量，找到最优的切分变量j，构成一个对\((j,s)\).依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止.这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(least squares regression tree)，现将算法叙述如下：</p>

<p>最小二乘回归树生成算法：<br/>
<img src="media/14908575557810/14909510077124.jpg" alt=""/></p>

<h4 id="toc_3">2.分类树的生成</h4>

<p>分类树用基尼指数选择最有特征，同时决定该特征的最优二值切分点。</p>

<p><strong>基尼指数：</strong><br/>
<img src="media/14908575557810/14913540871077.jpg" alt=""/><br/>
如果样本集合D根据特征A是否取某一可能值a被分割成\(D_1和D_2\)两部分，即：<br/>
<img src="media/14908575557810/14913550101283.jpg" alt=""/></p>

<p>基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p>

<p>图5.7显示二类分类问题中基尼指数Gini(p)、熵（单位比特）之半和分类误差率的关系.横坐标表示概率P，纵坐标表示损失.可以看出基尼指数和 熵之半的曲线很接近，都可以近似地代表分类误差率.<br/>
<img src="media/14908575557810/14913559817897.jpg" alt=""/></p>

<p><strong>CART生成算法：</strong><br/>
<img src="media/14908575557810/14913560936133.jpg" alt=""/><br/>
<img src="media/14908575557810/14913561059742.jpg" alt=""/></p>

<p><strong>下面继续用例题来说明该算法：</strong><br/>
<img src="media/14898241272608/14908402191601.jpg" alt=""/><br/>
根据上表所给的训练数据集，应用CART算法生成决策树。</p>

<p><img src="media/14908575557810/14913582940784.jpg" alt=""/></p>

<h4 id="toc_4">CART剪枝</h4>

<p>CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小(模型变简单)，从而能够对未知数据有更准确的预测.CART剪枝算法由两步组成：首先从生成算法产生的决策树\(T_0\)底端开始不断剪枝，直到\(T_0\)的根结点，形成一个子树序列\( {T_0,T_1,...,T_n}\);然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树.</p>

<p>1.<strong>剪枝</strong>，形成一个子树序列<br/>
在剪枝过程中，计算子树的损失函数：<br/>
<img src="media/14908575557810/14913604606599.jpg" alt=""/><br/>
其中，T为任意子树，C(T)为对训练数据的预测误差(如基尼指数)，丨T丨为子树的叶结点个数，\(α \geq0\)为参数，\(C_{\alpha}(T)\)为参数是\(\alpha\)时的子树T的整体损失.参数\(\alpha\)权衡训练数据的拟合程度与模型的复杂度.</p>

<p>对固定的\(\alpha\), —定存在使损失函数\(C_{\alpha}(T)\)最小的子树，将其表示为\(T_{\alpha}\)。\(T_{\alpha}\)在损失函数\(C_{\alpha}(T)\)最小的意义下是最优的.容易验证这样的最优子树是唯一的.当\(\alpha\)大的时候，最优子树\(T_{\alpha}\)偏小；当\(\alpha\)小的时候，最优子树\(T_{\alpha}\)偏大.极端情况，当\(\alpha\) = 0时，整体树是最优的.当\(\alpha \rightarrow \infty \)时，根结点组成的单结点树是最优的.</p>

<p>Breiman等人证明：可以用递归的方法对树进行剪枝.将\(\alpha\)从小增大，\(0=\alpha_0&lt;\alpha_1&lt;...&lt;\alpha_n&lt;\infty,\)，产生一系列的区间\([\alpha_i,\alpha_{i+1}),i=0,1,...n;\)，剪枝得到的子树 序列对应着区间\(\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,...n;\)的最优子树序列\( {T_0,T_1,...,T_n}\),序列中的子树是嵌套的.</p>

<p><img src="media/14908575557810/14913621104324.jpg" alt=""/><br/>
<img src="media/14908575557810/14913625337737.jpg" alt=""/></p>

<p><strong>CART剪枝算法：</strong><br/>
<img src="media/14908575557810/14913629643758.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/30</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14898241272608.html">
                
                  <h1>决策树的生成</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">ID3算法</a>
</li>
<li>
<a href="#toc_1">C4.5d的生成算法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_2">决策树的剪枝</a>


<p>这次将学习决策树的生成算法，首先了解ID3的生成算法，然后再学习C4.5的生成算法，这些都是决策树学习的经典算法。</p>

<h2 id="toc_0">ID3算法</h2>

<p>ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树.具体方法是：从根结点（root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点：再对子结点递归地调用以上方法，构建决策树；直到所有特征的 信息增益均很小或没有特征可以选择为止.最后得到一个决策树.ID3相当于用极大似然法进行概率模型的选择.</p>

<p><strong>该算法的具体步骤为：</strong><br/>
<img src="media/14898241272608/14908400085381.jpg" alt=""/><br/>
<img src="media/14898241272608/14908400481942.jpg" alt=""/></p>

<p><strong>这里我们继续使用之前用过的表来进行计算：</strong><br/>
<img src="media/14898241272608/14908402191601.jpg" alt=""/><br/>
利用ID3算法建立决策树：<br/>
<img src="media/14898241272608/14908417653584.jpg" alt=""/><br/>
<mark>但是ID3只有树的生成，容易发生过拟合现象。</mark></p>

<h2 id="toc_1">C4.5d的生成算法</h2>

<p>C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进.C4.5在生成的过程中，用<mark>信息增益比</mark>来选择特征(ID3为信息增益).<br/>
<img src="media/14898241272608/14908421490687.jpg" alt=""/></p>

<h1 id="toc_2">决策树的剪枝</h1>

<p>决策树生成算法递归地产生决策树，直到不能继续下去为止.这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象.<mark>过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树</mark>.解决这个问题的办法是考虑决策树的复 杂度，对已生成的决策树进行简化.</p>

<p>在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning).具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型.</p>

<p>这里学习一种简单的决策树学习的剪枝算法。</p>

<p>决策树的剪枝往往通过极小化决策树整体的损失函数(loss fimction)或代价函数(costfimction)来实现.设树T的叶结点个数为\(|T|\)，t是树T的叶结点，该叶结点有\(N_{t}\)个样本点，其中k类的样本点有\(N_{tk}个，k=1,2,3...,K,H_i(T)\)为叶结点t上的经验熵，\(a\geq0\)为参数，则决策树学习的损失函数可以定义为:<br/>
<img src="media/14898241272608/14908428498530.jpg" alt=""/><br/>
式(5.14)中，c(t)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数\(a\geq0\)控制两者之间的影响.较大的\(\alpha\)促使选择较简单的模型(树)，较小的\(\alpha\)促使选择较复杂的模型(树).\(\alpha = 0\)意味着只考虑 模型与训练数据的拟合程度，不考虑模型的复杂度.</p>

<p>剪枝，就是当\(\alpha\)确定时，选择损失函数最小的模型，即损失函数最小的子树.当\(\alpha\)值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好.损失函数正好表示了对两者的平衡.</p>

<p>可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合.而决策树剪枝通过优化损失函数还考虑了减小模型复杂度，决策树生成学习局部的模型，而决策树剪枝学习整体的模型.</p>

<p>式(5.11)或式(5.14)定义的损失函数的极小化等价于正则化的极大似然估计.所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择.</p>

<p>下图是决策树的剪枝过程示意图。<br/>
<img src="media/14898241272608/14908438468056.jpg" alt=""/></p>

<p>剪枝算法如下：<br/>
<img src="media/14898241272608/14908438815566.jpg" alt=""/><br/>
<img src="media/14898241272608/14908438965218.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14897993109270.html">
                
                  <h1>决策树</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">决策树模型与学习</a>
</li>
<li>
<a href="#toc_1">决策树与if-then规则</a>
</li>
<li>
<a href="#toc_2">决策树与条件概率分布</a>
</li>
<li>
<a href="#toc_3">特征选择</a>
<ul>
<li>
<a href="#toc_4">特征选择问题</a>
</li>
<li>
<a href="#toc_5">信息增益</a>
</li>
<li>
<a href="#toc_6">信息增益比</a>
</li>
</ul>
</li>
</ul>


<blockquote>
<p>决策树（decision tree)是一种基本的分类与回归方法。本章主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。这些决策树学习的思想主要来源于由Quinlan在1986年提出的ID3算法和1993年提出的C4。5算法,以及由Breiman等人在1984年提出的CART算法。</p>
</blockquote>

<h2 id="toc_0">决策树模型与学习</h2>

<p>定义（决策树）：分类决策树模型是一种描述对实例进行分类的树形结 构。决策树由结点（node)和有向边（directed edge)组成。结点有两种类型：内部结点（internal node)和叶结点（leafnode)。内部结点表示一个特征或属性，叶结点表示一个类。</p>

<p>用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p>

<p>图5.1是一个决策树的示意图。图中圆和方框分别表示内部结点和叶结点。<br/>
<img src="media/14897993109270/14897996064452.jpg" alt=""/></p>

<h2 id="toc_1">决策树与if-then规则</h2>

<p>可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</p>

<h2 id="toc_2">决策树与条件概率分布</h2>

<p>决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition)上。将特征空间划分为互不相交的单元（cell)或区域（region),并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量， Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>

<p>图5.2 (a)示意地表示了特征空间的一个划分。图中的大正方形表示特征空间。这个大正方形被若干个小矩形分割，每个小矩形表示一个单元。特征空间划分上的单元构成了一个集合，X取值为单元的集合。为简单起见，假设只有两类:正类和负类，即Y取值为+1和-1。小矩形中的数字表示单元的类。图5.2(b)示意地表示特征空间划分确定时，特征（单元）给定条件下类的条件概率分布。 图5.2 (b)中条件概率分布对应于图5.2 (a)的划分。当某个单元c的条件概率满足\(P(Y=+1|X= C)&gt;0.5\)时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例。图5.2(c)为对应于图5.2 (b)中条件概率分布的决策树。<br/>
<img src="media/14897993109270/14898004011784.jpg" alt=""/></p>

<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该 不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p>

<p>决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。<br/>
当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal)的.<br/>
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。<mark>开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去：如果还有子集 不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。</mark></p>

<p>以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即<mark>可能发生过拟合现象</mark>。我们需要对己生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。<mark>具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点</mark>。</p>

<p>如果特征数童很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。</p>

<p>可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。</p>

<p>决策树学习常用的算法有ID3、C4.5与CART,下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。</p>

<h2 id="toc_3">特征选择</h2>

<h3 id="toc_4">特征选择问题</h3>

<p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。</p>

<p>首先通过一个例子来说明特征选择问题：</p>

<p><strong>例5.1：表5.1是一个由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性)：第1个特征是年龄，有3个可能值：青年，中年，老年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子，有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一 般。表的最后一列是类别，是否同意贷款，取2个值：是，否.</strong></p>

<p><img src="media/14897993109270/14898020066279.jpg" alt=""/></p>

<p>希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p>

<p>特征选择是决定用哪个特征来划分特征空间。</p>

<p>图5.3表示从表5.1数据学习到的两个可能的决策树，分别由两个不同特征的根结点构成。图5.3(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图5.3(b)所示的根结点的特征是有工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特 征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益 (information gain〉就能够很好地表示这一直观的准则.</p>

<h3 id="toc_5">信息增益</h3>

<p>为了便于说明，先给出熵与条件熵的定义。<br/>
在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为：<br/>
\(P(X=x_i) = P_i  , i=1,2,3...n\)<br/>
则随机变量X的熵定义为：<br/>
\(H(X) = -\sum^{n}_{i=1}{p_ilogp_i}\)<br/>
在上式中，若\(p_i=0\),则定义\(olog0=0\)。通常，上式中的对数以2为底或者以e为底（自然对数），这时熵的单位分别称作比特(bit)或纳特(nat).由定义可 知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作开\(H_{(p)}\),即<br/>
<img src="media/14897993109270/14898028331151.jpg" alt=""/><br/>
熵越大，随机变量的不确定性就越大。从定义可验证：<br/>
<img src="media/14897993109270/14898033210936.jpg" alt=""/></p>

<p>设有随机变量(X,Y),其联合概率为：<br/>
<img src="media/14897993109270/14898040283563.jpg" alt=""/><br/>
条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy) H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望<br/>
<img src="media/14897993109270/14898042638745.jpg" alt=""/><br/>
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵 (empirical entropy)和经验条件熵（empirical conditional entropy )。此时，如果有0概率，令0log0=0。<br/>
信息增益（information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>

<p>信息增益的定义如下所示：<br/>
<img src="media/14897993109270/14898059454405.jpg" alt=""/></p>

<p>决策树学习应用信息增益准则选择特征。<mark>给定训练数据集D和特征A,经验 熵H(D)表示对数据集D进行分类的不确定性。而经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益</mark>，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。<br/>
根据信息增益准则的特征选择方法是：对训练数据集（或子集）D,计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>

<p>下面讲述信息增益的算法：<br/>
<img src="media/14897993109270/14898066826354.jpg" alt=""/><br/>
<img src="media/14897993109270/14898066953482.jpg" alt=""/></p>

<p>下面举例：<br/>
<img src="media/14897993109270/14898226986668.jpg" alt=""/><br/>
<img src="media/14897993109270/14898227425071.jpg" alt=""/></p>

<h3 id="toc_6">信息增益比</h3>

<p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问題困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比（information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。<br/>
定义（信息增益比）：特征A对训练数据集D的信息增益比\(g_R(D,A)\)定义为其信息增益\(g(D,A)\)与训练数据集D的经验熵H(D)之比：<br/>
<img src="media/14897993109270/14898241117010.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14893694433396.html">
                
                  <h1>朴素贝叶斯法</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">朴素贝叶斯的学习与分类</a>
<ul>
<li>
<a href="#toc_1">基本方法</a>
</li>
<li>
<a href="#toc_2">后验概率最大化的含义</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">朴素贝叶斯发的参数估计</a>
<ul>
<li>
<a href="#toc_4">极大似然估计</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">学习与分类算法</a>
</li>
<li>
<a href="#toc_6">贝叶斯估计</a>
</li>
</ul>


<p>朴素贝叶斯（naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布：然后基于此模型，对给定的输入:x，利用贝叶斯定理求出后验概率最大的输出:y。朴素贝叶斯法实现简单，学习与预测的效率都很髙，是一种常用的方法.</p>

<h2 id="toc_0">朴素贝叶斯的学习与分类</h2>

<h3 id="toc_1">基本方法</h3>

<p>设输入空间\(\chi \subseteq R^n\)为n维向量的集合，输出空间为类标记集合\(\mathcal{Y}={c_1,c_2,...c_k}\)。输入为特征向量\(x\in \chi\),输出为类标记\(y\in mathcal{Y}\)。X是定义在输入空间\(\chi\)上的随机向量，Y是定义在输出空间\(\mathcal{Y}\)上的随机变量。P(X,Y)是X和Y的联合概率分布。训练数据集：<br/>
\(T = {(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)<br/>
由P(X,Y)独立同分布产生。</p>

<p>朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y)。具体的，学习以下先验概率分布及条件概率分布。先验概率分布：<br/>
\(P(Y = c_k), k=1,2,3...K\)</p>

<p>条件概率分布：<br/>
\(P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,...K\)<br/>
于是学习到联合概率分布P(X,Y)。</p>

<p>条件概率分布P(X=x|Y=\(c_k\))有指数级数量的参数，其估计实际是不可行的，事实上，假设\(x^{(j)}可取值有S_j个，j=1,2,3...n,Y可取值为K个，那么参数个数为K\prod_{j=1}^n{S_j}\)</p>

<p>朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：<br/>
<img src="media/14893694433396/14897352338918.jpg" alt=""/></p>

<p>朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴 素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。<br/>
朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型计算后验概率分布\(P(Y=C_k|X=x)\),将后验概率最大的类作为x的类输出。后验概率计算根据贝 叶斯定理进行：<br/>
<img src="media/14893694433396/14897352467770.jpg" alt=""/></p>

<h3 id="toc_2">后验概率最大化的含义</h3>

<p>朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。假设选择0-1损失函数：<br/>
<img src="media/14893694433396/14897368963380.jpg" alt=""/><br/>
式中f(X)是分类决策函数。这时，期望风险函数为：<br/>
<img src="media/14893694433396/14897369463646.jpg" alt=""/><br/>
期望是对联合分布P(X,Y)取的，由此取条件期望：<br/>
<img src="media/14893694433396/14897370507779.jpg" alt=""/><br/>
为了是期望风险最小化，只需要对\(X=x\)逐个极小化，由此得到：<br/>
<img src="media/14893694433396/14897371045644.jpg" alt=""/><br/>
这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：<br/>
<img src="media/14893694433396/14897380067414.jpg" alt=""/><br/>
即朴素贝叶斯法所采用的原理。</p>

<h2 id="toc_3">朴素贝叶斯发的参数估计</h2>

<h3 id="toc_4">极大似然估计</h3>

<p>在朴素贝叶斯法中，学习意味着估计\(P(Y=c_k)\)和\(P(X^{(j)}=x^{(j)} |Y=c_k)\)。可以应用极大似然估计法估计相应的概率。先验概率\(P(Y=c_k)\)的极大似然估计是:<br/>
<img src="media/14893694433396/14897383598198.jpg" alt=""/><br/>
设第j个特征\(x^{(j)}\)可能取值的几何为{\(a_{j1},a_{j2}....a_{jS_j}\)},条件概率\(P(X^{j}=a_{jl}|Y=c_k)\)的极大似然估计是：<br/>
<img src="media/14893694433396/14897397226736.jpg" alt=""/></p>

<h2 id="toc_5">学习与分类算法</h2>

<p>下面给出朴素贝叶斯的学习与分类方法。</p>

<p>算法(朴素贝叶斯算法)：<br/>
<img src="media/14893694433396/14897403452985.jpg" alt=""/></p>

<p>下面用一个例题来解释说明上述算法：<br/>
<img src="media/14893694433396/14897405687808.jpg" alt=""/><br/>
<img src="media/14893694433396/14897405876725.jpg" alt=""/></p>

<h2 id="toc_6">贝叶斯估计</h2>

<p>用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差，解决这一问题的方法是采用贝叶斯估计。具 体地，条件概率的贝叶斯估计是：<br/>
<img src="media/14893694433396/14897409238342.jpg" alt=""/></p>

<p>这次再用上述例题来举例，如下所示：<br/>
<img src="media/14893694433396/14897415528528.jpg" alt=""/></p>

<blockquote>
<p><strong>总结：朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。</strong></p>
</blockquote>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html'>统计学习方法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_21.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_23.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="/asset/img/logn.png" /></div>
            
                <h1>LZH007</h1>
                <div class="site-des">LZH的技术杂事小博客~</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/lockxmonk" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lzhabc007@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="MAC%20OS.html"><strong>MAC OS</strong></a>
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>统计学习方法</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Python练习</strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>图像去雾技术</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html"><strong>算法学习</strong></a>
        
            <a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html"><strong>常见面试问题</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15119708919827.html">☆☆Find All Numbers Disappeared in an Array</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15119656914314.html">Max Area of Island</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15119430001845.html">Max Consecutive Ones</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15119407096444.html">Range Sum Query 2D - Immutable</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15119406678576.html">Reshape the Matrix</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
          <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1265629731'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s22.cnzz.com/z_stat.php%3Fid%3D1265629731%26online%3D1' type='text/javascript'%3E%3C/script%3E"));</script>    
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2017
Powered by <a target="_blank" href="https://lockxmonk.github.io/index.html">LZH</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
