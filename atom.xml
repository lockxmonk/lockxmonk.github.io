<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[LZH007]]></title>
  <link href="https://lockxmonk.github.io/atom.xml" rel="self"/>
  <link href="https://lockxmonk.github.io/"/>
  <updated>2017-04-20T11:35:51+08:00</updated>
  <id>https://lockxmonk.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im">MWeb</generator>

  
  <entry>
    <title type="html"><![CDATA[面向对象高级编程]]></title>
    <link href="https://lockxmonk.github.io/14926766188460.html"/>
    <updated>2017-04-20T16:23:38+08:00</updated>
    <id>https://lockxmonk.github.io/14926766188460.html</id>
    <content type="html"><![CDATA[
<p>正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法，这就是动态语言的灵活性。</p>

<p><strong><em>我们可以动态的给某一个实例绑定属性、方法。要绑定该类所有的实例的话，则可以将方法绑定到该类上。</em></strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-


class Student(object):
    &quot;&quot;&quot;docstring for Student&quot;&quot;&quot;
    pass
s = Student()
s.name = &#39;梁中豪&#39;  # 动态给实例绑定一个属性

from types import MethodType


def set_age(self, age):  # 定义一个函数作为实例方法
    self.age = age
s.set_age = MethodType(set_age, s, Student)  # 给实例绑定一个方法
s.set_age(25)

# 为了给所有实例都绑定方法，可以给class绑定方法：


def set_score(self, score):  # 定义一个函数作为实例方法
    self.score = score
Student.set_score = MethodType(set_score, None, Student)
s.set_score(100) #给class绑定方法后，所有实例均可调用
s2 = Student()
s2.set_score(90)
print s.name, s.age, s.score, s2.score
</code></pre>

<p><img src="media/14926766188460/14926773474695.jpg" alt=""/></p>

<h2 id="toc_0">使用__slots__</h2>

<p>如果我们想要限制class的属性怎么办？比如，只允许对Student实例添加<code>name和age</code>属性。那么可以使用一个特殊的<code>__slots__</code>变量。</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-


class Student(object):
    &quot;&quot;&quot;docstring for Student&quot;&quot;&quot;
    __slots__ = (&#39;name&#39;,&#39;age&#39;)
    
s = Student()
s.name = &#39;梁中豪&#39;
s.age = 21
print s.name, s.age
s.score = 90
print s.score

</code></pre>

<p><img src="media/14926766188460/14926779162430.jpg" alt=""/></p>

<p><font color=red>使用<code>__slots__</code>要注意，<code>__slots__</code>定义的属性仅对当前类起作用，对继承的子类是不起作用的.<br/>
</font></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[类和实例]]></title>
    <link href="https://lockxmonk.github.io/14926712777578.html"/>
    <updated>2017-04-20T14:54:37+08:00</updated>
    <id>https://lockxmonk.github.io/14926712777578.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">数据封装</a>
</li>
<li>
<a href="#toc_1">访问限制</a>
</li>
<li>
<a href="#toc_2">继承和多态</a>
</li>
<li>
<a href="#toc_3">获取对象信息</a>
<ul>
<li>
<a href="#toc_4">使用type()</a>
</li>
<li>
<a href="#toc_5">使用isinstance()</a>
</li>
<li>
<a href="#toc_6">使用dir()</a>
</li>
</ul>
</li>
</ul>


<p>在Python中，定义类是通过<code>class</code>关键字：</p>

<pre><code class="language-py">class Student(object):
    pass
</code></pre>

<p><code>class</code>后面紧接着是类名，即<code>Student</code>，类名通常是大写开头的单词，紧接着是<code>(object)</code>，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，<strong>如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类</strong>。</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
class Students(object):
    &quot;&quot;&quot;docstring for Students&quot;&quot;&quot;

    def __init__(self, name, score):
        super(Students, self).__init__()
        self.name = name
        self.score = score
</code></pre>

<p>注意到<code>__init__</code>方法的第一个参数永远是self，表示创建的实例本身，因此，在<code>__init__</code>方法内部，就可以把各种属性绑定到<code>self</code>，因为<code>self</code>就指向创建的实例本身。</p>

<p>和普通的函数相比，在类中定义的函数只有一点不同，<font color=red>就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数和关键字参数。</font></p>

<h2 id="toc_0">数据封装</h2>

<p>面向对象编程的一个重要特点就是数据封装，创建实例需要给出<code>name</code>和<code>score</code>，而如何打印，都是在Student类的内部定义的，这些数据和逻辑被“封装”起来了，调用很容易，但却不用知道内部实现的细节。</p>

<p>例如下面增加一个成绩分类的方法：</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
class Students(object):
    &quot;&quot;&quot;docstring for Students&quot;&quot;&quot;

    def __init__(self, name, score):
        super(Students, self).__init__()
        self.name = name
        self.score = score

    def print_score(self):
        print &#39;%s:%s&#39; % (self.name, self.score)

    def get_grade(self):
        if self.score &gt;= 90:
            print &#39;A&#39;
        elif self.score &gt;= 60:
            print &#39;B&#39;
        else:
            print &#39;C&#39;

bart = Students(&#39;Bart&#39;, 59)
lisa = Students(&#39;梁中豪&#39;, 100)
bart.print_score()
bart.get_grade()
lisa.print_score()
lisa.get_grade()
</code></pre>

<h2 id="toc_1">访问限制</h2>

<p>在Class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。</p>

<p>但是，从前面Student类的定义来看，外部代码还是可以自由地修改一个实例的<code>name</code>、<code>score</code>属性</p>

<p>如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线<code>__</code>，<font color=red>在Python中，实例的变量名如果以<code>__</code>开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，</font>所以，我们把Student类改一改：</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
class Students(object):
    &quot;&quot;&quot;docstring for Students&quot;&quot;&quot;

    def __init__(self, name, score):
        super(Students, self).__init__()
        self.__name = name
        self.__score = score

    def print_score(self):
        print &#39;%s:%s&#39; % (self.__name, self.__score)

   
bart = Students(&#39;Bart&#39;, 59)
lisa = Students(&#39;梁中豪&#39;, 100)
bart.print_score()
lisa.__name
</code></pre>

<p>改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.<code>__name</code>和实例变量<code>.__score</code>了：<br/>
<img src="media/14926712777578/14926727380387.jpg" alt=""/><br/>
如果又要允许外部代码修改score,可以给Student类增加set_score方法.这里跟java很像。不在过多叙述。</p>

<h2 id="toc_2">继承和多态</h2>

<p>在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。</p>

<p>继承最大的好处是子类获得了父类的全部功能。由于<code>Animial</code>实现了<code>run()</code>方法，因此，<code>Dog</code>和<code>Cat</code>作为它的子类，什么事也没干，就自动拥有了<code>run()</code>方法，当然也可以对父类方法进行重写。</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
class Animal(object):
    &quot;&quot;&quot;docstring for Animal&quot;&quot;&quot;

    def run(self):
        print &#39;Animal is running..&#39;


class Dog(Animal):
    pass


class Cat(Animal):
    &quot;&quot;&quot;docstring for Cat&quot;&quot;&quot;

    def run(self):
        print &#39;Cat is running..&#39;

dog = Dog()
dog.run()

cat = Cat()
cat.run()

</code></pre>

<p><img src="media/14926712777578/14926743181176.jpg" alt=""/></p>

<h2 id="toc_3">获取对象信息</h2>

<p>当我们拿到一个对象的引用时，如何知道这个对象是什么类型？</p>

<h3 id="toc_4">使用type()</h3>

<p>基本类型都可以用<code>type()</code>判断：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

print type(123),type(&#39;str&#39;),type(None)
</code></pre>

<p><img src="media/14926712777578/14926750871663.jpg" alt=""/></p>

<p>Python把每种type类型都定义好了常量，放在types模块里，使用之前，需要先导入：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import types
print type(&#39;abc&#39;)==types.StringType
print type(u&#39;abc&#39;)==types.UnicodeType
print type([])==types.ListType
print type(str) == types.TypeType
</code></pre>

<p><img src="media/14926712777578/14926752976543.jpg" alt=""/><br/>
<strong>最后注意到有一种类型就叫TypeType，所有类型本身的类型就是TypeType.比如<code>int，str</code>等</strong></p>

<h3 id="toc_5">使用isinstance()</h3>

<p>我们使用<code>isinstance()</code>来判断class的类型，例如：</p>

<pre><code class="language-py">class Animals(object):
    &quot;&quot;&quot;docstring for Animals&quot;&quot;&quot;
    pass
class Cat(Animals):
    &quot;&quot;&quot;docstring for Cat&quot;&quot;&quot;
    pass
class Dog(Animals):
    &quot;&quot;&quot;docstring for Dog&quot;&quot;&quot;
    pass
a = Animals()
d = Dog()
h = Cat()
print isinstance(h, Cat),isinstance(d, Dog),isinstance(a, Animals)
</code></pre>

<p><img src="media/14926712777578/14926756201012.jpg" alt=""/></p>

<p>当然也可以用<code>isinstance()</code>来判断基本类型</p>

<h3 id="toc_6">使用dir()</h3>

<p>如果要获得一个对象的所有属性和方法，可以使用<code>dir()</code>函数，它返回一个包含字符串的<code>list</code>，比如，获得一个str对象的所有属性和方法：</p>

<pre><code class="language-py">&gt;&gt;&gt; dir(&#39;abc&#39;)
</code></pre>

<p><img src="media/14926712777578/14926758082813.jpg" alt=""/></p>

<p><strong>仅仅把属性和方法列出来是不够的，配合<code>getattr()、setattr()以及hasattr()</code>，我们可以直接操作一个对象的状态：</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

class Animals(object):
    &quot;&quot;&quot;docstring for Animals&quot;&quot;&quot;

    def __init__(self):
        self.x = 9

    def power(self):
        print self.x * self.x
obj = Animals()

print hasattr(obj, &#39;x&#39;)  # 有属性&#39;x&#39;吗？
print hasattr(obj, &#39;y&#39;)  # 有属性&#39;y&#39;吗？
print setattr(obj, &#39;y&#39;, 19)  # 设置一个属性&#39;y&#39;
print getattr(obj, &#39;y&#39;)  # 获取属性&#39;y&#39;
print obj.y
</code></pre>

<p><img src="media/14926712777578/14926764199501.jpg" alt=""/></p>

<p>也可以获得对象的方法：</p>

<pre><code class="language-py">fn = getattr(obj, &#39;power&#39;)
print fn,fn(),
</code></pre>

<p><img src="media/14926712777578/14926765852729.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面向对象编程]]></title>
    <link href="https://lockxmonk.github.io/14926690841704.html"/>
    <updated>2017-04-20T14:18:04+08:00</updated>
    <id>https://lockxmonk.github.io/14926690841704.html</id>
    <content type="html"><![CDATA[
<p>采用面向对象的程序设计思想来设计一个简单的学生成绩打印，我们首选思考的不是程序的执行流程，而是<code>Student</code>这种数据类型应该被视为一个对象，这个对象拥有<code>name</code>和<code>score</code>这两个属性（Property）。如果要打印一个学生的成绩，首先必须创建出这个学生对应的对象，然后，给对象发一个<code>print_score</code>消息，让对象自己把自己的数据打印出来</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
class Students(object):
    &quot;&quot;&quot;docstring for Students&quot;&quot;&quot;

    def __init__(self, name, score):
        super(Students, self).__init__()
        self.name = name
        self.score = score

    def print_score(self):
        print &#39;%s:%s&#39; % (self.name, self.score)


bart = Students(&#39;Bart&#39;, 59)
lisa = Students(&#39;梁中豪&#39;, 100)
bart.print_score()
lisa.print_score()
</code></pre>

<p><img src="media/14926690841704/14926708025078.jpg" alt=""/></p>

<p><strong>面向对象三大特点：封装、继承、多态。</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用_future_]]></title>
    <link href="https://lockxmonk.github.io/14926579397626.html"/>
    <updated>2017-04-20T11:12:19+08:00</updated>
    <id>https://lockxmonk.github.io/14926579397626.html</id>
    <content type="html"><![CDATA[
<p>Python的每个新版本都会增加一些新的功能，或者对原来的功能作一些改动。有些改动是不兼容旧版本的，也就是在当前版本运行正常的代码，到下一个版本运行就可能不正常了。</p>

<p>从Python 2.7到Python 3.x就有不兼容的一些改动，比如2.x里的字符串用<code>&#39;xxx&#39;</code>表示str，Unicode字符串用<code>u&#39;xxx&#39;</code>表示unicode，而在3.x中，所有字符串都被视为unicode，因此，写<code>u&#39;xxx&#39;和&#39;xxx&#39;</code>是完全一致的.还有在2.x中以<code>&#39;xxx&#39;</code>表示的str就必须写成<code>b&#39;xxx&#39;</code>，以此表示“二进制字符串”。</p>

<p><font color=red>Python提供了<code>__future__模块</code>，把下一个新版本的特性导入到当前版本，于是我们就可以在当前版本中测试一些新版本的特性。</font>举例说明如下：</p>

<p>为了适应Python 3.x的新的字符串的表示方法，在2.7版本的代码中，可以通过unicode_literals来使用Python 3.x的新的语法：</p>

<pre><code class="language-py"># still running on Python 2.7
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

print &#39;\&#39;xxx\&#39; is unicode?&#39;, isinstance(&#39;xxx&#39;, unicode)
print &#39;u\&#39;xxx\&#39; is unicode?&#39;, isinstance(u&#39;xxx&#39;, unicode)
print &#39;\&#39;xxx\&#39; is str?&#39;, isinstance(&#39;xxx&#39;, str)
print &#39;b\&#39;xxx\&#39; is str?&#39;, isinstance(b&#39;xxx&#39;, str)
</code></pre>

<p><img src="media/14926579397626/14926584481460.jpg" alt=""/><br/>
注意到上面的代码仍然在Python 2.7下运行，但结果显示去掉前缀u的<code>&#39;a string&#39;</code>仍是一个unicode，而加上前缀b的<code>b&#39;a string&#39;</code>才变成了str</p>

<p>由于Python是由社区推动的开源并且免费的开发语言，不受商业公司控制，因此，<strong>Python的改进往往比较激进，不兼容的情况时有发生。Python为了确保你能顺利过渡到新版本，特别提供了__future__模块，让你在旧的版本中试验新版本的一些特性</strong>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用第三方模块]]></title>
    <link href="https://lockxmonk.github.io/14925924499918.html"/>
    <updated>2017-04-19T17:00:49+08:00</updated>
    <id>https://lockxmonk.github.io/14925924499918.html</id>
    <content type="html"><![CDATA[
<p>在Python中，安装第三方模块，是通过setuptools这个工具完成的。Python有两个封装了setuptools的包管理工具：<code>easy_install和pip</code>。<font color=red>目前官方推荐使用<code>pip</code>。</font></p>

<p>Mac和linux本身自带pip，不用安装，在win下需要安装。这里不再过多叙述。</p>

<p>这里试着安装一个第三方库：<strong>Python Imaging Library</strong>，这是Python下非常强大的处理图像的工具库。一般来说，第三方库都会在Python官方的<a href="http://pypi.python.org">http://pypi.python.org</a>网站注册，要安装一个第三方库，必须先知道该库的名称，可以在官网或者pypi上搜索，比如Python Imaging Library的名称叫PIL，因此，安装Python Imaging Library的命令就是：</p>

<p><del>pip install PIL</del>（现在用pillow来代替PIL）</p>

<pre><code>sudo pip install Pillow
</code></pre>

<p>处理图片的方法为：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from PIL import Image

im = Image.open(&#39;icon.png&#39;)
print im.format, im.size, im.mode

im.thumbnail((150,100))
im.save(&#39;thumb.jpg&#39;,&#39;JPEG&#39;)
</code></pre>

<p><img src="media/14925924499918/14926575371706.jpg" alt=""/><br/>
将原图<code>icon.png</code>:<br/>
<img src="media/14925924499918/icon.png" alt="icon"/></p>

<p>生成一个名为<code>thumb.jpg</code>的图片：<br/>
<img src="media/14925924499918/thumb.jpg" alt="thumb"/></p>

<blockquote>
<p>这两张图暂时没有看出明显的区别，需要深入研究。</p>
</blockquote>

<p>默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在<code>sys</code>模块的<code>path</code>变量中：</p>

<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.path
[&#39;&#39;, &#39;/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg&#39;, 
&#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&#39;,... ]
</code></pre>

<p>想要添加自己的搜索目录可以：</p>

<pre><code class="language-py">&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.path.append(&#39;/Users/michael/my_py_scripts&#39;)
</code></pre>

<p><font color=red>这种方法是在运行时修改，运行结束后失效。</font></p>

<p>第二种方法是设置环境变量<code>PYTHONPATH</code>这里不做详述。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用模块]]></title>
    <link href="https://lockxmonk.github.io/14922429288489.html"/>
    <updated>2017-04-15T15:55:28+08:00</updated>
    <id>https://lockxmonk.github.io/14922429288489.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">模块介绍</a>
</li>
<li>
<a href="#toc_1">模块的使用</a>
</li>
<li>
<a href="#toc_2">别名</a>
</li>
<li>
<a href="#toc_3">作用域</a>
</li>
</ul>


<h2 id="toc_0">模块介绍</h2>

<p>python中的文件关联模块和java语言类似，一个<code>abc.py</code>的文件就是一个名字叫abc的模块，一个<code>xyz.py</code>的文件就是一个名字叫xyz的模块。</p>

<p>现在，假设我们的<code>abc和xyz</code>这两个模块名字与其他模块冲突了，于是我们可以通过包来组织模块，避免冲突。方法是选择一个顶层包名，比如<code>mycompany</code>，按照如下目录存放：<br/>
<img src="media/14922429288489/14922430767659.jpg" alt=""/></p>

<p>引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py模块的名字就变成了mycompany.abc，类似的，xyz.py的模块名变成了mycompany.xyz。</p>

<p>请注意，每一个包目录下面都会有一个<code>__init__.py</code>的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。<code>__init__.py</code>可以是空文件，也可以有Python代码，因为<code>__init__.py</code>本身就是一个模块，而它的模块名就是<code>mycompany</code>。</p>

<p>类似的，可以有多级目录，组成多级层次的包结构，比如如下的目录结构：<br/>
<img src="media/14922429288489/14922431763848.jpg" alt=""/></p>

<h2 id="toc_1">模块的使用</h2>

<p>Python本身就内置了很多非常有用的模块，只要安装完毕，这些模块就可以立刻使用。</p>

<p>我们以内建的<code>sys</code>模块为例，编写一个<code>hello</code>的模块：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-


&#39;a test module&#39;

_author_ = &#39;梁中豪&#39;

import sys

def test():
    args = sys.argv
    if len(args)==1:
        print &#39;Hello ,world!&#39;
    elif len(args)==2:
        print &#39;Hello ,%s!&#39; % args[1]
    else:
        print &#39;Too many arguments!&#39;
if __name__==&#39;_main_&#39;:
    test()
</code></pre>

<p>第1行注释表示.py文件本身使用标准UTF-8编码；</p>

<p>第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释；</p>

<p>第6行使用__author__变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名；</p>

<p>以上就是Python模块的标准文件模板，当然也可以全部删掉不写，但是，按标准办事肯定没错。</p>

<p>后面开始就是真正的代码部分。</p>

<p>使用sys模块的第一步，就是导入该模块：<code>import sys</code><br/>
导入<code>sys</code>模块后，我们就有了变量<code>sys</code>指向该模块，利用<code>sys</code>这个变量，就可以访问<code>sys</code>模块的所有功能。</p>

<p><code>sys</code>模块有一个<code>argv</code>变量，用<code>list</code>存储了命令行的所有参数。<code>argv</code>至少有一个元素，因为第一个参数永远是该.py文件的名称，例如：</p>

<p>运行<code>python hello.py</code>获得的<code>sys.argv</code>就是<code>[&#39;hello.py&#39;]</code>；</p>

<p>运行<code>python hello.py Michael</code>获得的<code>sys.argv</code>就是<code>[&#39;hello.py&#39;, &#39;Michael]</code>。</p>

<p>最后，注意到这两行代码：</p>

<pre><code class="language-py">if __name__==&#39;__main__&#39;:
    test()
</code></pre>

<p>当我们在命令行运行<code>hello</code>模块文件时，Python解释器把一个特殊变量<code>__name__置为__main__</code>，而如果在其他地方导入该<code>hello</code>模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。</p>

<p>可以启动python交互环境，导入hello模块来测试下：</p>

<pre><code>$ python
Python 2.7.5 (default, Aug 25 2013, 00:04:04) 
[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import hello
&gt;&gt;&gt; hello.test()
Hello, world!
</code></pre>

<h2 id="toc_2">别名</h2>

<p>导入模块时，可以使用别名，这样，可以在运行时根据当前环境选择最合适的模块。比如Python标准库一般会提供<code>StringIO</code>和<code>cStringIO</code>两个库，这两个库的接口和功能是一样的，但是<code>cStringIO</code>是C写的，速度更快，所以，你会经常看到这样的写法：</p>

<pre><code class="language-py">try:
    import cStringIO as StringIO
except ImportError: # 导入失败会捕获到ImportError
    import StringIO
</code></pre>

<p>这样就可以优先导入<code>cStringIO</code>。如果有些平台不提供<code>cStringIO</code>，还可以降级使用<code>StringIO</code>。导入<code>cStringIO</code>时，用<code>import ... as ...</code>指定了别名<code>StringIO</code>，因此，后续代码引用<code>StringIO</code>即可正常工作。</p>

<h2 id="toc_3">作用域</h2>

<p>在一个模块中，我们可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在Python中，是通过<code>_</code>前缀来实现的。</p>

<p>正常的函数和变量名是公开的（public），可以被直接引用，比如：<code>abc，x123，PI</code>等；</p>

<p>类似<code>__xxx__</code>这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的<code>__author__</code>，<code>__name__</code>就是特殊变量，<code>hello</code>模块定义的文档注释也可以用特殊变量<code>__doc__</code>访问，<strong><em>我们自己的变量一般不要用这种变量名</em></strong>；</p>

<p><font color=red>类似<code>_xxx</code>和<code>__xxx</code>这样的函数或变量就是非公开的（private），不应该被直接引用，比如<code>_abc，__abc</code>等；</font></p>

<p>之所以我们说，<code>private</code>函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问<code>private</code>函数或变量，但是，从编程习惯上不应该引用<code>private</code>函数或变量。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[偏函数]]></title>
    <link href="https://lockxmonk.github.io/14922391300067.html"/>
    <updated>2017-04-15T14:52:10+08:00</updated>
    <id>https://lockxmonk.github.io/14922391300067.html</id>
    <content type="html"><![CDATA[
<p>Python的<code>functools</code>模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。要注意，这里的偏函数和数学意义上的偏函数不一样。</p>

<p><code>int()</code>函数可以把字符串转换为整数<br/>
例如：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
num = int(&#39;12345&#39;)
num1 = int(&#39;12345&#39;, base=8)
num2 = int(&#39;12345&#39;, 16)
print num, num1, num2
</code></pre>

<p><img src="media/14922391300067/14922396420577.jpg" alt=""/></p>

<p>其中<code>base</code>参数可以指定转换的进制</p>

<p>假设要转换大量的二进制字符串，每次都传入<code>int(x, base=2)</code>非常麻烦，于是，我们想到，可以定义一个<code>int2()</code>的函数，默认把<code>base=2</code>传进去：</p>

<pre><code class="language-py">def int2(x,base=2):
    return int(x,base)

</code></pre>

<p>这样我们转换二进制就很方便，<code>functools.partial</code>就是帮助我们创建一个偏函数的，不需要我们自己定义<code>int2()</code>，可以直接使用下面的代码创建一个新的函数<code>int2</code>：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import functools
int2 = functools.partial(int, base=8)
num = int2(&#39;10000&#39;)
num1 = int2(&#39;10000&#39;,base=2)
print num,num1  
</code></pre>

<p><img src="media/14922391300067/14922410514665.jpg" alt=""/></p>

<p>创建偏函数时，实际上可以接收函数对象、<code>*args和**kw</code>这3个参数:</p>

<pre><code class="language-py">int2 = functools.partial(int, base=2)
#相当于下面
kw = { base: 2 }
int(&#39;10010&#39;, **kw)

max2 = functools.partial(max, 10)
#相当于下面，实际上会把10作为*args的一部分自动加到左边
args = (10, 5, 6, 7)
max(*args)
</code></pre>

<blockquote>
<p>总结：当函数的参数个数太多，需要简化时，使用<code>functools.partial</code>可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[支持向量机]]></title>
    <link href="https://lockxmonk.github.io/14921577512970.html"/>
    <updated>2017-04-14T16:15:51+08:00</updated>
    <id>https://lockxmonk.github.io/14921577512970.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">线性可支持性良机与硬间隔最大化</a>
<ul>
<li>
<a href="#toc_1">线性可支持向量机</a>
</li>
<li>
<a href="#toc_2">函数间隔和几何间隔</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">间隔最大化</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_4">1. 最大间隔分离超平面</a>
</li>
<li>
<a href="#toc_5">2.最大间隔分离超平面的存在唯一性</a>
</li>
<li>
<a href="#toc_6">3.支持向量和间隔边界</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>支持向量机（support vector machines，SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器.<font color=red>支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming)的问题，也等价于正则化的合页损失函数的最小化问题.</font>支持向量机的<strong><em>学习算法是求解凸二次规划的最优化算法</em></strong>.</p>

<p>支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机（linear support vector machine in linearly separable ease)、线性支持向量机（linear support vector machine)及非线性支持向量机（ncm-linear support vector machine).简单模型是复杂模型的基础，也是复杂模型的特殊情况.当训练数据线性可分时，通过硬间隔最大化（hard margin maximization)，学习一个线性的分类器，<strong><em>即线性可分支持向量机，又称为硬间隔支持向量机</em></strong>；当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization)，也学习一个线性的分类器，<strong><em>即线性支持向量机，又称为软间隔支持向量机</em></strong>；<strong><em>当训练数据线性不可分时，通过使用核技巧 (kernel trick)及软间隔最大化，学习非线性支持向量机</em></strong>.</p>

<p>当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，<strong><em>核函数(kernel fimcticm)表示将输入从输入空间映射到特征空间得到的特征向量之间的内积.</em></strong>通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机.这样的方法称为核技巧.<strong>核方法（kernel method)是比支持向量机更为一般的机器学习方法</strong>.</p>

<p>Cortes与Vapnik提出线性支持向量机，Boser、Guyon与Vapnik又引入核技巧，提出非线性支持向量机.</p>

<p>本次一次学习3类支持向量机、核函数以及一种跨苏学习方法——序列最小最优化算法(SMO)</p>

<h2 id="toc_0">线性可支持性良机与硬间隔最大化</h2>

<h3 id="toc_1">线性可支持向量机</h3>

<p>考虑一个二类分类问题，假设输入空间与特征空间为两个不同的空间.输入空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间.线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量.非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量.所以，输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的.</p>

<p>假设给定一个特征空间上的训练数据集：<br/>
\(T={\{x_1,y_1},(x_2,y_2),...(x_N,y_N)\}\)<br/>
,其中，\(x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}={\{+1,-1}\},i=1,2,...N,x_i\)为第i个特征向量，也成为实例，\(y_i为x_i的类标记，当y_i=+1时，称x_i为正例；当y_i=-1时，称x_i为负例,(x_i,y_i)成为样本点，再假设训练数据集是线性可分的\)</p>

<p>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类，分离超平面对应于方程\(w*x+b=0\),它由法向量w和截距b决定，可用(w,b)来表示.分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类.法向量指向的一侧为正类，另一侧为负类.</p>

<p>一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开.感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个,<font color=red>线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的.</font></p>

<p><strong><em>下面给出线性可支持向量机的定义</em></strong>：<br/>
<img src="media/14921577512970/14922239390041.jpg" alt=""/></p>

<p>考虑如图7.1所示的二维特征空间中的分类问题.图中“。”表示正例，“x” 表示负例。训练数据集线性可分，这时有许多直线能将两类数据正确划分.线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线，如图7.1所示.<br/>
<img src="media/14921577512970/14922248209178.jpg" alt=""/></p>

<p>间隔最大及相应的约束最优化问题将在下面叙述.这里先介绍<font color=red>函数间隔和几何间隔的概念</font>.</p>

<h3 id="toc_2">函数间隔和几何间隔</h3>

<p>在图7.1中，有A,B,C三个点，表示3个实例，均在分离超平面的正类一侧，预测它们的类.点A距分离超平面较远，若预测该点为正类，就比较确信预测是正确的；点C距分离超平面较近，若预测该点为正类就不那么确信；点B介于点A与C之间，预测其为正类的确信度也在A与C之间.</p>

<p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度.在超平面w*x+ b = 0确定的情况下，|w*x+b丨能够相对地表示点x距离超平面的远近.而w*+b的符号与类标记y的符号是否一致能够表示分类是否正确.所以可用量y(w*x+b)来表示分类的正确性及确信度，这就是函数间隔（fimctional margin)的概念.</p>

<p><strong>下面给出函数间隔的定义：</strong></p>

<p><img src="media/14921577512970/14925656291102.jpg" alt=""/></p>

<p>函数间隔可以表示分类预测的正确性及确信度.<strong><em>但是选择分离超平面时，只有函数间隔还不够.因为只要成比例地改变w和例如将它们改为2w和2b,超平面并没有改变，但函数间隔却成为原来的2倍</em></strong>.这一事实启示我们，可以对分离超平面的法向量w加某些约束，如规范化，||w||=1，使得间隔是确定的.这时函数间隔成为几何间隔（geometric margin).</p>

<p>下图给出了超平面（w，b）及其法向量w，点A表示某一实例\(x_i\)，其类标记为\(y_i =+1\)，点A与超平面（w，b）的距离由线段AB给出，记作\(\gamma_i\)。<br/>
<img src="media/14921577512970/14925666238039.jpg" alt=""/><br/>
一般的，当样本点\((x_i,y_i)\)被超平面\((w,b)\)正确分类是，点\(x_i\)与超平面\((w,b)\)的距离是：<br/>
<img src="media/14921577512970/14925667203290.jpg" alt=""/><br/>
所以：由这一事实导出几何间隔的概念。<br/>
<img src="media/14921577512970/14925667539444.jpg" alt=""/></p>

<p>下面具体给出几何间隔的定义：<br/>
<img src="media/14921577512970/14925669395725.jpg" alt=""/></p>

<p>从函数间隔和几何间隔的定义（式(7.3)〜式(7.6))可知，函数间隔和几何间隔有下面的关系：<br/>
<img src="media/14921577512970/14925676726509.jpg" alt=""/></p>

<p><strong><em>如果||w||=1,那么函数间隔和几何间隔相等.如果超平面参数w和b成比例地改变（超平面没有改变)，函数间隔也按此比例改变，而几何间隔不变.</em></strong></p>

<h2 id="toc_3">间隔最大化</h2>

<p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且<strong><em>几何间隔</em></strong>最大的分离超平面.对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机)，<strong><em>但是几何间隔最大的分离超平面是唯一的</em></strong>.这里的间隔最大化又称为硬间隔最大化（与将要讨论的训练数据集近似线性可分时的软间隔最大化相对应).</p>

<p><font color=red>间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类.也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开.这样的超平面应该对未知的新实例有很好的分类预测能力.</font></p>

<h4 id="toc_4">1. 最大间隔分离超平面</h4>

<p>下面考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面.具体地，这个问题可以表示为下面的约束最优化问题：<br/>
<img src="media/14921577512970/14925687970547.jpg" alt=""/><br/>
即我们希望最大化超平面(w,b)关于训练数据集的几何间隔\(\gamma\),约束条件表示的是超平面(w, b)关于每个训练样本点的几何间隔至少是\(\gamma\).</p>

<p>考虑几何间隔和函数间隔的关系式(7.8),可将这个问题改写为:<br/>
<img src="media/14921577512970/14925690130129.jpg" alt=""/><br/>
这是一个凸二次规划（convex quadratic programming)问题，凸优化问题是指约束最优化问题。</p>

<p>凸优化问题是指约束最优化问题：<br/>
    <img src="media/14921577512970/14925706995554.jpg" alt=""/></p>

<p>当目标函数f(w)是二次函数且约束函数\(g_i(w)\)是仿射函数时，上述凸最优化问题成为凸二次规划问题。</p>

<blockquote>
<p>仿射函数即由1阶多项式构成的函数，一般形式为 f(x)=Ax+b，这里A是一个 m×k 矩阵，x是一个k向量,b是一个m向量，实际上反映了一种从 k维到m维的空间映射关系。</p>
</blockquote>

<p>如果求出了约束最优化问题(7.13)〜(7.14)的解\(w^*,b^*\)，那么就可以得到最大间隔分离超平面\(w^*·x + b = 0\)及分类决策函数\(f(x)=sign(w^*+b^*)\),即线性可分支持向量机模型.</p>

<p>根据上面所得出的结论，就有下面的支持向量机的学习算法————<strong>最大间隔法(maximum margin method )</strong></p>

<p><img src="media/14921577512970/14925717155289.jpg" alt=""/></p>

<h4 id="toc_5">2.最大间隔分离超平面的存在唯一性</h4>

<p>线性可分训练数据集的最大间隔分离超平面是存在且唯一的。</p>

<p><strong>定理7.1 (最大间隔分离趄平面的存在唯一性）</strong>若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一.</p>

<blockquote>
<p>这里关于唯一性的证明不在总结。相关资料可以阅读《统计学习方法》p101.</p>
</blockquote>

<h4 id="toc_6">3.支持向量和间隔边界</h4>

<p><font color=red>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector).</font>支持向量是使约束条件式(7.14)等号成立的点，即：<br/>
<img src="media/14921577512970/14925722828338.jpg" alt=""/><br/>
对\(y_i=+1\)的正例点，支持向量在超平面：<br/>
<img src="media/14921577512970/14925723574226.jpg" alt=""/></p>

<p>对\(y_i=-1\)的负例点，支持向量在超平面：<br/>
<img src="media/14921577512970/14925723990202.jpg" alt=""/></p>

<p>如下图所示，在\(H_1和H_2上的点就是支持向量\)<br/>
<img src="media/14921577512970/14925724626444.jpg" alt=""/></p>

<p>注意到\(H_1\)和\(H_2\)平行，并且没有实例点落在它们中间.在\(H_1\)与\(H_2\)之间形成一条长带，分离超平面与它们平行且位于它们中央.长带的宽度，即\(H_1\)与\(H_2\)之间的距离称为间隔（margin).间隔依赖于分离超平面的法向量w，等于\(\frac{2}{||w||}\).<br/>
\(H_1\)和\(H_2\)称为间隔边界.</p>

<p><font color=red>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用</font>。如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，<strong>甚至去掉这些点，则解是不会改变的</strong>.<font color=red>由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机</font>.支持向量的个数一般很少，所以支 持向量机由很少的“重要的”训练样本确定.</p>

<p><strong>下面举例说明：</strong></p>

<p><img src="media/14921577512970/14925909777337.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[拟牛顿法（最大熵模型的学习）]]></title>
    <link href="https://lockxmonk.github.io/14921570225186.html"/>
    <updated>2017-04-14T16:03:42+08:00</updated>
    <id>https://lockxmonk.github.io/14921570225186.html</id>
    <content type="html"><![CDATA[


<p>最大熵模型学习还可以应用牛顿法或者拟牛顿法。<br/>
对于最大熵模型而言，<br/>
<img src="media/14921570225186/14921574471740.jpg" alt=""/><br/>
<img src="media/14921570225186/14921574611293.jpg" alt=""/><br/>
相应的拟牛顿法BFGS算法如下。<br/>
<img src="media/14921570225186/14921576945600.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[装饰器]]></title>
    <link href="https://lockxmonk.github.io/14921324340299.html"/>
    <updated>2017-04-14T09:13:54+08:00</updated>
    <id>https://lockxmonk.github.io/14921324340299.html</id>
    <content type="html"><![CDATA[
<p>函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。</p>

<p>函数对象有一个<code>__name__</code>属性，可以拿到函数的名字：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def now():
    print &#39;1992-12-14&#39;
f = now
f()
print now.__name__,f.__name__

</code></pre>

<p><img src="media/14921324340299/14921328219891.jpg" alt=""/></p>

<p><strong>现在，假设我们要增强<code>now()</code>函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改<code>now()</code>函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”<code>（Decorator）</code>。</strong></p>

<p>本质上，<code>decorator</code>就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的<code>decorator</code>，可以定义如下：</p>

<pre><code class="language-py">def log(func):
    def wrapper(*args, **kw):
        print &#39;call %s():&#39; % func.__name__
        return func(*args, **kw)
    return wrapper
</code></pre>

<p>观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处：</p>

<pre><code>@log
def now():
    print &#39;2013-12-25&#39;
</code></pre>

<p><img src="media/14921324340299/14921338711853.jpg" alt=""/></p>

<p><strong>练习题：请编写一个decorator，能在函数调用的前后打印出&#39;begin call&#39;和&#39;end call&#39;的日志。</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def log(func):
    def wrapper(*args, **kw):
        print  &#39;begin call&#39;
        res =  func(*args, **kw)
        print &#39;end call2&#39;
        return res
    return wrapper
@log
def now():
    print &#39;test&#39;
now()
</code></pre>

<p><img src="media/14921324340299/14921348025676.jpg" alt=""/></p>

<p><strong>练习题：再思考一下能否写出一个@log的decorator，使它既支持：</strong></p>

<pre><code class="language-py">@log
def f():
    pass
</code></pre>

<p><strong>又支持</strong></p>

<pre><code class="language-py">@log(&#39;execute&#39;)
def f():
    pass
</code></pre>

<p>解答：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import functools

def log(text):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args ,**kw):
            if len(text)&gt;0:
                print &#39;%s %s()&#39; % (text, func.__name__)
            else:
                print &#39;%s()&#39; % func.__name__
            return func(*args,**kw)
        return wrapper
    return decorator

@log(&#39;sdfsf&#39;)
def now():
    print &#39;test&#39;
now()
</code></pre>

<p><img src="media/14921324340299/14921357641252.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[匿名函数]]></title>
    <link href="https://lockxmonk.github.io/14921312042142.html"/>
    <updated>2017-04-14T08:53:24+08:00</updated>
    <id>https://lockxmonk.github.io/14921312042142.html</id>
    <content type="html"><![CDATA[
<p>当我们在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。</p>

<p>关键字<code>lambda</code>表示匿名函数</p>

<p><strong>匿名函数有个限制，就是只能有一个表达式，不用写<code>return</code>，返回值就是该表达式的结果。</strong></p>

<p>例如：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
print map(lambda x:x*x, [1,2,3,4,5,6])
</code></pre>

<p><img src="media/14921312042142/14921315287449.jpg" alt=""/></p>

<p><font color=red>其中冒号前面的x表示函数参数。</font></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[返回函数]]></title>
    <link href="https://lockxmonk.github.io/14920643245223.html"/>
    <updated>2017-04-13T14:18:44+08:00</updated>
    <id>https://lockxmonk.github.io/14920643245223.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">函数作为返回值</h2>

<p>在求和函数中，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数！</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def lazy_sum(*args):
    def sum():
        ax = 0
        for n in args:
            ax = ax + n
        return ax
    return sum
f = lazy_sum(1,3,4,5,5)
#这里是f()不是f
print f()
</code></pre>

<h2 id="toc_1">闭包</h2>

<p>注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。</p>

<p><strong>返回的函数并没有立刻执行，而是直到调用了<code>f()</code>才执行</strong><br/>
例如：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def count():
    fs = []
    for i in range(1, 4):
        def f():
             return i*i
        fs.append(f)
    return fs

f1, f2, f3 = count()
print f1(),f2(),f3()
</code></pre>

<p>可能认为结果为<code>1，4，9</code>但是结果为：<br/>
<img src="media/14920643245223/14920656661011.jpg" alt=""/><br/>
<font color=red>原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。</font></p>

<blockquote>
<p>所以：返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。</p>
</blockquote>

<p>要引用的话，方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def count():
    fs=[]
    for i in range(1,4):
        def f(j):
            def g():
                return j*j
            return g
        fs.append(f(i))
    return fs
f1, f2, f3 = count()
print f1(),f2(),f3()
</code></pre>

<p><img src="media/14920643245223/14920669289447.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sorted（排序）]]></title>
    <link href="https://lockxmonk.github.io/14920539372230.html"/>
    <updated>2017-04-13T11:25:37+08:00</updated>
    <id>https://lockxmonk.github.io/14920539372230.html</id>
    <content type="html"><![CDATA[
<p><strong>1.用sorted进行排序</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
num =[2,5,6,4,3,29,12]
print sorted(num)

</code></pre>

<p><img src="media/14920539372230/14920542668861.jpg" alt=""/></p>

<p><strong>2.默认情况下，对字符串排序，是按照ASCII的大小比较的，由于&#39;Z&#39; &lt; &#39;a&#39;，结果，大写字母Z会排在小写字母a的前面。<br/>
现在，我们提出排序应该忽略大小写，按照字母序排序。</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def cmp_ignore_case(s1, s2):
    u1 = s1.upper()
    u2 = s2.upper()
    if u1 &lt; u2:
        return -1
    if u1 &gt; u2:
        return 1
    return 0
print sorted([&#39;bob&#39;, &#39;about&#39;, &#39;Zoo&#39;, &#39;Credit&#39;], cmp_ignore_case)
</code></pre>

<p><img src="media/14920539372230/14920543761031.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Filter]]></title>
    <link href="https://lockxmonk.github.io/14920527647833.html"/>
    <updated>2017-04-13T11:06:04+08:00</updated>
    <id>https://lockxmonk.github.io/14920527647833.html</id>
    <content type="html"><![CDATA[
<p><strong>1.请尝试用filter()删除1~100的素数。</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
num = range(1, 101)

def prime_num(s):
    for x in range(2, s):
        if (s % x) == 0:
            return True
        else:
            return False
print filter(prime_num, num)
</code></pre>

<p>结果：<br/>
<img src="media/14920527647833/14920538585519.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Map/Reduce]]></title>
    <link href="https://lockxmonk.github.io/14920523812017.html"/>
    <updated>2017-04-13T10:59:41+08:00</updated>
    <id>https://lockxmonk.github.io/14920523812017.html</id>
    <content type="html"><![CDATA[
<p>1.利用map()函数，把用户输入的不规范的英文名字，变为首字母大写，其他小写的规范名字。输入：[&#39;adam&#39;, &#39;LISA&#39;, &#39;barT&#39;]，输出：[&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;]。</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def upper(s):
    return s.capitalize()
print(map(upper, [&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;]) )
</code></pre>

<p><img src="media/14920523812017/14920538865556.jpg" alt=""/></p>

<p>2.Python提供的sum()函数可以接受一个list并求和，请编写一个prod()函数，可以接受一个list并利用reduce()求积。</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
def prod(x,y):
    return x*y
m = [1,2,3,4]
print (reduce(prod,m))
</code></pre>

<p><img src="media/14920523812017/14920539075590.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[模型学习的最优化算法（接上一文）]]></title>
    <link href="https://lockxmonk.github.io/14920445399813.html"/>
    <updated>2017-04-13T08:48:59+08:00</updated>
    <id>https://lockxmonk.github.io/14920445399813.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">改进的迭代尺度法</a>
</li>
</ul>


<p>逻辑斯谛回归模型、最大嫡模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解.从最优化的观点看，这时的目标函数具有很好的性质.<font color=red>它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解.</font>常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法.牛顿法或拟牛顿法一般收敛速度更快.</p>

<p>这次主要学习基于改进的迭代尺度法与拟牛顿法的最大熵模型学习算法。（还有梯度下降法，这次不过多学习）</p>

<h2 id="toc_0">改进的迭代尺度法</h2>

<p>改进的迭代尺度法（improved iterative scaling，IIS)是一种最大熵模型学习的最优化算法.</p>

<p>已知最大熵模型为：<br/>
<img src="media/14920445399813/14920447866369.jpg" alt=""/><br/>
对数似然函数为：<br/>
<img src="media/14920445399813/14920449425684.jpg" alt=""/><br/>
目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值\(\hat{w}\)</p>

<p>改进的迭代尺度算法（iis）的想法是：假设最大熵模型当前的参数向量是\(w=(w_1,w_2,...,w_n)^T\),我们 希望找到一个新的参数向量\(w+\delta = (w_1+\delta_1,w_2+\delta_2,....,w_n+\delta_n)^T\)，使得模型的对数似然函数值增大.如果能有这样一种参数向量更新的方法\(\tau:w \rightarrow w+\delta\),那么就 可以重复使用这一方法，直至找到对数似然函数的最大值.</p>

<p>对于给定的经验分布\(\tilde P(x,y)\),模型参数从\(w到w+\delta\)，对数似然函数的改变量是：<br/>
<img src="media/14920445399813/14920457816508.jpg" alt=""/></p>

<p>如果能找到适当的\(\delta使下届A(\delta | w)\)提高，那么对数似然函数也会提高。然而，函数\(A(\delta | w)\)中的\(\delta\)是一个向量，含有多个变量，不容易同时优化，IIS试图以此只优化其中一个变量\(\delta_i\)，而固定其他变量\(\delta_j,i  \neq j\).</p>

<p>为达到这一目的，IIS进一步降低下界\(A(\delta | w)\),具体的，IIS引进一个量\(f^{\#}(x,y)\):<br/>
<img src="media/14920445399813/14920481227290.jpg" alt=""/><br/>
因为\(f_i\)是一个二值函数，故\(f^{\#}(x,y)\)表示所有特征在（x，y）出现的次数，这样，\(A(\delta | w)\)可以改写为：</p>

<p><img src="media/14920445399813/14920483320007.jpg" alt=""/></p>

<p><strong>下面给出IIS算法：</strong><br/>
<img src="media/14920445399813/14920487060621.jpg" alt=""/><br/>
<img src="media/14920445399813/14920491294532.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[logistic regression(逻辑斯蒂回归)]]></title>
    <link href="https://lockxmonk.github.io/14917858710123.html"/>
    <updated>2017-04-10T08:57:51+08:00</updated>
    <id>https://lockxmonk.github.io/14917858710123.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">逻辑斯蒂回归模型</a>
<ul>
<li>
<a href="#toc_1">逻辑斯蒂分布</a>
</li>
<li>
<a href="#toc_2">二项逻辑斯蒂回归模型</a>
</li>
<li>
<a href="#toc_3">模型参数估计</a>
</li>
<li>
<a href="#toc_4">多项了逻辑斯蒂回归</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_5">最大熵模型</a>
<ul>
<li>
<a href="#toc_6">最大熵原理</a>
<ul>
<li>
<a href="#toc_7">最大熵模型的定义</a>
</li>
<li>
<a href="#toc_8">最大熵模型的学习</a>
</li>
<li>
<a href="#toc_9">极大似然估计</a>
</li>
</ul>
</li>
</ul>


<p>简介：逻辑斯谛回归（logistic regression)是统计学习中的经典分类方法.最大熵是 概率模型学习的一个准则，将其推广到分类问题得到最大熵模型(maximum entropy model).逻辑斯谛回归模型与最大熵模型都属于对数线性模型.</p>

<h2 id="toc_0">逻辑斯蒂回归模型</h2>

<h3 id="toc_1">逻辑斯蒂分布</h3>

<p><img src="media/14917858710123/14918919297549.jpg" alt=""/></p>

<h3 id="toc_2">二项逻辑斯蒂回归模型</h3>

<p>二项逻辑斯诗回归模型（binomial logistic regression model)是一种分类模型，由条件概率分布\(P(Y|X)\)表示，形式为参数化的逻辑斯谛分布.这里，随机变量X取值为实数，随机变量Y取值为1或0.我们通过监督学习的方法来估计模型参数.</p>

<p><img src="media/14917858710123/14918930389977.jpg" alt=""/><br/>
现在考査逻辑斯谛回归模型的特点.一个事件的几率（odds)是指该事件发生的概率与该事件不发生的概率的比值.如果事件发生的概率是p,那么该亊件的几率是\(\frac{1}{1-p}\) ,该事件的对数几率(log odds)或logit函数是<br/>
<img src="media/14917858710123/14918936472837.jpg" alt=""/><br/>
<font color = red>这就是说，在逻辑斯谛回归模型中，输出y = l的对数几率是输入x的线性函数.或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型.</font></p>

<p>换一个角度看，考虑对输入x进行分类的线性函数\(w*x\),其值域为实数域.注意，这里\(x\in R^{n+1},w \in R^{n+1}\).通过逻辑斯谛回归模型定义式(6.5)可以将线性函数\(w*x\)转换为概率：<br/>
<img src="media/14917858710123/14918951209571.jpg" alt=""/></p>

<p><strong>这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0(如图6.1所示).这样的模型就是逻辑斯谛回归模型.</strong></p>

<h3 id="toc_3">模型参数估计</h3>

<p>逻辑斯谛回归模型学习时，对于给定的训练数据集\(T=\{{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}}\),其中\(x_i\in R^n,y_i \in \{{0,1\}}\)，<strong>可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型.</strong><br/>
<img src="media/14917858710123/14918969386189.jpg" alt=""/></p>

<h3 id="toc_4">多项了逻辑斯蒂回归</h3>

<p>上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类.可以将其推广为多项逻辑斯缔回归模型(multi-nominal logistic regression model)，用于多类分类.假设离散型随机变量Y的取值集合是{1,2...K}，那么多项逻辑斯谛回归模型是:<br/>
<img src="media/14917858710123/14918973306162.jpg" alt=""/><br/>
二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯讳回归.</p>

<h1 id="toc_5">最大熵模型</h1>

<p>最大熵模型（maximum entropy model)由最大熵原理推导实现.这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。</p>

<h2 id="toc_6">最大熵原理</h2>

<p>最大熵原理是概率模型学习的一个准则.<font color=red>最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型.通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型.</font></p>

<p>假设离散随机变量X的概率分布是p(X),则其熵是：<br/>
<img src="media/14917858710123/14919006601164.jpg" alt=""/><br/>
直观地，<font color=red>最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件.在没有更多信息的情况下，那些不确定的部分都是“等可能的”.最大熵原理通过熵的最大化来表示等可能性.</font>“等可能”不容易操作，而熵则是一个可优化的数值指标.</p>

<p>首先，可以通过一个简单的例子来了解一下最大熵原理.<br/>
<img src="media/14917858710123/14919019478267.jpg" alt=""/></p>

<p>图6.2提供了用最大熵原理进行概率模型选择的几何解释.概率模型集合P可由欧氏空间中的单纯形(simplex)表示，如左图的三角形(2-单纯形).一个点代表一个模型，整个单纯形代表模型集合.右图上的一条直线对应于一个约束 条件，直线的交集对应于满足所有约束条件的模型集合.一般地，这样的模型仍有无穷多个.学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则给出最优模型选择的一个准则.<br/>
<img src="media/14917858710123/14919593499530.jpg" alt=""/></p>

<h3 id="toc_7">最大熵模型的定义</h3>

<p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型. </p>

<p>假设分类模型是一个条件概率分布\(P(Y|X),X \in \mathcal{X} \subseteq R^n\)表示输入，\(Y \in \mathcal{Y}\)表示输出，\(\mathcal{X}\)和\(\mathcal{Y}\)分别是输入和输出的集合.这个模型表示的是对于给定的输入\(\mathcal{X}\)，以条作概率\(P(Y|X)\)输出Y.<br/>
给定一个训练数据集<br/>
\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)<br/>
学习的目标是用最大熵原理选择最好的分类模型。</p>

<p>首先考虑模型应该满足的条件，给定训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布，分别以\(\tilde{P}(X,Y)和\tilde{P}(X)\)表示。这里：<br/>
<img src="media/14917858710123/14919609290680.jpg" alt=""/><br/>
 用特征函数\(f(x,y)描述输入x和输出y之间的某一个事实\)。其定义是：<br/>
 <img src="media/14917858710123/14919611561526.jpg" alt=""/><br/>
特征函数f(x,y)关于经验分布\(\tilde{P}(X,Y)\)的期望值，用\(E_\tilde{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919613926643.jpg" alt=""/><br/>
特征函数f(x,y)关于模型P(Y|X)与经验分布\(\tilde{P}(X)\)的期望值，用\(E_{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919617357953.jpg" alt=""/></p>

<p>最大熵模型的定义为：<br/>
<img src="media/14917858710123/14919625972087.jpg" alt=""/></p>

<h3 id="toc_8">最大熵模型的学习</h3>

<p>最大熵模型的学习过程就是求解最大熵模型的过程.最大熵模型的学习可以形式化为约束最优化问题.<br/>
对于给定的训练数据集\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)以及特征函数\(f_i(x,y),i=1,2,...n\),最大熵模型的学习等价于约束最优化问题：<br/>
<img src="media/14917858710123/14919630413332.jpg" alt=""/><br/>
求解约束最优化问题(6.14)〜(6.16)，所得出的解，就是最大熵模型学习的解.下面给出具体推导.<br/>
这里，将约束最优化的原始问题转换为无约束最优化的对偶问题.通过求解对偶问题求解原始问题.<br/>
<img src="media/14917858710123/14919633775267.jpg" alt=""/><br/>
<img src="media/14917858710123/14919633957280.jpg" alt=""/></p>

<p>下面举例来说明最大熵模型的计算：</p>

<p><strong>例：假设随机变量X有5个取值{A,B,C,D,E}，要估计各个值的概率P(A),P(B),P(C),P(D),P(E).</strong></p>

<p><img src="media/14917858710123/14919642672585.jpg" alt=""/><br/>
<img src="media/14917858710123/14919642856866.jpg" alt=""/><br/>
<img src="media/14917858710123/14919643007948.jpg" alt=""/></p>

<h3 id="toc_9">极大似然估计</h3>

<p>从以上最大熵模型学习中可以看出，最大熵模型是由式(6.22)、式(6.23)表示的 条件概率分布.下面证明对偶函数的极大化等价于最大嫡模型的极大似然估计.</p>

<p>己知训练数据的经验概率分布\(\tilde{P}(X,Y)\),条件概率分布\(P(Y|X)\)的对数似然 函数表示为:<br/>
<img src="media/14917858710123/14919659054476.jpg" alt=""/><br/>
当提哦啊煎概率分布P(y|x)是最大熵模型(6.22)和(6.23)时,对数似然函数\(L_ \tilde{p}(P_w)\)为：<br/>
<img src="media/14917858710123/14919660761710.jpg" alt=""/><br/>
比较算式(6.26)和式(6.27),可得：<br/>
<img src="media/14917858710123/14920442862315.jpg" alt=""/></p>

<p>既然对偶函数\(\psi(w)\)等价于对数似然函数\(L_{\tilde p}(P_w)\) ,于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实.<br/>
这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题.</p>

<p>可以将最大熵模型写成更一般的形式.<br/>
<img src="media/14917858710123/14920444597468.jpg" alt=""/><br/>
最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型(log linear model).模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[决策树生成--CART算法]]></title>
    <link href="https://lockxmonk.github.io/14908575557810.html"/>
    <updated>2017-03-30T15:05:55+08:00</updated>
    <id>https://lockxmonk.github.io/14908575557810.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">CART算法</a>
<ul>
<li>
<a href="#toc_1">CART生成</a>
<ul>
<li>
<a href="#toc_2">1.回归树的生成</a>
</li>
<li>
<a href="#toc_3">2.分类树的生成</a>
</li>
<li>
<a href="#toc_4">CART剪枝</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">CART算法</h2>

<p>分类与回归树(classification and regression tree, CART)模型由 Breiman等人在1984年提出，是应用广泛的决策树学习方法.CART同样由特征选择、树 的生成及剪枝组成，<mark>既可以用于分类也可以用于回归.</mark>以下将用于分类与回归的树统称为决策树.</p>

<p>CART是在给定输入随机变量X的条件下输出随机变量Y的条件概率分布的学习方法.CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支.这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布.</p>

<p><strong>CART算法由以下两步组成：</strong></p>

<p>1.决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大：<br/>
2.决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准.</p>

<h3 id="toc_1">CART生成</h3>

<p>决策树的生成就是递归地构建二叉决策树的过程.对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树.</p>

<h4 id="toc_2">1.回归树的生成</h4>

<p>假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集:<br/>
    \(D={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\)<br/>
考虑如何生成回归树。</p>

<p>一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上 的输出值.假设己将输入空间划分为M个单元\(R_1,R_2,R_3...R_M\),并且在每个单元\(R_M\)上 有一个固定的输出值\(c_m\)，于是回归树模型可表示为:<br/>
<img src="media/14908575557810/14909500378632.jpg" alt=""/></p>

<p>当输入空间的划分确定时，可以用平方误差\(\sum_{x_i\in R}(y_i-f(x_i))^2\)来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值.易知，单元\(R_m\)上的\(c_m\)的最优值\(\hat c_m\)是\(R_m\)上的所有输入实例\(x_i\)对应的输出\(y_i\)的均值，即:<br/>
<img src="media/14908575557810/14909504677321.jpg" alt=""/></p>

<p>问题是怎样对输入空间进行划分,这里采用启发式的方法，选择第j个变量\(x^{(j)}\)和它取的值s，作为切分变量（splitting variable)和切分点（splitting point),并定义两个区域：<br/>
<img src="media/14908575557810/14909505605933.jpg" alt=""/><br/>
然后寻找最优切分变量j和最优切分点s，最优地，求解：<br/>
<img src="media/14908575557810/14909506372664.jpg" alt=""/><br/>
对固定输入变量j可以找到最优切分点s:<br/>
<img src="media/14908575557810/14909506657025.jpg" alt=""/><br/>
便利所有输入变量，找到最优的切分变量j，构成一个对\((j,s)\).依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止.这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(least squares regression tree)，现将算法叙述如下：</p>

<p>最小二乘回归树生成算法：<br/>
<img src="media/14908575557810/14909510077124.jpg" alt=""/></p>

<h4 id="toc_3">2.分类树的生成</h4>

<p>分类树用基尼指数选择最有特征，同时决定该特征的最优二值切分点。</p>

<p><strong>基尼指数：</strong><br/>
<img src="media/14908575557810/14913540871077.jpg" alt=""/><br/>
如果样本集合D根据特征A是否取某一可能值a被分割成\(D_1和D_2\)两部分，即：<br/>
<img src="media/14908575557810/14913550101283.jpg" alt=""/></p>

<p>基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p>

<p>图5.7显示二类分类问题中基尼指数Gini(p)、熵（单位比特）之半和分类误差率的关系.横坐标表示概率P，纵坐标表示损失.可以看出基尼指数和 熵之半的曲线很接近，都可以近似地代表分类误差率.<br/>
<img src="media/14908575557810/14913559817897.jpg" alt=""/></p>

<p><strong>CART生成算法：</strong><br/>
<img src="media/14908575557810/14913560936133.jpg" alt=""/><br/>
<img src="media/14908575557810/14913561059742.jpg" alt=""/></p>

<p><strong>下面继续用例题来说明该算法：</strong><br/>
<img src="media/14898241272608/14908402191601.jpg" alt=""/><br/>
根据上表所给的训练数据集，应用CART算法生成决策树。</p>

<p><img src="media/14908575557810/14913582940784.jpg" alt=""/></p>

<h4 id="toc_4">CART剪枝</h4>

<p>CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小(模型变简单)，从而能够对未知数据有更准确的预测.CART剪枝算法由两步组成：首先从生成算法产生的决策树\(T_0\)底端开始不断剪枝，直到\(T_0\)的根结点，形成一个子树序列\( {T_0,T_1,...,T_n}\);然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树.</p>

<p>1.<strong>剪枝</strong>，形成一个子树序列<br/>
在剪枝过程中，计算子树的损失函数：<br/>
<img src="media/14908575557810/14913604606599.jpg" alt=""/><br/>
其中，T为任意子树，C(T)为对训练数据的预测误差(如基尼指数)，丨T丨为子树的叶结点个数，\(α \geq0\)为参数，\(C_{\alpha}(T)\)为参数是\(\alpha\)时的子树T的整体损失.参数\(\alpha\)权衡训练数据的拟合程度与模型的复杂度.</p>

<p>对固定的\(\alpha\), —定存在使损失函数\(C_{\alpha}(T)\)最小的子树，将其表示为\(T_{\alpha}\)。\(T_{\alpha}\)在损失函数\(C_{\alpha}(T)\)最小的意义下是最优的.容易验证这样的最优子树是唯一的.当\(\alpha\)大的时候，最优子树\(T_{\alpha}\)偏小；当\(\alpha\)小的时候，最优子树\(T_{\alpha}\)偏大.极端情况，当\(\alpha\) = 0时，整体树是最优的.当\(\alpha \rightarrow \infty \)时，根结点组成的单结点树是最优的.</p>

<p>Breiman等人证明：可以用递归的方法对树进行剪枝.将\(\alpha\)从小增大，\(0=\alpha_0&lt;\alpha_1&lt;...&lt;\alpha_n&lt;\infty,\)，产生一系列的区间\([\alpha_i,\alpha_{i+1}),i=0,1,...n;\)，剪枝得到的子树 序列对应着区间\(\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,...n;\)的最优子树序列\( {T_0,T_1,...,T_n}\),序列中的子树是嵌套的.</p>

<p><img src="media/14908575557810/14913621104324.jpg" alt=""/><br/>
<img src="media/14908575557810/14913625337737.jpg" alt=""/></p>

<p><strong>CART剪枝算法：</strong><br/>
<img src="media/14908575557810/14913629643758.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[决策树的生成]]></title>
    <link href="https://lockxmonk.github.io/14898241272608.html"/>
    <updated>2017-03-18T16:02:07+08:00</updated>
    <id>https://lockxmonk.github.io/14898241272608.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">ID3算法</a>
</li>
<li>
<a href="#toc_1">C4.5d的生成算法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_2">决策树的剪枝</a>


<p>这次将学习决策树的生成算法，首先了解ID3的生成算法，然后再学习C4.5的生成算法，这些都是决策树学习的经典算法。</p>

<h2 id="toc_0">ID3算法</h2>

<p>ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树.具体方法是：从根结点（root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点：再对子结点递归地调用以上方法，构建决策树；直到所有特征的 信息增益均很小或没有特征可以选择为止.最后得到一个决策树.ID3相当于用极大似然法进行概率模型的选择.</p>

<p><strong>该算法的具体步骤为：</strong><br/>
<img src="media/14898241272608/14908400085381.jpg" alt=""/><br/>
<img src="media/14898241272608/14908400481942.jpg" alt=""/></p>

<p><strong>这里我们继续使用之前用过的表来进行计算：</strong><br/>
<img src="media/14898241272608/14908402191601.jpg" alt=""/><br/>
利用ID3算法建立决策树：<br/>
<img src="media/14898241272608/14908417653584.jpg" alt=""/><br/>
<mark>但是ID3只有树的生成，容易发生过拟合现象。</mark></p>

<h2 id="toc_1">C4.5d的生成算法</h2>

<p>C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进.C4.5在生成的过程中，用<mark>信息增益比</mark>来选择特征(ID3为信息增益).<br/>
<img src="media/14898241272608/14908421490687.jpg" alt=""/></p>

<h1 id="toc_2">决策树的剪枝</h1>

<p>决策树生成算法递归地产生决策树，直到不能继续下去为止.这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象.<mark>过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树</mark>.解决这个问题的办法是考虑决策树的复 杂度，对已生成的决策树进行简化.</p>

<p>在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning).具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型.</p>

<p>这里学习一种简单的决策树学习的剪枝算法。</p>

<p>决策树的剪枝往往通过极小化决策树整体的损失函数(loss fimction)或代价函数(costfimction)来实现.设树T的叶结点个数为\(|T|\)，t是树T的叶结点，该叶结点有\(N_{t}\)个样本点，其中k类的样本点有\(N_{tk}个，k=1,2,3...,K,H_i(T)\)为叶结点t上的经验熵，\(a\geq0\)为参数，则决策树学习的损失函数可以定义为:<br/>
<img src="media/14898241272608/14908428498530.jpg" alt=""/><br/>
式(5.14)中，c(t)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数\(a\geq0\)控制两者之间的影响.较大的\(\alpha\)促使选择较简单的模型(树)，较小的\(\alpha\)促使选择较复杂的模型(树).\(\alpha = 0\)意味着只考虑 模型与训练数据的拟合程度，不考虑模型的复杂度.</p>

<p>剪枝，就是当\(\alpha\)确定时，选择损失函数最小的模型，即损失函数最小的子树.当\(\alpha\)值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好.损失函数正好表示了对两者的平衡.</p>

<p>可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合.而决策树剪枝通过优化损失函数还考虑了减小模型复杂度，决策树生成学习局部的模型，而决策树剪枝学习整体的模型.</p>

<p>式(5.11)或式(5.14)定义的损失函数的极小化等价于正则化的极大似然估计.所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择.</p>

<p>下图是决策树的剪枝过程示意图。<br/>
<img src="media/14898241272608/14908438468056.jpg" alt=""/></p>

<p>剪枝算法如下：<br/>
<img src="media/14898241272608/14908438815566.jpg" alt=""/><br/>
<img src="media/14898241272608/14908438965218.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[决策树]]></title>
    <link href="https://lockxmonk.github.io/14897993109270.html"/>
    <updated>2017-03-18T09:08:30+08:00</updated>
    <id>https://lockxmonk.github.io/14897993109270.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">决策树模型与学习</a>
</li>
<li>
<a href="#toc_1">决策树与if-then规则</a>
</li>
<li>
<a href="#toc_2">决策树与条件概率分布</a>
</li>
<li>
<a href="#toc_3">特征选择</a>
<ul>
<li>
<a href="#toc_4">特征选择问题</a>
</li>
<li>
<a href="#toc_5">信息增益</a>
</li>
<li>
<a href="#toc_6">信息增益比</a>
</li>
</ul>
</li>
</ul>


<blockquote>
<p>决策树（decision tree)是一种基本的分类与回归方法。本章主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。这些决策树学习的思想主要来源于由Quinlan在1986年提出的ID3算法和1993年提出的C4。5算法,以及由Breiman等人在1984年提出的CART算法。</p>
</blockquote>

<h2 id="toc_0">决策树模型与学习</h2>

<p>定义（决策树）：分类决策树模型是一种描述对实例进行分类的树形结 构。决策树由结点（node)和有向边（directed edge)组成。结点有两种类型：内部结点（internal node)和叶结点（leafnode)。内部结点表示一个特征或属性，叶结点表示一个类。</p>

<p>用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p>

<p>图5.1是一个决策树的示意图。图中圆和方框分别表示内部结点和叶结点。<br/>
<img src="media/14897993109270/14897996064452.jpg" alt=""/></p>

<h2 id="toc_1">决策树与if-then规则</h2>

<p>可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</p>

<h2 id="toc_2">决策树与条件概率分布</h2>

<p>决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition)上。将特征空间划分为互不相交的单元（cell)或区域（region),并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量， Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>

<p>图5.2 (a)示意地表示了特征空间的一个划分。图中的大正方形表示特征空间。这个大正方形被若干个小矩形分割，每个小矩形表示一个单元。特征空间划分上的单元构成了一个集合，X取值为单元的集合。为简单起见，假设只有两类:正类和负类，即Y取值为+1和-1。小矩形中的数字表示单元的类。图5.2(b)示意地表示特征空间划分确定时，特征（单元）给定条件下类的条件概率分布。 图5.2 (b)中条件概率分布对应于图5.2 (a)的划分。当某个单元c的条件概率满足\(P(Y=+1|X= C)&gt;0.5\)时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例。图5.2(c)为对应于图5.2 (b)中条件概率分布的决策树。<br/>
<img src="media/14897993109270/14898004011784.jpg" alt=""/></p>

<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该 不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p>

<p>决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。<br/>
当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal)的.<br/>
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。<mark>开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去：如果还有子集 不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。</mark></p>

<p>以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即<mark>可能发生过拟合现象</mark>。我们需要对己生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。<mark>具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点</mark>。</p>

<p>如果特征数童很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。</p>

<p>可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。</p>

<p>决策树学习常用的算法有ID3、C4.5与CART,下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。</p>

<h2 id="toc_3">特征选择</h2>

<h3 id="toc_4">特征选择问题</h3>

<p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。</p>

<p>首先通过一个例子来说明特征选择问题：</p>

<p><strong>例5.1：表5.1是一个由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性)：第1个特征是年龄，有3个可能值：青年，中年，老年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子，有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一 般。表的最后一列是类别，是否同意贷款，取2个值：是，否.</strong></p>

<p><img src="media/14897993109270/14898020066279.jpg" alt=""/></p>

<p>希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p>

<p>特征选择是决定用哪个特征来划分特征空间。</p>

<p>图5.3表示从表5.1数据学习到的两个可能的决策树，分别由两个不同特征的根结点构成。图5.3(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图5.3(b)所示的根结点的特征是有工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特 征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益 (information gain〉就能够很好地表示这一直观的准则.</p>

<h3 id="toc_5">信息增益</h3>

<p>为了便于说明，先给出熵与条件熵的定义。<br/>
在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为：<br/>
\(P(X=x_i) = P_i  , i=1,2,3...n\)<br/>
则随机变量X的熵定义为：<br/>
\(H(X) = -\sum^{n}_{i=1}{p_ilogp_i}\)<br/>
在上式中，若\(p_i=0\),则定义\(olog0=0\)。通常，上式中的对数以2为底或者以e为底（自然对数），这时熵的单位分别称作比特(bit)或纳特(nat).由定义可 知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作开\(H_{(p)}\),即<br/>
<img src="media/14897993109270/14898028331151.jpg" alt=""/><br/>
熵越大，随机变量的不确定性就越大。从定义可验证：<br/>
<img src="media/14897993109270/14898033210936.jpg" alt=""/></p>

<p>设有随机变量(X,Y),其联合概率为：<br/>
<img src="media/14897993109270/14898040283563.jpg" alt=""/><br/>
条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy) H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望<br/>
<img src="media/14897993109270/14898042638745.jpg" alt=""/><br/>
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵 (empirical entropy)和经验条件熵（empirical conditional entropy )。此时，如果有0概率，令0log0=0。<br/>
信息增益（information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>

<p>信息增益的定义如下所示：<br/>
<img src="media/14897993109270/14898059454405.jpg" alt=""/></p>

<p>决策树学习应用信息增益准则选择特征。<mark>给定训练数据集D和特征A,经验 熵H(D)表示对数据集D进行分类的不确定性。而经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益</mark>，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。<br/>
根据信息增益准则的特征选择方法是：对训练数据集（或子集）D,计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>

<p>下面讲述信息增益的算法：<br/>
<img src="media/14897993109270/14898066826354.jpg" alt=""/><br/>
<img src="media/14897993109270/14898066953482.jpg" alt=""/></p>

<p>下面举例：<br/>
<img src="media/14897993109270/14898226986668.jpg" alt=""/><br/>
<img src="media/14897993109270/14898227425071.jpg" alt=""/></p>

<h3 id="toc_6">信息增益比</h3>

<p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问題困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比（information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。<br/>
定义（信息增益比）：特征A对训练数据集D的信息增益比\(g_R(D,A)\)定义为其信息增益\(g(D,A)\)与训练数据集D的经验熵H(D)之比：<br/>
<img src="media/14897993109270/14898241117010.jpg" alt=""/></p>

]]></content>
  </entry>
  
</feed>
