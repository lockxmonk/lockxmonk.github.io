<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[LZH007]]></title>
  <link href="https://lockxmonk.github.io/atom.xml" rel="self"/>
  <link href="https://lockxmonk.github.io/"/>
  <updated>2017-06-15T11:34:21+08:00</updated>
  <id>https://lockxmonk.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Caffeä¸­Layerçš„å­¦ä¹ ]]></title>
    <link href="https://lockxmonk.github.io/14974936829967.html"/>
    <updated>2017-06-15T10:28:02+08:00</updated>
    <id>https://lockxmonk.github.io/14974936829967.html</id>
    <content type="html"><![CDATA[
<p>Layeræ˜¯Caffeçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œè‡³å°‘æœ‰ä¸€ä¸ªè¾“å…¥Blob (Bottom Blob)å’Œä¸€ä¸ªè¾“å‡ºBlob (Top Blob),éƒ¨åˆ†Layerå¸¦æœ‰æƒå€¼(Weight)å’Œåç½®é¡¹ï¼ˆBias),<strong>æœ‰ä¸¤ä¸ªè¿ç®—æ–¹å‘</strong>ï¼šå‰å‘ä¼ æ’­ï¼ˆForward)å’Œåå‘ä¼ æ’­ï¼ˆBackward)ï¼Œå…¶ä¸­å‰å‘ä¼ æ’­è®¡ç®—ä¼šå¯¹è¾“å…¥Blobè¿›è¡ŒæŸç§å¤„ç†ï¼ˆå­˜æƒå€¼å’Œåç½®é¡¹çš„Layerä¼šåˆ©ç”¨è¿™äº›å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼‰,å¾—åˆ°è¾“å‡ºBlob;è€Œåå‘ä¼ æ’­è®¡ç®—åˆ™å¯¹è¾“å‡ºBlobçš„diffè¿›è¡ŒæŸç§å¤„ç†ï¼Œå¾—åˆ°è¾“å…¥Blobçš„diff(æœ‰æƒå€¼å’Œåç½®é¡¹çš„Layerå¯èƒ½ä¹Ÿä¼šè®¡ç®—æƒå€¼Blobã€åç½®é¡¹Blobçš„diff)ã€‚</p>

<h2 id="toc_0">layerä¸­çš„æ•°æ®ç»“æ„æè¿°</h2>

<p>æˆ‘ä»¬å¯ä»¥æœç´¢caffeä¸­å…³äº<code>message LayerParameter</code>çš„ç±»,æ¥äº†è§£.<br/>
å¦‚æœä½ ä¸€å¼€å§‹æ‰¾ä¸åˆ°è¿™ä¸ªç±»åœ¨é‚£ä¸ªæ–‡ä»¶æè¿°,å¯ä»¥ç”¨ä¸‹é¢è¿™ä¸ªå‘½ä»¤å»æœç´¢:</p>

<pre><code>âœ  caffe git:(master) âœ— grep -n -H -R &quot;message LayerParameter&quot; *
</code></pre>

<p><img src="media/14974936829967/14974948024735.jpg" alt=""/><br/>
å¾—åˆ°å®ƒçš„è·¯å¾„.</p>

<p>æˆ‘ä»¬å‘ç°æ˜¯åœ¨<code>src/caffe/proto/caffe.proto</code>è¿™ä¸ªè·¯å¾„ä¸­.å› ä¸ºcaffeä½¿ç”¨<code>google_protobuf</code>æ•°æ®ç±»å‹æ¥å£°æ˜layer.å…³äº<code>google_protobuf</code>çš„ç›¸å…³å†…å®¹,ä¹‹åå¯ä»¥ç ”ç©¶ä¸€ä¸‹.</p>

<p>è¿™é‡Œæˆ‘ä»¬çœ‹ä¸€ä¸‹æºç :</p>

<pre><code class="language-protobuf">//æ³¨æ„ï¼šå¦‚æœä½ å¢åŠ äº†1ä¸ªæ–°çš„LayerParameteråŸŸï¼Œä¸€å®šè®°å¾—æ›´æ–°ä¸€ä¸ªå¯ç”¨ID
// LayerParameter ä¸‹ä¸€ä¸ªlayer-specific ID: 147 (last added: recurrent_param)
message LayerParameter {
  optional string name = 1; // the layer name
  optional string type = 2; // the layer type
  repeated string bottom = 3; // è¾“å…¥Blob(bottom Blob)çš„åç§°
  repeated string top = 4; // è¾“å‡ºBlob(Top Blob)çš„åç§°

  // å½“å‰è®¡ç®—é˜¶æ®µï¼ˆTRAIN æˆ– TEST)
    optional Phase phase = 10;

  // ä¸ºæ¯ä¸ªTop Blobåˆ†é…å¯¹æŸå¤±å‡½æ•°çš„æƒé‡ï¼Œæ¯ä¸ªLayeréƒ½æœ‰é»˜è®¤å€¼ï¼Œè¦ä¹ˆä¸º0,è¡¨ç¤ºä¸å‚ä¸ç›®æ ‡å‡½æ•°è®¡ç®—ï¼šè¦ä¹ˆä¸º1ï¼Œè¡¨ç¤ºå‚ä¸æŸå¤±å‡½æ•°è®¡ç®—
  repeated float loss_weight = 5;

  // æŒ‡å®šè®­ç»ƒå‚æ•°ï¼ˆä¾‹å¦‚ç›¸å¯¹å…¨å±€å­¦ä¹ å¸¸æ•°çš„ç¼©æ”¾å› å­ï¼Œä»¥åŠç”¨äºæƒå€¼å…±äº« çš„åç§°æˆ–å…¶ä»–è®¾ç½®)
  repeated ParamSpec param = 6;

  // æ‰¿è½½äº†è¯¥å±‚æ•°å€¼å‚æ•°çš„Blob
  repeated BlobProto blobs = 7;
  //æ˜¯å¦å¯¹Bottom Blobè¿›è¡Œåå‘ä¼ æ’­è¿‡ç¨‹ã€‚è¯¥å­—æ®µçš„ç»´åº¦åº”ä¸ Bottom Blobä¸ªæ•°ä¸€è‡´
  // Specifies whether to backpropagate to each bottom. If unspecified,
  // Caffe will automatically infer whether each input needs backpropagation
  // to compute parameter gradients. If set to true for some inputs,
  // backpropagation to those inputs is forced; if set false for some inputs,
  // backpropagation to those inputs is skipped.
  //
  // The size must be either 0 or equal to the number of bottoms.
  repeated bool propagate_down = 11;

 //æ§åˆ¶æŸä¸ªå±‚åœ¨æŸä¸ªæ—¶åˆ»æ˜¯å¦åŒ…å«åœ¨ç½‘ç»œä¸­ï¼ŒåŸºäºå½“å‰NetStateã€‚ä½ å¯ä»¥ä¸ºincludeæˆ–exclude(ä¸è¦åŒæ—¶ï¼‰æŒ‡å®šéé›¶å€¼ã€‚å¦‚æœæ²¡æœ‰ä»»ä½•è§„åˆ™ï¼Œé‚£ä¹ˆè¯¥å±‚ä¸€ç›´åŒ…å«åœ¨ç½‘ç»œä¸­ï¼šå¦‚æœå½“å‰NetStateæ»¡è¶³äº†ä»»ä½•1ä¸ªæŒ‡å®šè§„åˆ™ï¼Œè€¶ä¹ˆè¯¥å±‚ä¼šè¢«åŒ…å«æˆ–æ’æ–¥
  // Rules controlling whether and when a layer is included in the network,
  // based on the current NetState.  You may specify a non-zero number of rules
  // to include OR exclude, but not both.  If no include or exclude rules are
  // specified, the layer is always included.  If the current NetState meets
  // ANY (i.e., one or more) of the specified rules, the layer is
  // included/excluded.
  repeated NetStateRule include = 8;
  repeated NetStateRule exclude = 9;

  // Parameters for data pre-processing.æ•°æ®é¢„å¤„ç†å‚æ•°
  optional TransformationParameter transform_param = 100;

  // Parameters shared by loss layers.æ‰€æœ‰æŸå¤±å±‚å…±äº«çš„å‚æ•°
  optional LossParameter loss_param = 101;
  
  
  //ç‰¹å®šç±»å‹å±‚çš„å‚æ•°ã€‚æ³¨æ„ä¸€äº›å±‚å®ç°æ—¶å¯èƒ½æœ‰å¤šäºä¸€ç§çš„è®¡ç®—å¼•æ“ï¼Œè¿™äº›å±‚åŒ…æ‹¬ä¸€ä¸ªå¼•æ“ç±»å‹å’Œå¼•æ“å‚æ•°æ¥é€‰æ‹©å®ç°.é»˜è®¤å¼•æ“æ˜¯åœ¨ç¼–è¯‘é˜¶æ®µç”±å¼•æ“å¼€å…³è®¾ç½®çš„
  // Layer type-specific parameters.
  //
  // Note: certain layers may have more than one computational engine
  // for their implementation. These layers include an Engine type and
  // engine parameter for selecting the implementation.
  // The default for the engine is set by the ENGINE switch at compile-time.
  optional AccuracyParameter accuracy_param = 102;
  optional ArgMaxParameter argmax_param = 103;
  optional BatchNormParameter batch_norm_param = 139;
  optional BiasParameter bias_param = 141;
  optional ConcatParameter concat_param = 104;
  optional ContrastiveLossParameter contrastive_loss_param = 105;
  optional ConvolutionParameter convolution_param = 106;
  optional CropParameter crop_param = 144;
  optional DataParameter data_param = 107;
  optional DropoutParameter dropout_param = 108;
  optional DummyDataParameter dummy_data_param = 109;
  optional EltwiseParameter eltwise_param = 110;
  optional ELUParameter elu_param = 140;
  optional EmbedParameter embed_param = 137;
  optional ExpParameter exp_param = 111;
  optional FlattenParameter flatten_param = 135;
  optional HDF5DataParameter hdf5_data_param = 112;
  optional HDF5OutputParameter hdf5_output_param = 113;
  optional HingeLossParameter hinge_loss_param = 114;
  optional ImageDataParameter image_data_param = 115;
  optional InfogainLossParameter infogain_loss_param = 116;
  optional InnerProductParameter inner_product_param = 117;
  optional InputParameter input_param = 143;
  optional LogParameter log_param = 134;
  optional LRNParameter lrn_param = 118;
  optional MemoryDataParameter memory_data_param = 119;
  optional MVNParameter mvn_param = 120;
  optional ParameterParameter parameter_param = 145;
  optional PoolingParameter pooling_param = 121;
  optional PowerParameter power_param = 122;
  optional PReLUParameter prelu_param = 131;
  optional PythonParameter python_param = 130;
  optional RecurrentParameter recurrent_param = 146;
  optional ReductionParameter reduction_param = 136;
  optional ReLUParameter relu_param = 123;
  optional ReshapeParameter reshape_param = 133;
  optional ScaleParameter scale_param = 142;
  optional SigmoidParameter sigmoid_param = 124;
  optional SoftmaxParameter softmax_param = 125;
  optional SPPParameter spp_param = 132;
  optional SliceParameter slice_param = 126;
  optional TanHParameter tanh_param = 127;
  optional ThresholdParameter threshold_param = 128;
  optional TileParameter tile_param = 138;
  optional WindowDataParameter window_data_param = 129;
}

</code></pre>

<h2 id="toc_1">Layeræ˜¯æ€ä¹ˆç‚¼æˆçš„</h2>

<p>Layerå¤´æ–‡ä»¶ä½äº<code>include/caffe/layer.hpp</code>ä¸­ï¼Œæˆ‘ä»¬æ¥è§£æä¸€ä¸‹: </p>

<pre><code class="language-c++">#ifndef CAFFE_LAYER_H_
#define CAFFE_LAYER_H_

#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/layer_factory.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

/**
 Forward declare boost::thread instead of including boost/thread.hpp
 to avoid a boost/NVCC issues (#1009, #1010) on OSX.
 */
namespace boost { class mutex; }

namespace caffe {

/**
 * @brief An interface for the units of computation which can be composed into a
 *        Net.
 *
 * Layer%s must implement a Forward function, in which they take their input
 * (bottom) Blob%s (if any) and compute their output Blob%s (if any).
 * They may also implement a Backward function, in which they compute the error
 * gradients with respect to their input Blob%s, given the error gradients with
 * their output Blob%s.
 */
template &lt;typename Dtype&gt;
class Layer {
 public:
  /**
   * You should not implement your own constructor. Any set up code should go
   * to SetUp(), where the dimensions of the bottom blobs are provided to the
   * layer.
   */
   //æ˜¾å¼æ„é€ å‡½æ•°ï¼Œä»LayerParameterå¯¹è±¡ä¸­åŠ è½½é…ç½®
  explicit Layer(const LayerParameter&amp; param)
    : layer_param_(param) {
      // Set phase(è®­ç»ƒ/é¢„æµ‹) and copy blobs (if there are any).
      phase_ = param.phase();
      if (layer_param_.blobs_size() &gt; 0) {
        //æŒ‰ layer_param_è®¾ç½®æœ¬èº«Blobå¯¹è±¡ä¸ªæ•°ï¼Œå¹¶ä¾æ¬¡å°†æ¯ä¸ªBlobå¯¹è±¡å°ºå¯¸è°ƒæ•´ä¸ºä¸layer_param_ä¸­çš„Blobå°ºå¯¸ä¸€è‡´
        blobs_.resize(layer_param_.blobs_size());
        for (int i = 0; i &lt; layer_param_.blobs_size(); ++i) {
          blobs_[i].reset(new Blob&lt;Dtype&gt;());
          blobs_[i]-&gt;FromProto(layer_param_.blobs(i));
        }
      }
    }
    //ææ„å‡½æ•°
  virtual ~Layer() {}

  /**
   * @brief Implements common layer setup functionality.
   *
   * @param bottom the preshaped input blobs
   * @param top
   *     the allocated but unshaped output blobs, to be shaped by Reshape
   *
   * Checks that the number of bottom and top blobs is correct.
   * Calls LayerSetUp to do special layer setup for individual layer types,
   * followed by Reshape to set up sizes of top blobs and internal buffers.
   * Sets up the loss weight multiplier blobs for any non-zero loss weights.
   * This method may not be overridden.
   */
   
   //é…ç½®å‡½æ•°,å®ç°å¸¸ç”¨å±‚é…ç½®æ¥å£ï¼Œä¸å¯è¢«è¦†ç›–
  void SetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    CheckBlobCounts(bottom, top);   //æ£€æŸ¥Blob
    LayerSetUp(bottom, top);        //  ä¸å±‚ç±»å‹ç›¸å…³çš„é…ç½®è¿‡ç¨‹
    Reshape(bottom, top);       //å¯¹Top Blobå˜å½¢
    SetLossWeights(top);        //è®¾ç½®æŸå¤±æƒå€¼å› å­Blob
  }

  /**
   * @brief Does layer-specific setup: your layer should implement this function
   *        as well as Reshape.
   *
   * @param bottom
   *     the preshaped input blobs, whose data fields store the input data for
   *     this layer
   * @param top
   *     the allocated but unshaped output blobs
   *
   * This method should do one-time layer specific setup. This includes reading
   * and processing relevent parameters from the &lt;code&gt;layer_param_&lt;/code&gt;.
   * Setting up the shapes of top blobs and internal buffers should be done in
   * &lt;code&gt;Reshape&lt;/code&gt;, which will be called before the forward pass to
   * adjust the top blob sizes.
   */
   
   //å±‚é…ç½®ï¼ˆè™šï¼‰å‡½æ•°ï¼Œåšç‰¹å®šç±»å‹å±‚ç›¸å…³çš„é…ç½®ï¼Œç”±è¯¥ç±»å‹å±‚è‡ªå·±å®ç°
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {}

  /**
   * @brief Adjust the shapes of top blobs and internal buffers to accommodate
   *        the shapes of the bottom blobs.
   *
   * @param bottom the input blobs, with the requested input shapes
   * @param top the top blobs, which should be reshaped as needed
   *
   * This method should reshape top blobs as needed according to the shapes
   * of the bottom (input) blobs, as well as reshaping any internal buffers
   * and making any other necessary adjustments so that the layer can
   * accommodate the bottom blobs.
   */
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = 0;

  /**
   * @brief Given the bottom blobs, compute the top blobs and the loss.
   *
   * @param bottom
   *     the input blobs, whose data fields store the input data for this layer
   * @param top
   *     the preshaped output blobs, whose data fields will store this layers&#39;
   *     outputs
   * \return The total loss from the layer.
   *
   * The Forward wrapper calls the relevant device wrapper function
   * (Forward_cpu or Forward_gpu) to compute the top blob values given the
   * bottom blobs.  If the layer has any non-zero loss_weights, the wrapper
   * then computes and returns the loss.
   *
   * Your layer should implement Forward_cpu and (optionally) Forward_gpu.
   */
  inline Dtype Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Given the top blob error gradients, compute the bottom blob error
   *        gradients.
   *
   * @param top
   *     the output blobs, whose diff fields store the gradient of the error
   *     with respect to themselves
   * @param propagate_down
   *     a vector with equal length to bottom, with each index indicating
   *     whether to propagate the error gradients down to the bottom blob at
   *     the corresponding index
   * @param bottom
   *     the input blobs, whose diff fields will store the gradient of the error
   *     with respect to themselves after Backward is run
   *
   * The Backward wrapper calls the relevant device wrapper function
   * (Backward_cpu or Backward_gpu) to compute the bottom blob diffs given the
   * top blob diffs.
   *
   * Your layer should implement Backward_cpu and (optionally) Backward_gpu.
   */
  inline void Backward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);

  /**
   * @brief Returns the vector of learnable parameter blobs.
   */
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() {
    return blobs_;
  }

  /**
   * @brief Returns the layer parameter.
   */
  const LayerParameter&amp; layer_param() const { return layer_param_; }

  /**
   * @brief Writes the layer parameter to a protocol buffer
   */
  virtual void ToProto(LayerParameter* param, bool write_diff = false);

  /**
   * @brief Returns the scalar loss associated with a top blob at a given index.
   */
  inline Dtype loss(const int top_index) const {
    return (loss_.size() &gt; top_index) ? loss_[top_index] : Dtype(0);
  }

  /**
   * @brief Sets the loss associated with a top blob at a given index.
   */
  inline void set_loss(const int top_index, const Dtype value) {
    if (loss_.size() &lt;= top_index) {
      loss_.resize(top_index + 1, Dtype(0));
    }
    loss_[top_index] = value;
  }

  /**
   * @brief Returns the layer type.
   */
  virtual inline const char* type() const { return &quot;&quot;; }

  /**
   * @brief Returns the exact number of bottom blobs required by the layer,
   *        or -1 if no exact number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some exact number of bottom blobs.
   */
  virtual inline int ExactNumBottomBlobs() const { return -1; }
  /**
   * @brief Returns the minimum number of bottom blobs required by the layer,
   *        or -1 if no minimum number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some minimum number of bottom blobs.
   */
  virtual inline int MinBottomBlobs() const { return -1; }
  /**
   * @brief Returns the maximum number of bottom blobs required by the layer,
   *        or -1 if no maximum number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some maximum number of bottom blobs.
   */
  virtual inline int MaxBottomBlobs() const { return -1; }
  /**
   * @brief Returns the exact number of top blobs required by the layer,
   *        or -1 if no exact number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some exact number of top blobs.
   */
  virtual inline int ExactNumTopBlobs() const { return -1; }
  /**
   * @brief Returns the minimum number of top blobs required by the layer,
   *        or -1 if no minimum number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some minimum number of top blobs.
   */
  virtual inline int MinTopBlobs() const { return -1; }
  /**
   * @brief Returns the maximum number of top blobs required by the layer,
   *        or -1 if no maximum number is required.
   *
   * This method should be overridden to return a non-negative value if your
   * layer expects some maximum number of top blobs.
   */
  virtual inline int MaxTopBlobs() const { return -1; }
  /**
   * @brief Returns true if the layer requires an equal number of bottom and
   *        top blobs.
   *
   * This method should be overridden to return true if your layer expects an
   * equal number of bottom and top blobs.
   */
  virtual inline bool EqualNumBottomTopBlobs() const { return false; }

  /**
   * @brief Return whether &quot;anonymous&quot; top blobs are created automatically
   *        by the layer.
   *
   * If this method returns true, Net::Init will create enough &quot;anonymous&quot; top
   * blobs to fulfill the requirement specified by ExactNumTopBlobs() or
   * MinTopBlobs().
   */
  virtual inline bool AutoTopBlobs() const { return false; }

  /**
   * @brief Return whether to allow force_backward for a given bottom blob
   *        index.
   *
   * If AllowForceBackward(i) == false, we will ignore the force_backward
   * setting and backpropagate to blob i only if it needs gradient information
   * (as is done when force_backward == false).
   */
  virtual inline bool AllowForceBackward(const int bottom_index) const {
    return true;
  }

  /**
   * @brief Specifies whether the layer should compute gradients w.r.t. a
   *        parameter at a particular index given by param_id.
   *
   * You can safely ignore false values and always compute gradients
   * for all parameters, but possibly with wasteful computation.
   */
  inline bool param_propagate_down(const int param_id) {
    return (param_propagate_down_.size() &gt; param_id) ?
        param_propagate_down_[param_id] : false;
  }
  /**
   * @brief Sets whether the layer should compute gradients w.r.t. a
   *        parameter at a particular index given by param_id.
   */
  inline void set_param_propagate_down(const int param_id, const bool value) {
    if (param_propagate_down_.size() &lt;= param_id) {
      param_propagate_down_.resize(param_id + 1, true);
    }
    param_propagate_down_[param_id] = value;
  }


 protected:
  /** The protobuf that stores the layer parameters */
  LayerParameter layer_param_;
  /** The phase: TRAIN or TEST */
  Phase phase_;
  /** The vector that stores the learnable parameters as a set of blobs. */
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;
  /** Vector indicating whether to compute the diff of each param blob. */
  vector&lt;bool&gt; param_propagate_down_;

  /** The vector that indicates whether each top blob has a non-zero weight in
   *  the objective function. */
  vector&lt;Dtype&gt; loss_;

  /** @brief Using the CPU device, compute the layer output. */
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = 0;
  /**
   * @brief Using the GPU device, compute the layer output.
   *        Fall back to Forward_cpu() if unavailable.
   */
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // LOG(WARNING) &lt;&lt; &quot;Using CPU code as backup.&quot;;
    return Forward_cpu(bottom, top);
  }

  /**
   * @brief Using the CPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   */
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) = 0;
  /**
   * @brief Using the GPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   *        Fall back to Backward_cpu() if unavailable.
   */
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // LOG(WARNING) &lt;&lt; &quot;Using CPU code as backup.&quot;;
    Backward_cpu(top, propagate_down, bottom);
  }

  /**
   * Called by the parent Layer&#39;s SetUp to check that the number of bottom
   * and top Blobs provided as input match the expected numbers specified by
   * the {ExactNum,Min,Max}{Bottom,Top}Blobs() functions.
   */
  virtual void CheckBlobCounts(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
                               const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    if (ExactNumBottomBlobs() &gt;= 0) {
      CHECK_EQ(ExactNumBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes &quot; &lt;&lt; ExactNumBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (MinBottomBlobs() &gt;= 0) {
      CHECK_LE(MinBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes at least &quot; &lt;&lt; MinBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (MaxBottomBlobs() &gt;= 0) {
      CHECK_GE(MaxBottomBlobs(), bottom.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer takes at most &quot; &lt;&lt; MaxBottomBlobs()
          &lt;&lt; &quot; bottom blob(s) as input.&quot;;
    }
    if (ExactNumTopBlobs() &gt;= 0) {
      CHECK_EQ(ExactNumTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces &quot; &lt;&lt; ExactNumTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (MinTopBlobs() &gt;= 0) {
      CHECK_LE(MinTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces at least &quot; &lt;&lt; MinTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (MaxTopBlobs() &gt;= 0) {
      CHECK_GE(MaxTopBlobs(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces at most &quot; &lt;&lt; MaxTopBlobs()
          &lt;&lt; &quot; top blob(s) as output.&quot;;
    }
    if (EqualNumBottomTopBlobs()) {
      CHECK_EQ(bottom.size(), top.size())
          &lt;&lt; type() &lt;&lt; &quot; Layer produces one top blob as output for each &quot;
          &lt;&lt; &quot;bottom blob input.&quot;;
    }
  }

  /**
   * Called by SetUp to initialize the weights associated with any top blobs in
   * the loss function. Store non-zero loss weights in the diff blob.
   */
  inline void SetLossWeights(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    const int num_loss_weights = layer_param_.loss_weight_size();
    if (num_loss_weights) {
      CHECK_EQ(top.size(), num_loss_weights) &lt;&lt; &quot;loss_weight must be &quot;
          &quot;unspecified or specified once per top blob.&quot;;
      for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
        const Dtype loss_weight = layer_param_.loss_weight(top_id);
        if (loss_weight == Dtype(0)) { continue; }
        this-&gt;set_loss(top_id, loss_weight);
        const int count = top[top_id]-&gt;count();
        Dtype* loss_multiplier = top[top_id]-&gt;mutable_cpu_diff();
        caffe_set(count, loss_weight, loss_multiplier);
      }
    }
  }

 private:
  DISABLE_COPY_AND_ASSIGN(Layer);
};  // class Layer

// Forward and backward wrappers. You should implement the cpu and
// gpu specific implementations instead, and should not change these
// functions.
template &lt;typename Dtype&gt;
inline Dtype Layer&lt;Dtype&gt;::Forward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  Dtype loss = 0;
  Reshape(bottom, top);
  switch (Caffe::mode()) {
  case Caffe::CPU:
    Forward_cpu(bottom, top);
    for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
      if (!this-&gt;loss(top_id)) { continue; }
      const int count = top[top_id]-&gt;count();
      const Dtype* data = top[top_id]-&gt;cpu_data();
      const Dtype* loss_weights = top[top_id]-&gt;cpu_diff();
      loss += caffe_cpu_dot(count, data, loss_weights);
    }
    break;
  case Caffe::GPU:
    Forward_gpu(bottom, top);
#ifndef CPU_ONLY
    for (int top_id = 0; top_id &lt; top.size(); ++top_id) {
      if (!this-&gt;loss(top_id)) { continue; }
      const int count = top[top_id]-&gt;count();
      const Dtype* data = top[top_id]-&gt;gpu_data();
      const Dtype* loss_weights = top[top_id]-&gt;gpu_diff();
      Dtype blob_loss = 0;
      caffe_gpu_dot(count, data, loss_weights, &amp;blob_loss);
      loss += blob_loss;
    }
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
  return loss;
}

template &lt;typename Dtype&gt;
inline void Layer&lt;Dtype&gt;::Backward(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  switch (Caffe::mode()) {
  case Caffe::CPU:
    Backward_cpu(top, propagate_down, bottom);
    break;
  case Caffe::GPU:
    Backward_gpu(top, propagate_down, bottom);
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

// Serialize LayerParameter to protocol buffer
template &lt;typename Dtype&gt;
void Layer&lt;Dtype&gt;::ToProto(LayerParameter* param, bool write_diff) {
  param-&gt;Clear();
  param-&gt;CopyFrom(layer_param_);
  param-&gt;clear_blobs();
  for (int i = 0; i &lt; blobs_.size(); ++i) {
    blobs_[i]-&gt;ToProto(param-&gt;add_blobs(), write_diff);
  }
}

}  // namespace caffe

#endif  // CAFFE_LAYER_H_

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[æ€æ ·ç§»é™¤OSXRESERVEDåˆ†åŒº(å¦‚æœBootCamp Assistantåœ¨å®‰è£…ä¹‹åæ²¡æœ‰æˆåŠŸåˆ é™¤è¿™ä¸ªåˆ†åŒº)]]></title>
    <link href="https://lockxmonk.github.io/14974281629389.html"/>
    <updated>2017-06-14T16:16:02+08:00</updated>
    <id>https://lockxmonk.github.io/14974281629389.html</id>
    <content type="html"><![CDATA[
<ol>
<li>æ‰“å¼€ç£ç›˜å·¥å…·</li>
<li>ç‚¹å‡»ä¸»ç‰©ç†ç£ç›˜(ä¸æ˜¯ä¸‹è¾¹çš„åˆ†åŒº)</li>
<li>ç‚¹å‡»åˆ†åŒºæŒ‰é’®</li>
<li>When you see the pie chart, click on the OSXRESERVED partition in the pie chart</li>
<li>ç‚¹å‡»<code>-</code>æ ‡å¿—</li>
<li>click apply</li>
</ol>

<p>Thats it! </p>

<p><strong>DON&#39;T use the ERASE Button in Disk Utility! That will cause problems and it won&#39;t reallocate space back.</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[é‡æ–°åˆ©ç”¨boot campå®‰è£…win10]]></title>
    <link href="https://lockxmonk.github.io/14974276192148.html"/>
    <updated>2017-06-14T16:06:59+08:00</updated>
    <id>https://lockxmonk.github.io/14974276192148.html</id>
    <content type="html"><![CDATA[
<p>ç”±äºåŸæ¥ç»™win10åˆ†çš„ç£ç›˜å®¹é‡å¤ªå°,è€Œmacåˆä¸èƒ½åœ¨åç»­åŠ¨æ€ç»™winåˆ†åŒºå¢åŠ ç£ç›˜å®¹é‡,æ‰€ä»¥åªèƒ½é‡æ–°å®‰è£…åˆ†åŒºäº†.</p>

<ol>
<li>æ‰“å¼€macç³»ç»Ÿå·¥å…·ä¸­çš„ç£ç›˜<code>ç£ç›˜å·¥å…·</code>é€‰æ‹©æ€»çš„ç£ç›˜,ç‚¹å‡»ä¸Šæ–¹çš„<code>åˆ†åŒº</code>æŒ‰é’®,å°†ç³»ç»Ÿä¸­å…¶å®ƒåˆ†åŒºåˆ æ‰(é€šè¿‡ç‚¹å‡»é¥¼å›¾ä¸­çš„å¯¹åº”åŒºåŸŸ,ç‚¹å‡»ä¸‹æ–¹çš„<code>-</code>å·),å°±å¯ä»¥å°†åˆ†åŒºåˆ æ‰,åˆå¹¶åˆ°macç³»ç»Ÿç£ç›˜.å…¶ä»–åˆ†åŒºæ¯”å¦‚ä¹‹å‰å®‰è£…winäº§ç”Ÿçš„<code>OSXRESERVED Partition</code> </li>
<li>æ‰“å¼€bootcamp,ç‚¹å‡»æ¢å¤æŒ‰é’®,å°†ä¹‹å‰çš„winç³»ç»ŸæŠ¹æ‰,ä¹‹åå°±ä¼šå‘ç°,ä½ çš„macç³»ç»Ÿç£ç›˜å˜å›æ¥äº†.</li>
<li>é‡å¯ä¸€ä¸‹,ä¹‹åæ‰“å¼€bootcamp,ç‚¹å‡»ä¸‹æ–¹çš„<code>ç»§ç»­</code>æŒ‰é’®,é€‰æ‹©ç³»ç»Ÿisoé•œåƒ,ä¸ºwinç³»ç»Ÿé€‰æ‹©åˆ†åŒºå¤§å°(è¿™å›åˆ†å¤šäº›ğŸ˜“).</li>
<li>ç‚¹å‡»ç¡®å®š,ç­‰å¾…ä¸‹è½½Windowsæ”¯æŒè½¯ä»¶.
<img src="media/14974276192148/14974281373736.jpg" alt=""/></li>
<li>å®‰è£…ä¹‹å,åœ¨OSRESEVERåˆ†åŒºä¸­,çš„bootcampæ–‡ä»¶å¤¹ä¸­æ‰“å¼€å®‰è£…é©±åŠ¨çš„ç¨‹åº.</li>
</ol>

<p>åˆ°æ­¤win10 åº”è¯¥å°±å®‰è£…å¥½äº†~</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[caffeæ•°æ®ç»“æ„æè¿°]]></title>
    <link href="https://lockxmonk.github.io/14970763342670.html"/>
    <updated>2017-06-10T14:32:14+08:00</updated>
    <id>https://lockxmonk.github.io/14970763342670.html</id>
    <content type="html"><![CDATA[
<p>æ‰“å¼€caffeç›®å½•ä¸‹çš„<code>src/caffe/proto/caffe.proto</code>æ–‡ä»¶,é¦–å…ˆè®²çš„å°±æ˜¯Blobçš„æè¿°.</p>

<pre><code class="language-proto">// è¯¥ç»“æ„æè¿°äº† Blobçš„å½¢çŠ¶ä¿¡æ¯
message BlobShape {
  repeated int64 dim = 1 [packed = true];  //åªåŒ…æ‹¬è‹¥å¹²int64ç±»å‹å€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºBlobæ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚packedè¡¨ç¤ºè¿™äº›å€¼åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒï¼Œæ²¡æœ‰ç©ºæ´
}

//è¯¥ç»“æ„æè¿°Blobåœ¨ç£ç›˜ä¸­åºåˆ—åŒ–åçš„å½¢æ€
message BlobProto {
  optional BlobShape shape = 7;    //å¯é€‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªBlobShapeå¯¹è±¡
  repeated float data = 5 [packed = true]; // //åŒ…æ‹¬è‹¥åƒæµ®ç‚¹å…ƒç´ ï¼Œå­˜å‚¨æ•°æ®æˆ–æƒå€¼ï¼Œå…ƒç´ æ•°ç›®ç”±shapeæˆ–ï¼ˆnum, channels, height, width)ç¡®å®šï¼Œè¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒ.
  repeated float diff = 6 [packed = true];  ////åŒ…æ‹¬è‹¥å¹²æµ®ç‚¹å…ƒç´ ï¼Œç”¨äºå­˜å‚¨å¢é‡ä¿¡æ¯ï¼Œç»´åº¦ä¸data æ•°ç»„ä¸€è‡´
  repeated double double_data = 8 [packed = true];  // ä¸ dataå¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸ºdouble
  repeated double double_diff = 9 [packed = true];  // ä¸ diff å¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸º double

  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}
</code></pre>

<p><strong>è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨protobufferä¸»è¦æ˜¯å› ä¸ºå®ƒå…·æœ‰å¾ˆå¥½çš„å¥å£®æ€§,å°†ç¼–ç¨‹æœ€å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹åŠ ä»¥éšè—ï¼Œè®©æœºå™¨è‡ªåŠ¨å¤„ç†.</strong></p>

<h2 id="toc_0">Blobçš„æ„æˆ</h2>

<p>Blobæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»,å£°æ˜åœ¨<code>include/caffe/blob.hppä¸­</code>,é‡Œé¢å°è£…äº†ä¸€äº›åŸºæœ¬çš„Layer,Net,Solverç­‰,è¿˜æœ‰syncedmemç±»:</p>

<pre><code class="language-C++">
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;//ç”±protocç”Ÿæˆçš„å¤´æ–‡ä»¶ï¼Œå£°æ˜äº† BlobProtoã€BlobShapeç­‰éµå¾ªcaffe.protoåè®®çš„æ•°æ®ç»“æ„ å¯ä»¥åœ¨src/caffe/protoæ–‡ä»¶ä¸‹è¿è¡Œprotoc caffe.proto --cpp_out=./å‘½ä»¤ç”Ÿæˆè¯¥å¤´æ–‡ä»¶.
#include &quot;caffe/syncedmem.hpp&quot;  //CPU/GPUå…±äº«å†…å­˜ç±»ï¼Œç”¨äºæ•°æ®åŒæ­¥

const int kMaxBlobAxes = 32;    //Blobæœ€å¤§ç»´æ•°ç›®
template &lt;typename Dtype&gt;
class Blob {    //ç±»å£°æ˜
 public:
    //é»˜è®¤æ„é€ å‡½æ•°
  Blob()
       : data_(), diff_(), count_(0), capacity_(0) {}
    //æ˜¾å¼æ„é€ å‡½æ•°
  explicit Blob(const int num, const int channels, const int height, const int width);
  explicit Blob(const vector&lt;int&gt;&amp; shape);

 //å˜å½¢å‡½æ•°ï¼ŒæŠ¥æ®è¾“å…¥å‚æ•°é‡æ–°è®¾ç½®å½“å‰Blobå½¢çŠ¶,å¿…è¦æ—¶é‡æ–°åˆ†é…å†…å­˜
  void Reshape(const int num, const int channels, const int height,
      const int width);
  
  void Reshape(const vector&lt;int&gt;&amp; shape);
  void Reshape(const BlobShape&amp; shape);
  void ReshapeLike(const Blob&amp; other);
  //å¾—åˆ°Blobå½¢çŠ¶å­—ç¬¦ä¸²ç”¨äºæ‰“å°log,è§Caffeè¿è¡Œlog,ç±»ä¼¼&quot;Top shape: 100 1 28 28 (78400)&quot;
  inline string shape_string() const {
    ostringstream stream;
    for (int i = 0; i &lt; shape_.size(); ++i) {
      stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;
    }
    stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;
    return stream.str();
  }
  //è¿”å›Blobå½¢çŠ¶
  inline const vector&lt;int&gt;&amp; shape() const { return shape_; }
    //è¿”å›æŸ1ç»´åº¦çš„å°ºå¯¸
  inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  //è¿”å›ç»´åº¦æ•°ç›®
  inline int num_axes() const { return shape_.size(); }
  //è¿”å›Blobä¸­å…ƒç´ æ€»æ•°
  inline int count() const { return count_; }
    //è¿”å›Blobä¸­æŸå‡ ç»´å­é›†çš„å…ƒç´ æ€»æ•°
    inline int count(int start_axis, int end_axis) const {
    CHECK_LE(start_axis, end_axis); //ä¿è¯ start_axis &lt;= end_axis
    CHECK_GE(start_axis, 0);  // ä¿è¯ start_axis &gt;= 0
    CHECK_GE(end_axis, 0);      // ä¿è¯ end_axis &gt;= 0
    CHECK_LE(start_axis, num_axes()); //ä¿è¯start_axis    &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    CHECK_LE(end_axis, num_axes()); //ä¿è¯end_axis &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    int count = 1;
    for (int i = start_axis; i &lt; end_axis; ++i) {
      count *= shape(i);
    }
    return count;
  }
  //è®¡ç®—ä»æŸä¸€ç»´åº¦å¼€å§‹çš„å…ƒç´ æ€»æ•°
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
  //è½¬æ¢åæ ‡è½´ç´¢å¼•[-N,N)ä¸ºæ™®é€šç´¢å¼•[0,N)
  inline int CanonicalAxisIndex(int axis_index) const {
    CHECK_GE(axis_index, -num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    CHECK_LT(axis_index, num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    if (axis_index &lt; 0) {
    //è´Ÿç´¢å¼•è¡¨ç¤ºä»åå‘å‰è®¿é—®ï¼Œ-1è¡¨ç¤ºæœ€åä¸€ä¸ªä¸ªå…ƒç´ ï¼Œæ™®é€šç´¢å¼•å€¼ä¸º N-1:åŒç†ï¼Œ-2 =&gt; N-2, -3 =&gt; N-3,â€¦
      return axis_index + num_axes();
    }
    return axis_index;
  }
  //è·å–æŸä¸€ç»´çš„å°ºå¯¸
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.
  inline int num() const { return LegacyShape(0); }
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.
  inline int channels() const { return LegacyShape(1); }
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.
  inline int height() const { return LegacyShape(2); }
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.
  inline int width() const { return LegacyShape(3); }
  inline int LegacyShape(int index) const {
    CHECK_LE(num_axes(), 4)
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;
    CHECK_LT(index, 4);
    CHECK_GE(index, -4);
    if (index &gt;= num_axes() || index &lt; -num_axes()) {
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse
      // indexing) -- this special case simulates the one-padding used to fill
      // extraneous axes of legacy blobs.
      return 1;
    }
    return shape(index);
  }
  //ä¸‹é¢çš„æ˜¯è®¡ç®—åç§»é‡çš„å‡½æ•°
  inline int offset(const int n, const int c = 0, const int h = 0,
      const int w = 0) const {
    CHECK_GE(n, 0);
    CHECK_LE(n, num());
    CHECK_GE(channels(), 0);
    CHECK_LE(c, channels());
    CHECK_GE(height(), 0);
    CHECK_LE(h, height());
    CHECK_GE(width(), 0);
    CHECK_LE(w, width());
    return ((n * channels() + c) * height() + h) * width() + w;
  }

  inline int offset(const vector&lt;int&gt;&amp; indices) const {
    CHECK_LE(indices.size(), num_axes());
    int offset = 0;
    for (int i = 0; i &lt; num_axes(); ++i) {
      offset *= shape(i);
      if (indices.size() &gt; i) {
        CHECK_GE(indices[i], 0);
        CHECK_LT(indices[i], shape(i));
        offset += indices[i];
      }
    }
    return offset;
  }
  //æŒ‰å€¼æ‹·è´Blobåˆ°å½“å‰Blob
  void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false);
  
  //ä¸‹é¢å‡ ä¸ªå‡½æ•°æ˜¯å­˜å–å™¨(getter/setter)
  inline Dtype data_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_data()[offset(n, c, h, w)];
  }

  inline Dtype diff_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_diff()[offset(n, c, h, w)];
  }

  inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_data()[offset(index)];
  }

  inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_diff()[offset(index)];
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;
  }
  
  //åªè¯»è®¿é—®cpu_date
  const Dtype* cpu_data() const;
  //è®¾ç½®cpu_date
  void set_cpu_data(Dtype* data);
  const int* gpu_shape() const;
  //åªè¯»è®¿é—®gpu_date
  const Dtype* gpu_data() const;
  //è®¾ç½®gpu_date
  void set_gpu_data(Dtype* data);
  //åªè¯»è®¿é—®cpu_diff
  const Dtype* cpu_diff() const;
  //åªè¯»è®¿é—®gpu_diff
  const Dtype* gpu_diff() const;
  //ä¸‹é¢å››ä¸ªæ˜¯è¯»å†™è®¿é—®æ•°æ®
  Dtype* mutable_cpu_data();
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  void Update();    //Blobæ›´æ–°è¿ç®—ï¼Œå¯ç®€å•ç†è§£ä¸ºdataä¸diffçš„mergeè¿‡ç¨‹
  //ååºåˆ—åŒ–å‡½æ•°ï¼Œä»BlobProtoä¸­æ¢å¤ä¸ªBlobå¯¹è±¡
  void FromProto(const BlobProto&amp; proto, bool reshape = true);
  //åºåˆ—åŒ–å‡½æ•°ï¼Œå°†å†…å­˜ä¸­çš„Blobå¯¹è±¡ä¿å­˜åˆ°BlobProtoä¸­
  void ToProto(BlobProto* proto, bool write_diff = false) const;

  /// @brief Compute the sum of absolute values (L1 norm) of the data.
  Dtype asum_data() const;
  /// @brief Compute the sum of absolute values (L1 norm) of the diff.
  Dtype asum_diff() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the data.
  Dtype sumsq_data() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the diff.
  Dtype sumsq_diff() const;

/// @brief Scale the blob data by a constant factor.
  void scale_data(Dtype scale_factor);
  /// @brief Scale the blob diff by a constant factor.
  void scale_diff(Dtype scale_factor);
 // å…±äº«å¦ä¸€ä¸ª Blob çš„ diff
  void ShareData(const Blob&amp; other);
  void ShareDiff(const Blob&amp; other);
  
  protected:
  shared_ptr&lt;SyncedMemory&gt; data_;   //å­˜æ”¾æŒ‡å‘dataçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; diff_;   //å­˜æ”¾æŒ‡å‘diffçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; shape_data_; 
  vector&lt;int&gt; shape_;   //å½¢çŠ¶ä¿¡æ¯
  int count_;   //å­˜æ”¾æœ‰æ•ˆå…ƒç´ æ•°ç›®ä¿¡æ¯
  int capacity_;    //å­˜æ”¾Blobå®¹å™¨çš„å®¹é‡ä¿¡æ¯

  DISABLE_COPY_AND_ASSIGN(Blob);    //ç¦ç”¨æ‹·è´æ„é€ å‡½æ•°ã€é™šå€¼è¿ç®—ç¬¦é‡è½½
};  // class Blob

</code></pre>

<p><strong>æ³¨æ„åˆ°Caffeç±»ä¸­æˆå‘˜å˜é‡åéƒ½å¸¦æœ‰åç¼€ï¼Œè¿™æ ·åœ¨å‡½æ•°å®ç°ä¸­å®¹æ˜“åŒºåˆ†ä¸´æ—¶å˜é‡å’Œç±»æˆå‘˜å˜é‡ã€‚</strong></p>

<p>æ‰“å¹µ<code>include/caffe/syncedmem.hpp</code>ï¼ŒæŸ»çœ‹è¯¥ç±»çš„ç”¨æ³•:</p>

<pre><code class="language-c++">#ifndef CAFFE_SYNCEDMEM_HPP_
#define CAFFE_SYNCEDMEM_HPP_

#include &lt;cstdlib&gt;

#ifdef USE_MKL
  #include &quot;mkl.h&quot;
#endif

#include &quot;caffe/common.hpp&quot;

namespace caffe {

//å¦‚æœåœ¨GPUæ¨¡å¼ï¼Œä¸”CUDAä½¿èƒ½ï¼Œé‚£ä¹ˆä¸»æœºå†…å­˜ä¼šä»¥é¡µé”å®šå†…å­˜æ–¹å¼åˆ†é…ï¼ˆä½¿ç”¨cudaMallocHostUå‡½æ•°ã€‚å¯¹f-å•GPUçš„æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œä½†å¤šGPUä¼šéå¸¸æ˜æ˜¾)
inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) {
#ifndef CPU_ONLY
  if (Caffe::mode() == Caffe::GPU) {
    CUDA_CHECK(cudaMallocHost(ptr, size));
    *use_cuda = true;
    return;
  }
#endif
#ifdef USE_MKL
  *ptr = mkl_malloc(size ? size:1, 64);
#else
  *ptr = malloc(size);
#endif
  *use_cuda = false;
  CHECK(*ptr) &lt;&lt; &quot;host allocation of size &quot; &lt;&lt; size &lt;&lt; &quot; failed&quot;;
}
// ä¸CaffeMallocHostå¯¹åº”
inline void CaffeFreeHost(void* ptr, bool use_cuda) {
#ifndef CPU_ONLY
  if (use_cuda) {
    CUDA_CHECK(cudaFreeHost(ptr));
    return;
  }
#endif
#ifdef USE_MKL
  mkl_free(ptr);
#else
  free(ptr);
#endif
}

//è¯¥ç±»è´Ÿè´£å­˜å‚¨åˆ†é…ä»¥åŠä¸»æœºå’Œè®¾å¤‡é—´åŒæ­¥
class SyncedMemory {
 public:
 //æ„é€ å‡½æ•°
  SyncedMemory();
  //æ˜¾å¼æ„é€ å‡½æ•°
  explicit SyncedMemory(size_t size);
  //ææ„å‡½æ•°
  ~SyncedMemory();
  const void* cpu_data();       //åªè¯»è·å–cpu data
  void set_cpu_data(void* data);    //è®¾ç½®cpu data
  const void* gpu_data();       //åªè¯»è·å–gpu data
  void set_gpu_data(void* data);    //è®¾ç½®gpu data
  void* mutable_cpu_data();     // è¯»å†™è·å– cpu data
  void* mutable_gpu_data();     // è¯»å†™è·å– gpu data
  //çŠ¶æ€æœºå˜é‡ï¼Œè¡¨ç¤º4ç§çŠ¶æ€ï¼šæœ¯åˆå§‹åŒ–ã€CPUæ•°æ®å¥‹æ•ˆã€GPUæ•°æ®æœ‰æ•ˆã€å·±åŒæ­¥
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  //è·å¾—å½“å‰çŠ¶æ€æœºå˜é‡å€¼
  SyncedHead head() { return head_; }
  //è·å¾—å½“å‰å­˜å‚¨ç©ºé—´å°ºå¯¸
  size_t size() { return size_; }

#ifndef CPU_ONLY
  void async_gpu_push(const cudaStream_t&amp; stream);
#endif

 private:
  void check_device();

  void to_cpu();    //æ•°æ®åŒæ­¥è‡³CPU
  void to_gpu();    //æ•°æ®åŒæ­¥è‡³GPU
  void* cpu_ptr_;   //ä½äºCPUçš„æ•°æ®æŒ‡é’ˆ
  void* gpu_ptr_;   //ä½äºGPUçš„æ•°æ®æŒ‡é’ˆ
  size_t size_;     //å­˜å‚¨ç©ºé—´å¤§å°
  SyncedHead head_; //çŠ¶æ€æœºå˜é‡
  bool own_cpu_data_;   //æ ‡å¿—æ˜¯å¦æ‹¥æœ‰CPUæ•°æ®æ‰€æœ‰æƒï¼ˆå¦ï¼Œå³ä»åˆ«çš„å¯¹è±¡å…±äº«)
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;   ////æ ‡å¿—æ˜¯å¦æ‹¥æœ‰GPUæ•°æ®æ‰€æœ‰æƒ
  int device_;      //è®¾å¤‡å·

  DISABLE_COPY_AND_ASSIGN(SyncedMemory);
};  // class SyncedMemory

}  // namespace caffe

#endif  // CAFFE_SYNCEDMEM_HPP_

</code></pre>

<p>Blobç±»å®ç°çš„æºç ä½äº<code>src/caffe/blob.cpp</code>ä¸­ï¼Œå†…å®¹å¦‚ä¸‹:</p>

<pre><code class="language-c++">
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {
//å˜ç»´å‡½æ•°ï¼Œå°†ï¼ˆnum, channels, height, width}å‚æ•°è½¬æ¢ä¸ºvector&lt;int&gt;ï¼Œç„¶åè°ƒç”¨é‡è½½çš„å˜ç»´å‡½æ•°void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape)
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
//çœŸæ­£å˜ç»´å‡½æ•°
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes); //ä¿è¯vectorç»´åº¦&lt;=kMaxBlobAxes
  count_ = 1;   //ç”¨äºè®¡ç®—å…ƒç´ æ€»æ•°=num * channels * height * width 
  shape_.resize(shape.size());  //æˆå‘˜å˜é‡ç»´åº¦ä¹Ÿè¢«é‡ç½
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);  // ä¿è¯æ¯ç»´åº¦å°ºå¯¸éƒ½&gt;=0
    if (count_ != 0) {
    //è¯count_ä¸æº¢å‡º
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];     //count_ç´¯ä¹˜
    shape_[i] = shape[i];   //ä¸ºæˆå‘˜å˜é‡èµ‹å€¼
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {     //å¦‚æœæ–°çš„count_å¤§äºå½“å‰å·±åˆ†fé…ç©ºé—´å®¹é‡
    capacity_ = count_;         //æ‰©å®¹ï¼Œé‡æ–°åˆ†é…data_å’Œdif f_ç©ºé—´
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

//void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) å’Œvoid Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)ä¸ä¸Šé¢ç±»ä¼¼. 

//æ„é€ å‡½æ•°
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  // è°ƒç”¨Reshapeä¹‹å‰å¿…é¡»åˆå§‹åŒ–capacity_ï¼Œå¦åˆ™ä¼šå¯¼è‡´ä¸å¯é¢„æœŸç»“æœ
  : capacity_(0) {
  Reshape(num, channels, height, width);
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(shape);
}

template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}
//åªè¯»è·å–cpu dateæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);     //ä¿è¯data_ä¸ä¸º NULL
  return (const Dtype*)data_-&gt;cpu_data();
}
//ä¿®æ”¹cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}
//åªè¯»è·å–cpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}
//åªè¯»è·å–gpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}
//è¯»å†™è®¿é—®cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}
//è¯»å†™è®¿é—®gpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}
//ä¸ä¸Šé¢ç›¸åŒ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}
//Update()å‡½æ•°ç”¨äºç½‘ç»œå‚æ•°Blobçš„æ›´æ–°ã€‚å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  // We will perform update based on where the data is located.dataåœ¨å“ªé‡Œæˆ‘ä»¬å°±åœ¨é‚£é‡Œæ›´æ–°
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:       //dataä½äºcpuç«¯
    // æ‰§è¡ŒCPUè®¡ç®—
        caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:   //dataä½äºGPUç«¯,æˆ–è€…CPU/GPUå·²ç»åŒæ­¥
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // æ‰§è¡Œ CPU ä¸Šçš„è®¡ç®—ï¼Œdata_[iã€‘=data_[i] - diff_[i], i = 0,1,2,â€¦ï¼Œcount_-1
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;     //ç¼–æ³½æ—¶æ‰“å¼€äº†CPU_ONLYé€‰é¡¹ï¼Œé‚£ä¹ˆGPUæ¨¡å¼ç¦ç”¨
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
  }
}
//è®¡ç®—data_çš„L1-èŒƒæ•°,å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());  //æ‰§è¡ŒCPUä¸Šçš„asumè®¡ç®—
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
//åŒä¸Š,è®¡ç®—diff_çš„L1èŒƒæ•°
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
  return 0;
}
//è®¡ç®—data_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);  //æ‰§è¡Œ CPUä¸Šçš„dotè®¡ç®—
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//åŒä¸Š,è®¡ç®—diff_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//å¯¹data_è¿›è¡Œå¹…åº¦ç¼©æ”¾
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:   //æ‰§è¡ŒCPUä¸Šçš„è®¡ç®—
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}
//å¯¹diff_è¿›è¡Œç¼©æ”¾,åŒç†
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
}
//åˆ¤æ–­å½¢çŠ¶æ˜¯å¦ç›¸åŒ
template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    // Using deprecated 4D Blob dimensions --
    // shape is (num, channels, height, width).
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.
    // methods as these index from the beginning of the blob shape, where legacy parameter blobs were indexed from the end of the blob shape (e.g., bias Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).
    //è¾“å…¥çš„ç»´åº¦è‹¥ä½¿ç”¨è¿‡æ—¶çš„ç»´åº¦ä¿¡æ¯ï¼ˆnum, channels,height, width)ï¼Œåˆ™éœ€è¦è½¬æ¢ä¸ºæ–°çš„vectorå‚æ•°,ä»£ç ä½¿ç”¨äº†C++ä¸­çš„â€œæ‡’â€é€»è¾‘
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  //ç›´æ¥å¯¹æ¯”
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}
//ä»å¦ä¸€ä¸ªBlobå¯¹è±¡æ‹·è´data (å¯é€‰diff),å¿…è¦æ—¶è¿›è¡Œå˜ç»´
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);      //å¦‚æœè¦å˜ç»´,åˆ™æ‰§è¡Œè¿™ä¸ª
    } else {    //ä¸¤ä¸ªblobå½¢çŠ¶ä¸åŒ,åˆ™æŠ¥é”™
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:      //GPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:      //CPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

//ä»BlobProtoä¸­åŠ è½½ä¸€ä¸ªBlob,é€‚ç”¨äºä»ç£ç›˜è½½å…¥ä¹‹å‰å¯¼å‡ºçš„Blob
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {        //ä»BlobProtoå¯¹è±¡ä¸­è·å¾—æ‰€éœ€å„ä¸ªç»´åº¦ä¿¡æ¯
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      // Using deprecated 4D Blob dimensions --
      // shape is (num, channels, height, width).
      shape.resize(4);
      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }
    Reshape(shape);     //BlobæŒ‰ç…§ç»´åº¦ä¿¡æ¯è¿›è¡Œå˜ç»´
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
  }
  // copy data åŠ è½½æ•°æ®
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯doubleç±»å‹ data
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);   //åŠ è½½double date
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);  //å¦åˆ™åŠ è½½float data
    }
  }
  if (proto.double_diff_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯ double ç±»å‹ diff
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}
//å°†Blobä¸­çš„data(å¯é€‰diff)å¯¼å‡ºåˆ°BlobProtoç»“æ„ä½“.ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜æ–‡ä»¶ä¸­
template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();     //é‡ç½®protoçš„ç»´åº¦,ä¿è¯ä¸blobç›¸åŒ
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();   //æ¸…é™¤data
  proto-&gt;clear_double_diff();   //æ¸…é™¤diff
  const double* data_vec = cpu_data();  //å°†dataå¯¼å‡ºåˆ°proto
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {         //  è‹¥æœ‰write_diffçš„éœ€æ±‚
    const double* diff_vec = cpu_diff();    //å°†diffå¯¼å‡ºåˆ°proto
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}
//åŒä¸Š,åªä¸è¿‡ç±»å‹ä¸ºfloat
template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}
//å®ä¾‹åŒ–Blob   ç±»æ¨¡æ¿ï¼ˆfloat, double)
INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;

}  // namespace caffe

</code></pre>

<p><strong>åˆ°æ­¤,æˆ‘ä»¬å°±äº†è§£äº†Caffeä¸€äº›åŸºæœ¬çš„æ•°æ®ç»“æ„.åé¢å°±åº”è¯¥å­¦ä¹ Layerå±‚ä¸­å¯¹æ•°æ®çš„ä¸€äº›å¤„ç†.</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[caffeæ•°æ®ç»“æ„]]></title>
    <link href="https://lockxmonk.github.io/14969765744787.html"/>
    <updated>2017-06-09T10:49:34+08:00</updated>
    <id>https://lockxmonk.github.io/14969765744787.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Blob</a>
</li>
<li>
<a href="#toc_1">Blobçš„åŸºæœ¬ç”¨æ³•</a>
</li>
</ul>


<p>ä¸€ä¸ªCNNç½‘ç»œæ˜¯ç”±å¤šä¸ªLayerå †å è€Œæˆçš„.å¦‚å›¾æ‰€ç¤º:<br/>
<img src="media/14966518170972/14966520513325.jpg" alt=""/></p>

<p>caffeæŒ‰ç…§æˆ‘ä»¬è®¾è®¡çš„å›¾çº¸(prototxt),ç”¨Blobè¿™äº›ç –å—å»ºæˆä¸€å±‚å±‚(Layer)æ¥¼æˆ¿,æœ€åé€šè¿‡æ–¹æ³•SGDæ–¹æ³•(Solver)è¿›è¡Œç®€è£…ä¿®(Train),ç²¾è£…ä¿®(Finetune)å®ç°çš„.æˆ‘ä»¬è¿™é‡Œå°±æ˜¯å­¦ä¹ è¿™äº›åŸºæœ¬æ¦‚å¿µ.</p>

<h2 id="toc_0">Blob</h2>

<p>Caffeä½¿ç”¨ç§°ä¸ºBlobçš„4ç»´æ•°ç»„ç”¨äºå­˜å‚¨å’Œäº¤æ¢æ•°æ®.Blobæä¾›äº†ç»Ÿä¸€çš„å­˜å‚¨å™¨æ¥å£,æŒæœ‰ä¸€æ‰¹å›¾åƒæˆ–å…¶å®ƒæ•°æ®,æƒå€¼,æƒå€¼æ›´æ–°å€¼. å…¶å®ƒæœºå™¨å­¦ä¹ æ¡†æ¶ä¹Ÿæœ‰ç±»ä¼¼çš„æ•°æ®ç»“æ„.</p>

<p><strong>Blobåœ¨å†…å­˜ä¸­ä¸º4ç»´æ•°ç»„,åˆ†åˆ«ä¸º<code>(width_,height_,channels_,num_)</code>,width_å’Œheight_è¡¨ç¤ºå›¾åƒçš„å®½å’Œé«˜,channel_è¡¨ç¤ºé¢œè‰²é€šé“RGB,num_è¡¨ç¤ºç¬¬å‡ å¸§,ç”¨äºå­˜å‚¨æ•°æ®æˆ–æƒå€¼(data)å’Œæƒå€¼å¢é‡(diff),åœ¨è¿›è¡Œç½‘è·¯è®¡ç®—æ—¶,æ¯å±‚çš„è¾“å…¥,è¾“å‡ºéƒ½éœ€è¦Blobå¯¹è±¡ç¼“å†².Blobæ˜¯Caffeçš„åŸºæœ¬å­˜å‚¨å•å…ƒ.</strong></p>

<h2 id="toc_1">Blobçš„åŸºæœ¬ç”¨æ³•</h2>

<p>Blobæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»,æ‰€ä»¥åˆ›å»ºå¯¹è±¡æ—¶éœ€è¦åˆ¶å®šæ¨¡æ¿å‚æ•°.æˆ‘ä»¬è¿™é‡Œå†™ä¸€ä¸ªç®€å•çš„æµ‹è¯•ç¨‹åº<code>blob_demo.cpp</code>å°†å®ƒæ”¾åœ¨<code>caffe</code>çš„å®‰è£…ç›®å½•ä¸‹:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>ä¸Šé¢ä»£ç é¦–å…ˆåˆ›å»ºäº†æ•´å‹Blobå¯¹è±¡a,æ‰“å°å…¶ç»´åº¦ä¿¡æ¯,ç„¶åè°ƒç”¨å…¶<code>Reshape()</code>æ–¹æ³•,å†æ¬¡æ‰“å°å…¶ç»´åº¦ä¿¡æ¯.</p>

<p>ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æ¥ç¼–è¯‘ä¸Šé¢çš„æ–‡ä»¶.</p>

<pre><code>g++ -o app blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe
</code></pre>

<p>ç”Ÿæˆäº†å¯æ‰§è¡Œç¨‹åº<code>app</code></p>

<p>è¿™ä¸ªæ—¶å€™è¿è¡Œ<code>app</code>çš„è¯å¯èƒ½ä¼šé‡åˆ°ä¸‹é¢è¿™ä¸ªé”™è¯¯:<br/>
<img src="media/14966518170972/14968048615065.jpg" alt=""/><br/>
è¿™ä¸ªå› ä¸º<code>app</code>æ²¡æœ‰é“¾æ¥åˆ°è¿™ä¸ªåŠ¨æ€åº“æ–‡ä»¶,æ‰§è¡Œä¸‹è¾¹è¿™ä¸ªå‘½ä»¤é“¾æ¥:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app
</code></pre>

<p><code>/usr/local/Cellar/caffe/build/lib/</code>ä¸º<code>@rpath/libcaffe.so.1.0.0</code>åŠ¨æ€åº“çš„è·¯å¾„.</p>

<p>æ‰§è¡Œå,å†æ¬¡è¿è¡Œä¼šé‡åˆ°é”™è¯¯:<br/>
<img src="media/14966518170972/14968049981988.jpg" alt=""/></p>

<p>ä¸ä¸Šé¢ç±»ä¼¼,è¿™æ˜¯å› ä¸ºæ²¡æœ‰é“¾æ¥åˆ°<code>@rpath/libhdf5_hl.10.dylib</code><br/>
æ‰§è¡Œä¸‹é¢è¿™ä¸ªå‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/lib/libcaffe.so.1.0.0
</code></pre>

<p>å…¶ä¸­<code>/Users/liangzhonghao/anaconda2/lib</code>åŒ…å«è¿™ä¸ªåº“æ–‡ä»¶.</p>

<p>å†æ¬¡æ‰§è¡Œapp,ç»ˆäºæˆåŠŸäº†!<br/>
<img src="media/14966518170972/14968051024802.jpg" alt=""/></p>

<p>åˆ›å»ºäº†Blobå¯¹è±¡ä¹‹å,æˆ‘ä»¬å¯ä»¥é€šè¿‡<code>mutable_cpu[gpu]_data[diff]</code>å‡½æ•°æ¥ä¿®æ”¹å…¶å†…éƒ¨æ•°å€¼:</p>

<p>ä»£ç ä¸º:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    for(int i=0;i&lt;a.count();i++){
        p[i]=i;
    }
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    return 0;
}
</code></pre>

<p>è·Ÿä¸Šé¢ä¸€æ ·ç»§ç»­ç¼–è¯‘å’Œæ‰§è¡Œ,è¿™é‡ŒæŒ‰ç…§ä¸Šé¢çš„å‘½ä»¤ç»§ç»­æ¥ç¼–è¯‘çš„è¯,é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯:<br/>
<img src="media/14966518170972/14968061663863.jpg" alt=""/></p>

<p>ä¹‹åæ¢æˆä¸‹è¾¹çš„å‘½ä»¤æ‰§è¡ŒåæˆåŠŸ:</p>

<pre><code>g++ -o app2 blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe -lglog -lboost_system -lprotobuf
</code></pre>

<p>å·®åˆ«åœ¨äº,åè¾¹åŠ ä¸Šäº†<code>-lglog -lboost_system -lprotobuf</code>å‘½ä»¤,å…·ä½“ä½œç”¨åç»­å°†ç ”ç©¶(æš‚æ—¶ä¸ç†è§£),ç»§ç»­è¿è¡Œå,åˆå‡ºç°äº†é”™è¯¯:<br/>
<img src="media/14966518170972/14968063024883.jpg" alt=""/></p>

<p>åŒæ ·æ˜¯åŠ¨æ€åº“çš„è¿æ¥é—®é¢˜:<br/>
è¿è¡Œå‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app2
</code></pre>

<p>æ‰§è¡Œå‘½ä»¤,ç„¶åè¿è¡Œ<code>app2</code>.å¾—åˆ°è¾“å‡º:<br/>
<img src="media/14966518170972/14968063915684.jpg" alt=""/></p>

<p><strong>å¯è§,Blobä¸‹æ ‡çš„è®¿é—®ä¸c/c++é«˜ç»´æ•°ç»„å‡ ä¹ä¸€è‡´,è€ŒBlobå¥½å¤„åœ¨äºå¯ä»¥ç›´æ¥åŒæ­¥CPU/GPUä¸Šçš„æ•°æ®.</strong></p>

<p>Blobè¿˜æ”¯æŒè®¡ç®—æ‰€æœ‰å…ƒç´ çš„ç»å¯¹å€¼ä¹‹å’Œ(L1-èŒƒæ•°),å¹³æ–¹å’Œ(L2-èŒƒæ•°):</p>

<pre><code>cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
</code></pre>

<p>è¾“å‡ºç»“æœä¸º:</p>

<pre><code>ASUM = 276
SUMSQ = 4324

</code></pre>

<p>é™¤äº†data,æˆ‘ä»¬è¿˜å¯ä»¥æ”¹difféƒ¨åˆ†,ä¸dataçš„æ“ä½œåŸºæœ¬ä¸€è‡´:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //å°†dataåˆå§‹åŒ–ä¸º1,2,3....
        q[i]= a.count()-1-i;   //å°†diffåˆå§‹åŒ–ä¸º23,22,21,...
    }
    
    a.Update();         //æ‰§è¡Œupdateæ“ä½œ,å°†diffä¸dataèåˆ,è¿™ä¹Ÿæ˜¯CNNæƒå€¼æ›´æ–°æ­¥éª¤çš„æœ€ç»ˆå®æ–½è€…
   
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
    
    return 0;
}

</code></pre>

<p>ç„¶åæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ç¼–è¯‘,é“¾æ¥åº“æ–‡ä»¶:</p>

<pre><code>g++ -o app blob_demo_diff.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe  -lglog -lboost_system -lprotobuf

install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/LZHcaffe/./app
</code></pre>

<p><img src="media/14969765744787/14969953318311.jpg" alt=""/></p>

<p>è¿è¡Œ.app,ç»“æœä¸º:<br/>
<img src="media/14969765744787/14969953744774.jpg" alt=""/></p>

<p>ä¸Šé¢è¡¨æ˜,åœ¨<code>Update()</code>å‡½æ•°ä¸­,å®ç°äº†<code>data = data -diff</code>æ“ä½œ,è¿™ä¸ªä¸»è¦æ˜¯åœ¨CNNæƒå€¼æ›´æ–°æ—¶ä¼šç”¨åˆ°,åé¢ç»§ç»­å­¦ä¹ .</p>

<p>å°†Blobå†…éƒ¨å€¼ä¿å­˜åˆ°ç¡¬ç›˜,æˆ–è€…å†²ç¡¬ç›˜è½½å…¥åˆ°å†…å­˜,å¯ä»¥åˆ†åˆ«é€šè¿‡<code>ToProto(),FromProto()</code>å®ç°:</p>

<pre><code class="language-c++">
#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
#include&lt;caffe/util/io.hpp&gt;   //éœ€è¦åŒ…å«è¿™ä¸ªå¤´æ–‡ä»¶
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //å°†dataåˆå§‹åŒ–ä¸º1,2,3....
        q[i]= a.count()-1-i;   //å°†diffåˆå§‹åŒ–ä¸º23,22,21,...
    }
    
    a.Update();         //æ‰§è¡Œupdateæ“ä½œ,å°†diffä¸dataèåˆ,è¿™ä¹Ÿæ˜¯CNNæƒå€¼æ›´æ–°æ­¥éª¤çš„æœ€ç»ˆå®æ–½è€…
   
    BlobProto bp;          //æ„é€ ä¸€ä¸ªBlobProtoå¯¹è±¡
    a.ToProto(&amp;bp,true);    //å°†aåºåˆ—åŒ–,è¿åŒdiff(é»˜è®¤ä¸å¸¦)
    WriteProtoToBinaryFile(bp,&quot;a.blob&quot;);     //å†™å…¥ç£ç›˜æ–‡ä»¶&quot;a.blob&quot;
    BlobProto bp2;           //æ„é€ ä¸€ä¸ªæ–°çš„BlobProtoå¯¹è±¡
    ReadProtoFromBinaryFileOrDie(&quot;a.blob&quot;,&amp;bp2);    //è¯»å–ç£ç›˜æ–‡ä»¶
    Blob&lt;float&gt; b;          //æ–°å»ºä¸€ä¸ªBlobå¯¹è±¡b
    b.FromProto(bp2,true);  //ä»åºåˆ—åŒ–å¯¹è±¡bp2ä¸­å…‹éš†b(è¿åŒå½¢çŠ¶)
    
    for(int u=0;u&lt;b.num();u++){
        for(int v=0;v&lt;b.channels();v++){
            for(int w=0;w&lt;b.height();w++){
                for(int x=0;x&lt;b.width();x++){
                    cout&lt;&lt;&quot;b[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;b.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;b.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;b.sumsq_data()&lt;&lt;endl;
    
    
    return 0;
}

</code></pre>

<p>ç¼–è¯‘,è¿æ¥åº“æ–‡ä»¶å(æ³¨æ„ç¼–è¯‘æ—¶æœ«å°¾åŠ å…¥<code>&quot;-lglog -lboost_system -lprotobuf&quot;</code>é€‰é¡¹),è¾“å‡ºå¦‚ä¸‹:</p>

<p><img src="media/14969765744787/14969964533804.jpg" alt=""/></p>

<p>å¯ä»¥å‘ç°ä¸ä¸Šé¢æ²¡æœ‰å·®åˆ«,åªæ˜¯åœ¨æ–‡ä»¶å¤¹ä¸­å¤šäº†ä¸€ä¸ª<code>Blob.a</code>æ–‡ä»¶,<strong>æ‰€ä»¥<code>BlobProto</code>å¯¹è±¡å®ç°äº†ç¡¬ç›˜ä¸å†…å­˜ä¹‹é—´çš„æ•°æ®é€šä¿¡.å¯ä»¥å¸®åŠ©ä¿å­˜ä¸­é—´æƒå€¼å’Œæ•°æ®</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[æ¿€æ´»å‡½æ•°]]></title>
    <link href="https://lockxmonk.github.io/14964511552671.html"/>
    <updated>2017-06-03T08:52:35+08:00</updated>
    <id>https://lockxmonk.github.io/14964511552671.html</id>
    <content type="html"><![CDATA[
<p>æ·±åº¦ç¥ç»ç½‘ç»œä¹‹æ‰€ä»¥å…·æœ‰ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›ï¼Œé™¤äº†æœ‰æ·±å±‚æ¬¡çš„ç½‘ç»œä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦å› ç´ å³éçº¿æ€§å¤„ç†å•å…ƒ,ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼ˆActivation Function)æˆ–æŒ¤å‹å‡½æ•°ï¼ˆSquashing Function).<strong>æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è¦å…³æ³¨æ€ä¹ˆåœ¨caffeä¸­å®ç°è¿™äº›å‡½æ•°.</strong></p>

<p>ä¸‹å›¾æ˜¯ä¸€ä¸ªç¥ç»å…ƒæ¨¡å‹.\(\varphi(.)\)ä¸ºæ¿€æ´»å‡½æ•°.ä¸»è¦ä½œç”¨æ˜¯å°†ä¸Šä¸€å±‚çš„è¾“å…¥çº¿æ€§ç»„åˆç»“æœ\(u_k\)åŠ¨æ€èŒƒå›´å‹ç¼©åˆ°ç‰¹å®šå€¼åŸŸ(ä¾‹å¦‚[-1,1]).ä¸€èˆ¬æ¥è¯´å…·å¤‡éçº¿æ€§å¤„ç†å•å…ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œ(å¤§äºç­‰äº3å±‚),ç†è®ºä¸Šå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°.<br/>
<img src="media/14964511552671/14964521741597.jpg" alt=""/></p>

<p>å…¶ä¸­å‡ ä¸ªå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°å¦‚ä¸‹:<br/>
1.Sigmoidå‡½æ•°,å€¼åŸŸä¸º(0,1)<br/>
\[<br/>
\varphi(x) = \frac{1}{1+e^{-ax}}<br/>
\]<br/>
<img src="media/14964511552671/14964523348860.jpg" alt=""/></p>

<p>2.tanhå‡½æ•°,å€¼åŸŸä¸º(-1,1):<br/>
\[<br/>
\varphi(x) = \frac{1-e^{-2x}}{1+e^{-2x}}<br/>
\]<br/>
<img src="media/14964511552671/14964526906602.jpg" alt=""/></p>

<p>3.ReLu(Rectified Linear Unitï¼Œè§„æ•´åŒ–çº¿æ€§å•å…ƒ)å‡½æ•°,å€¼åŸŸä¸º\([0,+ \infty)\),æ˜¯ä¸€ç§éé¥±å’Œæ¿€æ´»å‡½æ•°.<br/>
\[<br/>
\varphi(x) = max(0,x)<br/>
\]<br/>
<img src="media/14964511552671/14964530576767.jpg" alt=""/></p>

<p>è¿œä¸æ­¢ä¸Šé¢è¿™äº›æ¿€æ´»å‡½æ•°,éšç€å‘å±•,é™†ç»­åˆå‡ºç°äº†å¾ˆå¤šæ¿€æ´»å‡½æ•°.è¿™é‡Œä¸å¤šä»‹ç».åé¢è¿˜è¦è‡ªå­¦å¾ˆå¤šè¿™ç±»ç›¸å…³çŸ¥è¯†.</p>

<p>ç¥ç»ç½‘ç»œä¸­æœ€å¤§çš„é—®é¢˜æ˜¯æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆGradient Vanishing Problem),è¿™åœ¨ä½¿ç”¨ <code>Sigmoidã€tanh</code>ç­‰é¥±å’Œæ¿€æ´»å‡½æ•°æƒ…å†µä¸‹å°¤ä¸ºä¸¥é‡(ç¥ç»ç½‘ç»œè¿›è¡Œè¯¯å·®åå‘ä¼ æ’­æ—¶ï¼Œå„å±‚éƒ½è¦ä¹˜ä»¥æ¿€æ´»å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°\(G=e\cdot \varphi&#39;(x) \cdot x\)),æ¢¯åº¦æ¯ä¼ é€’ä¸€å±‚éƒ½ä¼šè¡°å‡ä¸€æ¬¡,ç½‘ç»œå±‚æ•°è¾ƒå¤šæ—¶,æ¢¯åº¦Gå°±ä¼šä¸åœçš„è¡°å‡è‡³æ¶ˆå¤±),ä½¿å¾—è®­ç»ƒç½‘ç»œæ—¶æ”¶æ•›ææ…¢,è€ŒReLUè¿™ç±»éé¥±å’Œæ¿€æ´»å‡½æ•°æ”¶æ•›é€Ÿåº¦å°±å¿«å¾ˆå¤š.æ‰€ä»¥å­¦ä¹ ç½‘ç»œæ¨¡å‹ä¸­ä¸€èˆ¬éƒ½ä¼šé€‰ç”¨ç±»ä¼¼ReLuè¿™ç§æ­»æ´»å‡½æ•°.</p>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬å­¦ä¹ åœ¨caffeç”¨ä»£ç å®ç°å¯¹åº”å±‚çš„è®¡ç®—,åŒ…æ‹¬å‰å‘ä¼ æ’­è®¡ç®—å’Œåå‘ä¼ æ’­è®¡ç®—.Caffeçš„æ‰€æœ‰ä¸æ¿€æ´»å‡½æ•°ç›¸å…³çš„Layerç±»å£°æ˜åœ¨<code>include/caffe/layers</code>æ–‡ä»¶å¤¹ä¸­åˆ†åˆ«ä¸º<code>sigmoid_layer.hpp,relu_layer.hpp,tanh_layer.hpp</code>,æˆ‘ä»¬å°†å®ƒä»¬ç»Ÿç§°ä¸º<strong>éçº¿æ€§å±‚</strong>,æˆ‘ä»¬é‡ç‚¹å…³æ³¨<code>ReLULayer,SigmoidLayerå’ŒTanHLayer</code>è¿™ä¸‰ç±».</p>

<p>åœ¨å‰é¢æˆ‘ä»¬æµ‹è¯•çš„LeNet-5æ¨¡å‹ä¸­ä½¿ç”¨äº†ReLuå±‚,æˆ‘ä»¬åœ¨<code>example/mnist/lenet_train_test.prototxt</code>ä¸­æ‰¾åˆ°æè¿°:</p>

<pre><code>layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
</code></pre>

<p>ä¸å·ç§¯å±‚ã€å…¨è¿æ¥å±‚æœ€å¤§çš„ä¸åŒ,å°±æ˜¯æ²¡æœ‰æƒå€¼ç›¸å…³çš„å‚æ•°ï¼Œæè¿°ç›¸å¯¹ç®€å•ã€‚å¦å¤–ä¸¤ç§å±‚æ²¡æœ‰å®é™…æ ·ä¾‹ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶æŒ‰ç…§æˆ‘ä»¬çš„Caffeæºç é˜…è¯»æ–¹æ³•è®º.ä»<code>src/caffe/proto/caffe.proto</code>ä¸­è·å¾—çµæ„Ÿã€‚</p>

<pre><code class="language-c++">// ReLUå±‚å‚æ•°
message ReLUParameter {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  // Leaky ReLUå‚æ•°ï¼Œæˆ‘ä»¬æš‚ä¸å…³å¿ƒ
  optional float negative_slope = 1 [default = 0];
  enum Engine {     //è®¡ç®—å¼•æ“é€‰æ‹©
    DEFAULT = 0;
    CAFFE = 1;      // Caffe å®ç°
    CUDNN = 2;      // CUDNN å®ç°
  }
  optional Engine engine = 2 [default = DEFAULT];
}
</code></pre>

<pre><code class="language-c++">// Sigmoidå±‚å‚æ•°
message SigmoidParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}

</code></pre>

<pre><code class="language-c++">//  tanh å±‚å‚æ•°
message TanHParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}
</code></pre>

<p>éçº¿æ€§å±‚çš„å…±åŒç‰¹ç‚¹å°±æ˜¯å¯¹å‰ä¸€å±‚blobä¸­çš„æ•°å€¼é€ä¸€è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¹¶æ”¾å›åŸblobä¸­ã€‚æ¿€æ´»å‡½æ•°çš„ç±»å£°æ˜å¦‚ä¸‹:</p>

<pre><code class="language-c++">namespace caffe {
//éçº¿æ€§å±‚çš„é¼»ç¥–NeuronLayerï¼Œæ´¾ç”ŸäºLayerç±»ï¼Œç‰¹ç‚¹æ˜¯è¾“å‡ºblob(y)ä¸è¾“å…¥blob(x)å°ºå¯¸ç›¸åŒ

/**
 * @brief An interface for layers that take one blob as input (@f$ x @f$)
 *        and produce one equally-sized blob as output (@f$ y @f$), where
 *        each element of the output depends only on the corresponding input
 *        element.
 */
template &lt;typename Dtype&gt;
class NeuronLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit NeuronLayer(const LayerParameter&amp; param)
     : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline int ExactNumBottomBlobs() const { return 1; }
  virtual inline int ExactNumTopBlobs() const { return 1; }
};

}  // namespace caffe

#endif  // CAFFE_NEURON_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// ReLULayerï¼Œæ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†ReLuæ¿€æ´»å‡½æ•°è®¡ç®—

/**
 * @brief Rectified Linear Unit non-linearity @f$ y = \max(0, x) @f$.
 *        The simple max is fast to compute, and the function does not saturate.
 */
template &lt;typename Dtype&gt;
class ReLULayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
 
  /**
   * @param param provides ReLUParameter relu_param,
   *     with ReLULayer options:
   *   - negative_slope (\b optional, default 0).
   *     the value @f$ \nu @f$ by which negative values are multiplied.
   */
  explicit ReLULayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;ReLU&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \max(0, x)
   *      @f$ by default.  If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed outputs are @f$ y = \max(0, x) + \nu \min(0, x) @f$.
   */
   //å‰å‘ä¼ æ³¢å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the ReLU inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            0 &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$ if propagate_down[0], by default.
   *      If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed gradients are @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            \nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$.
   */
   
   //åå‘ä¼ æ³¢å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_RELU_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// SigmoidLayer,æ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†Sigmoidæ¿€æ´»å‡½æ•°çš„è®¡ç®—
/**
 * @brief Sigmoid function non-linearity @f$
 *         y = (1 + \exp(-x))^{-1}
 *     @f$, a classic choice in neural networks.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class SigmoidLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
  explicit SigmoidLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;Sigmoid&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = (1 + \exp(-x))^{-1}
   *      @f$
   */
   
   //å‰å‘ä¼ æ’­å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y} y (1 - y)
   *      @f$ if propagate_down[0]
   */
   
   //åå‘ä¼ æ’­å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_SIGMOID_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// TanHLayerï¼Œæ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†tanhæ¿€æ´»å‡½æ•°è®¡ç®—
/**
 * @brief TanH hyperbolic tangent non-linearity @f$
 *         y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
 *     @f$, popular in auto-encoders.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class TanHLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
  explicit TanHLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;TanH&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
   *      @f$
   */
   
   //å‰å‘ä¼ æ’­å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y}
   *              \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
   *            = \frac{\partial E}{\partial y} (1 - y^2)
   *      @f$ if propagate_down[0]
   */
   
   //åå‘ä¼ æ’­å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_TANH_LAYER_HPP_
</code></pre>

<p>ä¸Šé¢ç±»çš„å£°æ˜æ¯”è¾ƒç®€å•,å„è‡ªå£°æ˜äº†Forwardå’ŒBackwardå‡½æ•°.ä¸‹é¢å¯¹è¿™äº›å‡½æ•°çš„å®ç°è¿›è¡Œè§£æ.æˆ‘ä»¬é¦–å…ˆçœ‹ä¸‹<code>src/caffe/layers/relu_layer.cpp</code>ä¸­å‰å‘ä¼ æ’­å‡½æ•°çš„å®ç°ä»£ç ã€‚</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // (åªè¯») è·å¾—è¾“äººblobçš„dataæŒ‡é’ˆ
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  // (è¯»å†™ï¼‰è·å¾—è¾“å‡ºblobçš„dataæŒ‡é’ˆ
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  //è·å¾—è¾“å…¥blobå…ƒç´ ä¸ªæ•°
  const int count = bottom[0]-&gt;count();
  // Leaky ReLUå‚æ•°ï¼Œä»layer_paramä¸­è·å¾—ï¼Œé»˜è®¤ä¸º0ï¼Œå³æ™®é€šReLU
  Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
  //æ‰§è¡ŒReLUæ“ä½œæˆ‘ä»¬å§‘ä¸”è®¤ä¸ºnegative_slopå€¼ä¸º0,ä¸è€ƒè™‘Leaky ReLU
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = std::max(bottom_data[i], Dtype(0))
        + negative_slope * std::min(bottom_data[i], Dtype(0));
  }
}
</code></pre>

<p>ä¸å‡ºæ‰€æ–™ï¼Œç”¨ä¸€å±‚forå¾ªç¯å°±æå®šäº†,ä¸‹é¢æˆ‘ä»¬æ¥çœ‹<strong>åå‘ä¼ æ’­å‡½æ•°</strong>çš„å®ç°ä»£ç .</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // å¦‚æœéœ€è¦åšåå‘ä¼ æ’­è®¡ç®—
  if (propagate_down[0]) {
    //(åªè¯»ï¼‰è·å¾—å‰ä¸€å±‚çš„dataæŒ‡é’ˆ
    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
    //(åªè¯») è·å¾—åä¸€å±‚çš„diffæŒ‡é’ˆ
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    //(è¯»å†™) è·å¾—å‰ä¸€å±‚çš„diffæŒ‡é’ˆ
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    //è·å¾—è¦å‚è®¡ç®—çš„å…ƒç´ æ€»æ•°
    const int count = bottom[0]-&gt;count();
    // Leaky ReLUå‚æ•°ï¼Œå§‘ä¸”è®¤ä¸ºæ˜¯0
    Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
    for (int i = 0; i &lt; count; ++i) {
    // ReLUçš„å¯¼å‡½æ•°å°±æ˜¯ï¼ˆbottom_data[i] &gt; 0)ï¼Œæ ¹æ®æ±‚å¯¼é“¾å¼æ³•åˆ™ï¼Œåä¸€å±‚çš„è¯¯å·®ä¹˜ä»¥å¯¼å‡½æ•°å¾—åˆ°å‰ä¸€å±‚çš„è¯¯å·®
      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)
          + negative_slope * (bottom_data[i] &lt;= 0));
    }
  }
}
</code></pre>

<p>åˆ°è¿™é‡Œå¯ä»¥çœ‹åˆ°ReLuè®¡ç®—éå¸¸ç®€å•(ç›®å‰å¦‚æ­¤)</p>

<p>å…¶å®ƒæ¿€æ´»å‡½æ•°æºç ,ä¹‹åä¹Ÿè®¸ç”¨çš„æ¯”è¾ƒå°‘,è¿™é‡Œä¸åšå¤šçš„ä»‹ç».</p>

<p>æ‰€ä»¥,éçº¿æ€§å±‚è™½ç„¶å…¬å¼è¡¨ç¤ºè¾ƒä¸ºå¤æ‚,ä½†ä»£ç å®ç°éƒ½éå¸¸ç®€æ´ã€ç›´è§‚ï¼Œåªè¦æŒæ¡äº†åŸºæœ¬æ±‚å¯¼æŠ€å·§ï¼ŒåŒæ ·å¯ä»¥æ¨å¯¼å‡ºéçº¿æ€§å±‚å…¶ä»–ç±»çš„åå‘ä¼ æ’­å…¬å¼.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UDPç¼–ç¨‹]]></title>
    <link href="https://lockxmonk.github.io/14957648679184.html"/>
    <updated>2017-05-26T10:14:27+08:00</updated>
    <id>https://lockxmonk.github.io/14957648679184.html</id>
    <content type="html"><![CDATA[
<p>TCPæ˜¯å»ºç«‹å¯é è¿æ¥ï¼Œå¹¶ä¸”é€šä¿¡åŒæ–¹éƒ½å¯ä»¥ä»¥æµçš„å½¢å¼å‘é€æ•°æ®ã€‚ç›¸å¯¹TCPï¼ŒUDPåˆ™æ˜¯<strong>é¢å‘æ— è¿æ¥</strong>çš„åè®®ã€‚</p>

<p><font color=red>ä½¿ç”¨UDPåè®®æ—¶ï¼Œä¸éœ€è¦å»ºç«‹è¿æ¥ï¼Œåªéœ€è¦çŸ¥é“å¯¹æ–¹çš„IPåœ°å€å’Œç«¯å£å·ï¼Œå°±å¯ä»¥ç›´æ¥å‘æ•°æ®åŒ…ã€‚ä½†æ˜¯ï¼Œèƒ½ä¸èƒ½åˆ°è¾¾å°±ä¸çŸ¥é“äº†ã€‚</p>

<p>è™½ç„¶ç”¨UDPä¼ è¾“æ•°æ®ä¸å¯é ï¼Œä½†å®ƒçš„ä¼˜ç‚¹æ˜¯å’ŒTCPæ¯”ï¼Œé€Ÿåº¦å¿«ï¼Œå¯¹äºä¸è¦æ±‚å¯é åˆ°è¾¾çš„æ•°æ®ï¼Œå°±å¯ä»¥ä½¿ç”¨UDPåè®®ã€‚</font></p>

<p>æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•é€šè¿‡UDPåè®®ä¼ è¾“æ•°æ®ã€‚å’ŒTCPç±»ä¼¼ï¼Œä½¿ç”¨<mark>UDPçš„é€šä¿¡åŒæ–¹ä¹Ÿåˆ†ä¸ºå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨</mark>ã€‚æœåŠ¡å™¨é¦–å…ˆéœ€è¦ç»‘å®šç«¯å£ï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# ç»‘å®šç«¯å£:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>åˆ›å»ºSocketæ—¶ï¼Œ<code>SOCK_DGRAM</code>æŒ‡å®šäº†è¿™ä¸ªSocketçš„ç±»å‹æ˜¯UDPã€‚ç»‘å®šç«¯å£å’ŒTCPä¸€æ ·ï¼Œä½†æ˜¯ä¸éœ€è¦è°ƒç”¨<code>listen()</code>æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥æ¥æ”¶æ¥è‡ªä»»ä½•å®¢æˆ·ç«¯çš„æ•°æ®ï¼š</p>

<pre><code class="language-py">print &#39;Bind UDP on 9999...&#39;
while True:
    # æ¥æ”¶æ•°æ®:
    data, addr = s.recvfrom(1024)
    print &#39;Received from %s:%s.&#39; % addr
    s.sendto(&#39;Hello, %s!&#39; % data, addr)
</code></pre>

<p><code>recvfrom()</code>æ–¹æ³•è¿”å›æ•°æ®å’Œå®¢æˆ·ç«¯çš„åœ°å€ä¸ç«¯å£ï¼Œè¿™æ ·ï¼ŒæœåŠ¡å™¨æ”¶åˆ°æ•°æ®åï¼Œç›´æ¥è°ƒç”¨<code>sendto()</code>å°±å¯ä»¥æŠŠæ•°æ®ç”¨UDPå‘ç»™å®¢æˆ·ç«¯ã€‚</p>

<p>æ³¨æ„è¿™é‡Œçœæ‰äº†å¤šçº¿ç¨‹ï¼Œå› ä¸ºè¿™ä¸ªä¾‹å­å¾ˆç®€å•ã€‚</p>

<p>å®¢æˆ·ç«¯ä½¿ç”¨UDPæ—¶ï¼Œé¦–å…ˆä»ç„¶åˆ›å»ºåŸºäºUDPçš„Socketï¼Œç„¶åï¼Œä¸éœ€è¦è°ƒç”¨<code>connect()</code>ï¼Œç›´æ¥é€šè¿‡<code>sendto()</code>ç»™æœåŠ¡å™¨å‘æ•°æ®ï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # å‘é€æ•°æ®:
    s.sendto(data, (&#39;127.0.0.1&#39;, 9999))
    # æ¥æ”¶æ•°æ®:
    print s.recv(1024)
s.close()
</code></pre>

<p>å®¢æˆ·ç«¯:<br/>
<img src="media/14957648679184/14957678758780.jpg" alt=""/><br/>
æœåŠ¡å™¨:<br/>
<img src="media/14957648679184/14957678951579.jpg" alt=""/><br/>
å®¢æˆ·ç«¯ä»æœåŠ¡å™¨æ¥æ”¶æ•°æ®ä»ç„¶è°ƒç”¨<code>recv()</code>æ–¹æ³•ã€‚</p>

<h2 id="toc_0">å°ç»“</h2>

<p>UDPçš„ä½¿ç”¨ä¸TCPç±»ä¼¼ï¼Œä½†æ˜¯ä¸éœ€è¦å»ºç«‹è¿æ¥ã€‚æ­¤å¤–ï¼ŒæœåŠ¡å™¨ç»‘å®šUDPç«¯å£å’ŒTCPç«¯å£äº’ä¸å†²çªï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒUDPçš„9999ç«¯å£ä¸TCPçš„9999ç«¯å£å¯ä»¥å„è‡ªç»‘å®šã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ç½‘ç»œç¼–ç¨‹]]></title>
    <link href="https://lockxmonk.github.io/14957594179111.html"/>
    <updated>2017-05-26T08:43:37+08:00</updated>
    <id>https://lockxmonk.github.io/14957594179111.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">TCP/IP</a>
</li>
<li>
<a href="#toc_1">TCPç¼–ç¨‹</a>
<ul>
<li>
<a href="#toc_2">å®¢æˆ·ç«¯</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">æœåŠ¡å™¨</a>
</li>
<li>
<a href="#toc_4">å°ç»“</a>
</li>
</ul>


<h2 id="toc_0">TCP/IP</h2>

<p>ç°åœ¨çš„ç½‘ç»œç¼–ç¨‹åŸºæœ¬éƒ½æ˜¯åœ¨ä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨å†™ä¸€ä¸‹æ¥è¿›è¡Œçš„,<code>äº’è”ç½‘åè®®ç°‡ï¼ˆInternet Protocol Suiteï¼‰</code>å°±æ˜¯é€šç”¨åè®®æ ‡å‡†ã€‚Internetæ˜¯ç”±interå’Œnetä¸¤ä¸ªå•è¯ç»„åˆèµ·æ¥çš„ï¼ŒåŸæ„å°±æ˜¯è¿æ¥â€œç½‘ç»œâ€çš„ç½‘ç»œï¼Œæœ‰äº†Internetï¼Œä»»ä½•ç§æœ‰ç½‘ç»œï¼Œåªè¦æ”¯æŒè¿™ä¸ªåè®®ï¼Œå°±å¯ä»¥è”å…¥äº’è”ç½‘ã€‚</p>

<p>å› ä¸ºäº’è”ç½‘åè®®åŒ…å«äº†ä¸Šç™¾ç§åè®®æ ‡å‡†ï¼Œä½†æ˜¯æœ€é‡è¦çš„ä¸¤ä¸ªåè®®æ˜¯TCPå’ŒIPåè®®ï¼Œæ‰€ä»¥ï¼Œå¤§å®¶æŠŠäº’è”ç½‘çš„åè®®ç®€ç§°<strong>TCP/IP</strong>åè®®ã€‚</p>

<p>é€šä¿¡çš„æ—¶å€™ï¼ŒåŒæ–¹å¿…é¡»çŸ¥é“å¯¹æ–¹çš„æ ‡è¯†ï¼Œå¥½æ¯”å‘é‚®ä»¶å¿…é¡»çŸ¥é“å¯¹æ–¹çš„é‚®ä»¶åœ°å€ã€‚äº’è”ç½‘ä¸Šæ¯ä¸ªè®¡ç®—æœºçš„å”¯ä¸€æ ‡è¯†å°±æ˜¯IPåœ°å€ï¼Œç±»ä¼¼123.123.123.123ã€‚å¦‚æœä¸€å°è®¡ç®—æœºåŒæ—¶æ¥å…¥åˆ°ä¸¤ä¸ªæˆ–æ›´å¤šçš„ç½‘ç»œï¼Œæ¯”å¦‚è·¯ç”±å™¨ï¼Œå®ƒå°±ä¼šæœ‰ä¸¤ä¸ªæˆ–å¤šä¸ªIPåœ°å€ï¼Œæ‰€ä»¥ï¼ŒIPåœ°å€å¯¹åº”çš„å®é™…ä¸Šæ˜¯è®¡ç®—æœºçš„ç½‘ç»œæ¥å£ï¼Œé€šå¸¸æ˜¯ç½‘å¡ã€‚</p>

<p><strong>IPåè®®è´Ÿè´£æŠŠæ•°æ®ä»ä¸€å°è®¡ç®—æœºé€šè¿‡ç½‘ç»œå‘é€åˆ°å¦ä¸€å°è®¡ç®—æœºã€‚æ•°æ®è¢«åˆ†å‰²æˆä¸€å°å—ä¸€å°å—ï¼Œç„¶åé€šè¿‡IPåŒ…å‘é€å‡ºå»ã€‚</strong>ç”±äºäº’è”ç½‘é“¾è·¯å¤æ‚ï¼Œä¸¤å°è®¡ç®—æœºä¹‹é—´ç»å¸¸æœ‰å¤šæ¡çº¿è·¯ï¼Œå› æ­¤ï¼Œ<strong>è·¯ç”±å™¨å°±è´Ÿè´£å†³å®šå¦‚ä½•æŠŠä¸€ä¸ªIPåŒ…è½¬å‘å‡ºå»ã€‚IPåŒ…çš„ç‰¹ç‚¹æ˜¯æŒ‰å—å‘é€ï¼Œé€”å¾„å¤šä¸ªè·¯ç”±ï¼Œä½†ä¸ä¿è¯èƒ½åˆ°è¾¾ï¼Œä¹Ÿä¸ä¿è¯é¡ºåºåˆ°è¾¾ã€‚</strong></p>

<p><font color=red>TCPåè®®åˆ™æ˜¯å»ºç«‹åœ¨IPåè®®ä¹‹ä¸Šçš„ã€‚TCPåè®®è´Ÿè´£åœ¨ä¸¤å°è®¡ç®—æœºä¹‹é—´å»ºç«‹å¯é è¿æ¥ï¼Œä¿è¯æ•°æ®åŒ…æŒ‰é¡ºåºåˆ°è¾¾ã€‚TCPåè®®ä¼šé€šè¿‡æ¡æ‰‹å»ºç«‹è¿æ¥ï¼Œç„¶åï¼Œå¯¹æ¯ä¸ªIPåŒ…ç¼–å·ï¼Œç¡®ä¿å¯¹æ–¹æŒ‰é¡ºåºæ”¶åˆ°ï¼Œå¦‚æœåŒ…ä¸¢æ‰äº†ï¼Œå°±è‡ªåŠ¨é‡å‘ã€‚</font></p>

<p>è®¸å¤šå¸¸ç”¨çš„æ›´é«˜çº§çš„åè®®éƒ½æ˜¯å»ºç«‹åœ¨TCPåè®®åŸºç¡€ä¸Šçš„ï¼Œæ¯”å¦‚ç”¨äºæµè§ˆå™¨çš„HTTPåè®®ã€å‘é€é‚®ä»¶çš„SMTPåè®®ç­‰ã€‚</p>

<p>ä¸€ä¸ªIPåŒ…é™¤äº†åŒ…å«è¦ä¼ è¾“çš„æ•°æ®å¤–ï¼Œè¿˜åŒ…å«æºIPåœ°å€å’Œç›®æ ‡IPåœ°å€ï¼Œæºç«¯å£å’Œç›®æ ‡ç«¯å£ã€‚</p>

<p>ç«¯å£æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿåœ¨ä¸¤å°è®¡ç®—æœºé€šä¿¡æ—¶ï¼Œåªå‘IPåœ°å€æ˜¯ä¸å¤Ÿçš„ï¼Œå› ä¸ºåŒä¸€å°è®¡ç®—æœºä¸Šè·‘ç€å¤šä¸ªç½‘ç»œç¨‹åºã€‚ä¸€ä¸ªIPåŒ…æ¥äº†ä¹‹åï¼Œåˆ°åº•æ˜¯äº¤ç»™æµè§ˆå™¨è¿˜æ˜¯QQï¼Œå°±éœ€è¦<strong>ç«¯å£å·</strong>æ¥åŒºåˆ†ã€‚æ¯ä¸ªç½‘ç»œç¨‹åºéƒ½å‘æ“ä½œç³»ç»Ÿç”³è¯·å”¯ä¸€çš„ç«¯å£å·ï¼Œè¿™æ ·ï¼Œä¸¤ä¸ªè¿›ç¨‹åœ¨ä¸¤å°è®¡ç®—æœºä¹‹é—´å»ºç«‹ç½‘ç»œè¿æ¥å°±éœ€è¦å„è‡ªçš„IPåœ°å€å’Œå„è‡ªçš„ç«¯å£å·ã€‚(åœ¨è¿›è¡Œtomcatè®¾ç½®çš„æ—¶å€™,ç±»ä¼¼ä¹Ÿæ˜¯éœ€è¦è®¾ç½®ç«¯å£å·.)</p>

<p>ä¸€ä¸ªè¿›ç¨‹ä¹Ÿå¯èƒ½åŒæ—¶ä¸å¤šä¸ªè®¡ç®—æœºå»ºç«‹é“¾æ¥ï¼Œå› æ­¤å®ƒä¼šç”³è¯·å¾ˆå¤šç«¯å£ã€‚</p>

<h2 id="toc_1">TCPç¼–ç¨‹</h2>

<p>Socketæ˜¯ç½‘ç»œç¼–ç¨‹çš„ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µã€‚é€šå¸¸æˆ‘ä»¬ç”¨ä¸€ä¸ªSocketè¡¨ç¤ºâ€œæ‰“å¼€äº†ä¸€ä¸ªç½‘ç»œé“¾æ¥â€ï¼Œè€Œæ‰“å¼€ä¸€ä¸ªSocketéœ€è¦çŸ¥é“ç›®æ ‡è®¡ç®—æœºçš„IPåœ°å€å’Œç«¯å£å·ï¼Œå†æŒ‡å®šåè®®ç±»å‹å³å¯ã€‚</p>

<h3 id="toc_2">å®¢æˆ·ç«¯</h3>

<p>å¤§å¤šæ•°è¿æ¥éƒ½æ˜¯å¯é çš„TCPè¿æ¥ã€‚åˆ›å»ºTCPè¿æ¥æ—¶ï¼Œä¸»åŠ¨å‘èµ·è¿æ¥çš„å«å®¢æˆ·ç«¯ï¼Œè¢«åŠ¨å“åº”è¿æ¥çš„å«æœåŠ¡å™¨ã€‚</p>

<p>æˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªåŸºäºTCPè¿æ¥çš„Socketï¼Œå¯ä»¥è¿™æ ·åšï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import socket

s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

s = connect((&#39;www.sina.com.cn&#39;,80))
</code></pre>

<p>åˆ›å»ºSocketæ—¶ï¼Œ<code>AF_INET</code>æŒ‡å®šä½¿ç”¨<code>IPv4</code>åè®®ï¼Œå¦‚æœè¦ç”¨æ›´å…ˆè¿›çš„<code>IPv6</code>ï¼Œå°±æŒ‡å®šä¸º<code>AF_INET6</code>ã€‚<code>SOCK_STREAM</code>æŒ‡å®šä½¿ç”¨é¢å‘æµçš„TCPåè®®ï¼Œè¿™æ ·ï¼Œä¸€ä¸ª<code>Socket</code>å¯¹è±¡å°±åˆ›å»ºæˆåŠŸï¼Œä½†æ˜¯è¿˜æ²¡æœ‰å»ºç«‹è¿æ¥ã€‚</p>

<p>å®¢æˆ·ç«¯è¦ä¸»åŠ¨å‘èµ·TCPè¿æ¥ï¼Œå¿…é¡»çŸ¥é“æœåŠ¡å™¨çš„IPåœ°å€å’Œç«¯å£å·ã€‚æ–°æµªç½‘ç«™çš„IPåœ°å€å¯ä»¥ç”¨åŸŸå<code>www.sina.com.cn</code>è‡ªåŠ¨è½¬æ¢åˆ°IPåœ°å€ï¼Œä½†æ˜¯æ€ä¹ˆçŸ¥é“æ–°æµªæœåŠ¡å™¨çš„ç«¯å£å·å‘¢ï¼Ÿ</p>

<p>ç­”æ¡ˆæ˜¯ä½œä¸ºæœåŠ¡å™¨ï¼Œæä¾›ä»€ä¹ˆæ ·çš„æœåŠ¡ï¼Œç«¯å£å·å°±å¿…é¡»å›ºå®šä¸‹æ¥ã€‚ç”±äºæˆ‘ä»¬æƒ³è¦è®¿é—®ç½‘é¡µï¼Œå› æ­¤æ–°æµªæä¾›ç½‘é¡µæœåŠ¡çš„æœåŠ¡å™¨å¿…é¡»æŠŠç«¯å£å·å›ºå®šåœ¨<code>80</code>ç«¯å£ï¼Œå› ä¸º<code>80</code>ç«¯å£æ˜¯WebæœåŠ¡çš„æ ‡å‡†ç«¯å£ã€‚å…¶ä»–æœåŠ¡éƒ½æœ‰å¯¹åº”çš„æ ‡å‡†ç«¯å£å·ï¼Œä¾‹å¦‚SMTPæœåŠ¡æ˜¯25ç«¯å£ï¼ŒFTPæœåŠ¡æ˜¯21ç«¯å£ï¼Œç­‰ç­‰ã€‚ç«¯å£å·å°äº1024çš„æ˜¯Internetæ ‡å‡†æœåŠ¡çš„ç«¯å£ï¼Œç«¯å£å·å¤§äº1024çš„ï¼Œå¯ä»¥ä»»æ„ä½¿ç”¨ã€‚</p>

<p>å› æ­¤ï¼Œæˆ‘ä»¬è¿æ¥æ–°æµªæœåŠ¡å™¨çš„ä»£ç å¦‚ä¸‹ï¼š</p>

<pre><code class="language-py">s.connect((&#39;www.sina.com.cn&#39;, 80))
</code></pre>

<p><strong>æ³¨æ„å‚æ•°æ˜¯ä¸€ä¸ªtupleï¼ŒåŒ…å«åœ°å€å’Œç«¯å£å·ã€‚</strong></p>

<p>å»ºç«‹TCPè¿æ¥åï¼Œæˆ‘ä»¬å°±å¯ä»¥å‘æ–°æµªæœåŠ¡å™¨å‘é€è¯·æ±‚ï¼Œè¦æ±‚è¿”å›é¦–é¡µçš„å†…å®¹ï¼š</p>

<pre><code class="language-py">s.send(&#39;GET / HTTP/1.1\r\nHost: www.sina.com.cn\r\nConnection: close\r\n\r\n&#39;)
</code></pre>

<p>TCPè¿æ¥åˆ›å»ºçš„æ˜¯åŒå‘é€šé“ï¼ŒåŒæ–¹éƒ½å¯ä»¥åŒæ—¶ç»™å¯¹æ–¹å‘æ•°æ®ã€‚ä½†æ˜¯è°å…ˆå‘è°åå‘ï¼Œæ€ä¹ˆåè°ƒï¼Œè¦æ ¹æ®å…·ä½“çš„åè®®æ¥å†³å®šã€‚ä¾‹å¦‚ï¼ŒHTTPåè®®è§„å®šå®¢æˆ·ç«¯å¿…é¡»å…ˆå‘è¯·æ±‚ç»™æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨æ”¶åˆ°åæ‰å‘æ•°æ®ç»™å®¢æˆ·ç«¯ã€‚</p>

<p>å‘é€çš„æ–‡æœ¬æ ¼å¼å¿…é¡»ç¬¦åˆHTTPæ ‡å‡†ï¼Œå¦‚æœæ ¼å¼æ²¡é—®é¢˜ï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥æ¥æ”¶æ–°æµªæœåŠ¡å™¨è¿”å›çš„æ•°æ®äº†ï¼š</p>

<pre><code class="language-py"># æ¥æ”¶æ•°æ®:
buffer = []
while True:
    # æ¯æ¬¡æœ€å¤šæ¥æ”¶1kå­—èŠ‚:
    d = s.recv(1024)
    if d:
        buffer.append(d)
    else:
        break
data = &#39;&#39;.join(buffer)
</code></pre>

<p>æ¥æ”¶æ•°æ®æ—¶ï¼Œè°ƒç”¨<code>recv(max)</code>æ–¹æ³•ï¼Œä¸€æ¬¡æœ€å¤šæ¥æ”¶æŒ‡å®šçš„å­—èŠ‚æ•°ï¼Œå› æ­¤ï¼Œåœ¨ä¸€ä¸ªwhileå¾ªç¯ä¸­åå¤æ¥æ”¶ï¼Œç›´åˆ°<code>recv()</code>è¿”å›ç©ºæ•°æ®ï¼Œè¡¨ç¤ºæ¥æ”¶å®Œæ¯•ï¼Œé€€å‡ºå¾ªç¯ã€‚</p>

<p>å½“æˆ‘ä»¬æ¥æ”¶å®Œæ•°æ®åï¼Œè°ƒç”¨<code>close()</code>æ–¹æ³•å…³é—­<code>Socket</code>ï¼Œè¿™æ ·ï¼Œä¸€æ¬¡å®Œæ•´çš„ç½‘ç»œé€šä¿¡å°±ç»“æŸäº†ï¼š</p>

<pre><code class="language-py"># å…³é—­è¿æ¥:
s.close()
</code></pre>

<p>æ¥æ”¶åˆ°çš„æ•°æ®åŒ…æ‹¬<code>HTTPå¤´</code>å’Œ<code>ç½‘é¡µæœ¬èº«</code>ï¼Œæˆ‘ä»¬åªéœ€è¦æŠŠHTTPå¤´å’Œç½‘é¡µåˆ†ç¦»ä¸€ä¸‹ï¼ŒæŠŠ<code>HTTPå¤´</code>æ‰“å°å‡ºæ¥ï¼Œç½‘é¡µå†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ï¼š</p>

<pre><code class="language-py">header, html = data.split(&#39;\r\n\r\n&#39;, 1)
print header
# æŠŠæ¥æ”¶çš„æ•°æ®å†™å…¥æ–‡ä»¶:
with open(&#39;sina.html&#39;, &#39;wb&#39;) as f:
    f.write(html)
</code></pre>

<p>ç°åœ¨ï¼Œåªéœ€è¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ª<code>sina.html</code>æ–‡ä»¶ï¼Œå°±å¯ä»¥çœ‹åˆ°æ–°æµªçš„é¦–é¡µäº†ã€‚</p>

<h2 id="toc_3">æœåŠ¡å™¨</h2>

<p>å’Œå®¢æˆ·ç«¯ç¼–ç¨‹ç›¸æ¯”ï¼ŒæœåŠ¡å™¨ç¼–ç¨‹å°±è¦å¤æ‚ä¸€äº›ã€‚</p>

<p>æœåŠ¡å™¨è¿›ç¨‹é¦–å…ˆè¦ç»‘å®šä¸€ä¸ªç«¯å£å¹¶ç›‘å¬æ¥è‡ªå…¶ä»–å®¢æˆ·ç«¯çš„è¿æ¥ã€‚å¦‚æœæŸä¸ªå®¢æˆ·ç«¯è¿æ¥è¿‡æ¥äº†ï¼ŒæœåŠ¡å™¨å°±ä¸è¯¥å®¢æˆ·ç«¯å»ºç«‹Socketè¿æ¥ï¼Œéšåçš„é€šä¿¡å°±é è¿™ä¸ªSocketè¿æ¥äº†ã€‚</p>

<p>æ‰€ä»¥ï¼ŒæœåŠ¡å™¨ä¼šæ‰“å¼€å›ºå®šç«¯å£ï¼ˆæ¯”å¦‚80ï¼‰ç›‘å¬ï¼Œæ¯æ¥ä¸€ä¸ªå®¢æˆ·ç«¯è¿æ¥ï¼Œå°±åˆ›å»ºè¯¥Socketè¿æ¥ã€‚ç”±äºæœåŠ¡å™¨ä¼šæœ‰å¤§é‡æ¥è‡ªå®¢æˆ·ç«¯çš„è¿æ¥ï¼Œæ‰€ä»¥ï¼ŒæœåŠ¡å™¨è¦èƒ½å¤ŸåŒºåˆ†ä¸€ä¸ªSocketè¿æ¥æ˜¯å’Œå“ªä¸ªå®¢æˆ·ç«¯ç»‘å®šçš„ã€‚ä¸€ä¸ªSocketä¾èµ–4é¡¹ï¼šæœåŠ¡å™¨åœ°å€ã€æœåŠ¡å™¨ç«¯å£ã€å®¢æˆ·ç«¯åœ°å€ã€å®¢æˆ·ç«¯ç«¯å£æ¥å”¯ä¸€ç¡®å®šä¸€ä¸ªSocketã€‚</p>

<p>ä½†æ˜¯æœåŠ¡å™¨è¿˜éœ€è¦åŒæ—¶å“åº”å¤šä¸ªå®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œæ‰€ä»¥ï¼Œæ¯ä¸ªè¿æ¥éƒ½éœ€è¦ä¸€ä¸ªæ–°çš„è¿›ç¨‹æˆ–è€…æ–°çš„çº¿ç¨‹æ¥å¤„ç†ï¼Œå¦åˆ™ï¼ŒæœåŠ¡å™¨ä¸€æ¬¡å°±åªèƒ½æœåŠ¡ä¸€ä¸ªå®¢æˆ·ç«¯äº†ã€‚</p>

<p>æˆ‘ä»¬æ¥ç¼–å†™ä¸€ä¸ªç®€å•çš„æœåŠ¡å™¨ç¨‹åºï¼Œå®ƒæ¥æ”¶å®¢æˆ·ç«¯è¿æ¥ï¼ŒæŠŠå®¢æˆ·ç«¯å‘è¿‡æ¥çš„å­—ç¬¦ä¸²åŠ ä¸Š<code>Hello</code>å†å‘å›å»ã€‚</p>

<p>é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªåŸºäºIPv4å’ŒTCPåè®®çš„Socketï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
</code></pre>

<p>ç„¶åï¼Œæˆ‘ä»¬è¦ç»‘å®šç›‘å¬çš„åœ°å€å’Œç«¯å£ã€‚æœåŠ¡å™¨å¯èƒ½æœ‰å¤šå—ç½‘å¡ï¼Œå¯ä»¥ç»‘å®šåˆ°æŸä¸€å—ç½‘å¡çš„IPåœ°å€ä¸Šï¼Œä¹Ÿå¯ä»¥ç”¨<code>0.0.0.0</code>ç»‘å®šåˆ°æ‰€æœ‰çš„ç½‘ç»œåœ°å€ï¼Œè¿˜å¯ä»¥ç”¨<code>127.0.0.1</code>ç»‘å®šåˆ°æœ¬æœºåœ°å€ã€‚<code>127.0.0.1</code>æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„IPåœ°å€ï¼Œè¡¨ç¤ºæœ¬æœºåœ°å€ï¼Œå¦‚æœç»‘å®šåˆ°è¿™ä¸ªåœ°å€ï¼Œå®¢æˆ·ç«¯å¿…é¡»åŒæ—¶åœ¨æœ¬æœºè¿è¡Œæ‰èƒ½è¿æ¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¤–éƒ¨çš„è®¡ç®—æœºæ— æ³•è¿æ¥è¿›æ¥ã€‚</p>

<p>ç«¯å£å·éœ€è¦é¢„å…ˆæŒ‡å®šã€‚å› ä¸ºæˆ‘ä»¬å†™çš„è¿™ä¸ªæœåŠ¡ä¸æ˜¯æ ‡å‡†æœåŠ¡ï¼Œæ‰€ä»¥ç”¨<code>9999</code>è¿™ä¸ªç«¯å£å·ã€‚è¯·æ³¨æ„ï¼Œå°äº<code>1024</code>çš„ç«¯å£å·å¿…é¡»è¦æœ‰ç®¡ç†å‘˜æƒé™æ‰èƒ½ç»‘å®šï¼š</p>

<pre><code class="language-py"># ç›‘å¬ç«¯å£:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>ç´§æ¥ç€ï¼Œè°ƒç”¨listen()æ–¹æ³•å¼€å§‹ç›‘å¬ç«¯å£ï¼Œä¼ å…¥çš„å‚æ•°æŒ‡å®šç­‰å¾…è¿æ¥çš„æœ€å¤§æ•°é‡ï¼š<br/>
<code>py<br/>
s.listen(5)<br/>
print &#39;Waiting for connection...&#39;<br/>
</code></p>

<p>æ¥ä¸‹æ¥ï¼ŒæœåŠ¡å™¨ç¨‹åºé€šè¿‡ä¸€ä¸ªæ°¸ä¹…å¾ªç¯æ¥æ¥å—æ¥è‡ªå®¢æˆ·ç«¯çš„è¿æ¥ï¼Œ<code>accept()</code>ä¼šç­‰å¾…å¹¶è¿”å›ä¸€ä¸ªå®¢æˆ·ç«¯çš„è¿æ¥:</p>

<pre><code class="language-py">while True:
    # æ¥å—ä¸€ä¸ªæ–°è¿æ¥:
    sock, addr = s.accept()
    # åˆ›å»ºæ–°çº¿ç¨‹æ¥å¤„ç†TCPè¿æ¥:
    t = threading.Thread(target=tcplink, args=(sock, addr))
    t.start()
</code></pre>

<p>æ¯ä¸ªè¿æ¥éƒ½å¿…é¡»åˆ›å»ºæ–°çº¿ç¨‹ï¼ˆæˆ–è¿›ç¨‹ï¼‰æ¥å¤„ç†ï¼Œå¦åˆ™ï¼Œå•çº¿ç¨‹åœ¨å¤„ç†è¿æ¥çš„è¿‡ç¨‹ä¸­ï¼Œæ— æ³•æ¥å—å…¶ä»–å®¢æˆ·ç«¯çš„è¿æ¥ï¼š</p>

<pre><code class="language-py">def tcplink(sock, addr):
    print &#39;Accept new connection from %s:%s...&#39; % addr
    sock.send(&#39;Welcome!&#39;)
    while True:
        data = sock.recv(1024)
        time.sleep(1)
        if data == &#39;exit&#39; or not data:
            break
        sock.send(&#39;Hello, %s!&#39; % data)
    sock.close()
    print &#39;Connection from %s:%s closed.&#39; % addr
</code></pre>

<p>è¿æ¥å»ºç«‹åï¼ŒæœåŠ¡å™¨é¦–å…ˆå‘ä¸€æ¡æ¬¢è¿æ¶ˆæ¯ï¼Œç„¶åç­‰å¾…å®¢æˆ·ç«¯æ•°æ®ï¼Œå¹¶åŠ ä¸Š<code>Hello</code>å†å‘é€ç»™å®¢æˆ·ç«¯ã€‚å¦‚æœå®¢æˆ·ç«¯å‘é€äº†<code>exit</code>å­—ç¬¦ä¸²ï¼Œå°±ç›´æ¥å…³é—­è¿æ¥ã€‚</p>

<p>è¦æµ‹è¯•è¿™ä¸ªæœåŠ¡å™¨ç¨‹åºï¼Œæˆ‘ä»¬è¿˜éœ€è¦ç¼–å†™ä¸€ä¸ªå®¢æˆ·ç«¯ç¨‹åºï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# å»ºç«‹è¿æ¥:
s.connect((&#39;127.0.0.1&#39;, 9999))
# æ¥æ”¶æ¬¢è¿æ¶ˆæ¯:
print s.recv(1024)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # å‘é€æ•°æ®:
    s.send(data)
    print s.recv(1024)
s.send(&#39;exit&#39;)
s.close()
</code></pre>

<p>æˆ‘ä»¬éœ€è¦æ‰“å¼€ä¸¤ä¸ªå‘½ä»¤è¡Œçª—å£ï¼Œä¸€ä¸ªè¿è¡ŒæœåŠ¡å™¨ç¨‹åºï¼Œå¦ä¸€ä¸ªè¿è¡Œå®¢æˆ·ç«¯ç¨‹åºï¼Œå°±å¯ä»¥çœ‹åˆ°æ•ˆæœäº†ï¼š<br/>
(å…ˆè¿è¡ŒæœåŠ¡å™¨ç«¯ç¨‹åº)<br/>
æ‰§è¡Œäº†ä¸¤æ¬¡å®¢æˆ·ç«¯ç¨‹åº:<br/>
<img src="media/14957594179111/14957645115702.jpg" alt=""/><br/>
æœåŠ¡ç«¯:<br/>
<img src="media/14957594179111/14957645546966.jpg" alt=""/><br/>
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®¢æˆ·ç«¯ç¨‹åºè¿è¡Œå®Œæ¯•å°±é€€å‡ºäº†ï¼Œè€ŒæœåŠ¡å™¨ç¨‹åºä¼šæ°¸è¿œè¿è¡Œä¸‹å»ï¼Œå¿…é¡»æŒ‰Ctrl+Cé€€å‡ºç¨‹åºã€‚</p>

<h2 id="toc_4">å°ç»“</h2>

<p>ç”¨TCPåè®®è¿›è¡ŒSocketç¼–ç¨‹åœ¨Pythonä¸­ååˆ†ç®€å•ï¼Œå¯¹äºå®¢æˆ·ç«¯ï¼Œè¦ä¸»åŠ¨è¿æ¥æœåŠ¡å™¨çš„IPå’ŒæŒ‡å®šç«¯å£ï¼Œå¯¹äºæœåŠ¡å™¨ï¼Œè¦é¦–å…ˆç›‘å¬æŒ‡å®šç«¯å£ï¼Œç„¶åï¼Œå¯¹æ¯ä¸€ä¸ªæ–°çš„è¿æ¥ï¼Œåˆ›å»ºä¸€ä¸ªçº¿ç¨‹æˆ–è¿›ç¨‹æ¥å¤„ç†ã€‚é€šå¸¸ï¼ŒæœåŠ¡å™¨ç¨‹åºä¼šæ— é™è¿è¡Œä¸‹å»ã€‚</p>

<p>åŒä¸€ä¸ªç«¯å£ï¼Œè¢«ä¸€ä¸ªSocketç»‘å®šäº†ä»¥åï¼Œå°±ä¸èƒ½è¢«åˆ«çš„Socketç»‘å®šäº†ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[å›¾å½¢ç•Œé¢]]></title>
    <link href="https://lockxmonk.github.io/14957005904776.html"/>
    <updated>2017-05-25T16:23:10+08:00</updated>
    <id>https://lockxmonk.github.io/14957005904776.html</id>
    <content type="html"><![CDATA[
<p>Pythonæ”¯æŒå¤šç§å›¾å½¢ç•Œé¢çš„ç¬¬ä¸‰æ–¹åº“ï¼ŒåŒ…æ‹¬ï¼š<br/>
* Tk<br/>
* wxWidgets<br/>
* Qt<br/>
* GTK<br/>
ç­‰ç­‰.</p>

<p>ä½†æ˜¯Pythonè‡ªå¸¦çš„åº“æ˜¯æ”¯æŒTkçš„Tkinterï¼Œä½¿ç”¨Tkinterï¼Œæ— éœ€å®‰è£…ä»»ä½•åŒ…ï¼Œå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚æœ¬ç« ç®€å•ä»‹ç»å¦‚ä½•ä½¿ç”¨Tkinterè¿›è¡ŒGUIç¼–ç¨‹ã€‚</p>

<h2 id="toc_0">Tkinter</h2>

<p>æˆ‘ä»¬æ¥æ¢³ç†ä¸€ä¸‹æ¦‚å¿µï¼š</p>

<p>æˆ‘ä»¬ç¼–å†™çš„Pythonä»£ç ä¼šè°ƒç”¨å†…ç½®çš„Tkinterï¼ŒTkinterå°è£…äº†è®¿é—®Tkçš„æ¥å£ï¼›</p>

<p>Tkæ˜¯ä¸€ä¸ªå›¾å½¢åº“ï¼Œæ”¯æŒå¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œä½¿ç”¨Tclè¯­è¨€å¼€å‘ï¼›</p>

<p>Tkä¼šè°ƒç”¨æ“ä½œç³»ç»Ÿæä¾›çš„æœ¬åœ°GUIæ¥å£ï¼Œå®Œæˆæœ€ç»ˆçš„GUIã€‚</p>

<p>æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„ä»£ç åªéœ€è¦è°ƒç”¨Tkinteræä¾›çš„æ¥å£å°±å¯ä»¥äº†ã€‚</p>

<h2 id="toc_1">ç¬¬ä¸€ä¸ªGUIç¨‹åº</h2>

<p>ä½¿ç”¨Tkinterååˆ†ç®€å•ï¼Œæˆ‘ä»¬æ¥ç¼–å†™ä¸€ä¸ªGUIç‰ˆæœ¬çš„â€œHello, world!â€ã€‚</p>

<p>ç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥TkinteråŒ…çš„æ‰€æœ‰å†…å®¹ï¼š</p>

<pre><code class="language-py">from Tkinter import *
</code></pre>

<p>ç¬¬äºŒæ­¥æ˜¯ä»<code>Frame</code>æ´¾ç”Ÿä¸€ä¸ª<code>Application</code>ç±»ï¼Œè¿™æ˜¯æ‰€æœ‰Widgetçš„çˆ¶å®¹å™¨ï¼š</p>

<pre><code class="language-py">
class Applicition(Frame):
    def __init__(self, master=None):
        Frame.__init__(self,master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.helloLabel = Label(self,text = &#39;Hello,world&#39;)
        self.helloLabel.pack()
        self.quitButton = Button(self, text = &#39;Quit&#39;,command = self.quit)
        self.quitButton.pack()

</code></pre>

<p>åœ¨GUIä¸­ï¼Œæ¯ä¸ªButtonã€Labelã€è¾“å…¥æ¡†ç­‰ï¼Œéƒ½æ˜¯ä¸€ä¸ªWidgetã€‚Frameåˆ™æ˜¯å¯ä»¥å®¹çº³å…¶ä»–Widgetçš„Widgetï¼Œæ‰€æœ‰çš„Widgetç»„åˆèµ·æ¥å°±æ˜¯ä¸€æ£µæ ‘ã€‚</p>

<p><code>pack()</code>æ–¹æ³•æŠŠWidgetåŠ å…¥åˆ°çˆ¶å®¹å™¨ä¸­ï¼Œå¹¶å®ç°å¸ƒå±€ã€‚<code>pack()</code>æ˜¯æœ€ç®€å•çš„å¸ƒå±€ï¼Œ<code>grid()</code>å¯ä»¥å®ç°æ›´å¤æ‚çš„å¸ƒå±€ã€‚</p>

<p>åœ¨<code>createWidgets()</code>æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª<code>Label</code>å’Œä¸€ä¸ª<code>Button</code>ï¼Œå½“Buttonè¢«ç‚¹å‡»æ—¶ï¼Œè§¦å‘<code>self.quit()</code>ä½¿ç¨‹åºé€€å‡ºã€‚</p>

<p>ç¬¬ä¸‰æ­¥ï¼Œå®ä¾‹åŒ–<code>Application</code>ï¼Œå¹¶å¯åŠ¨æ¶ˆæ¯å¾ªç¯ï¼š</p>

<pre><code class="language-py">app = Application()
# è®¾ç½®çª—å£æ ‡é¢˜:
app.master.title(&#39;Hello World&#39;)
# ä¸»æ¶ˆæ¯å¾ªç¯:
app.mainloop()
</code></pre>

<p>GUIç¨‹åºçš„ä¸»çº¿ç¨‹è´Ÿè´£ç›‘å¬æ¥è‡ªæ“ä½œç³»ç»Ÿçš„æ¶ˆæ¯ï¼Œå¹¶ä¾æ¬¡å¤„ç†æ¯ä¸€æ¡æ¶ˆæ¯ã€‚å› æ­¤ï¼Œå¦‚æœæ¶ˆæ¯å¤„ç†éå¸¸è€—æ—¶ï¼Œå°±éœ€è¦åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†ã€‚</p>

<p>è¿è¡Œè¿™ä¸ªGUIç¨‹åºï¼Œå¯ä»¥çœ‹åˆ°ä¸‹é¢çš„çª—å£ï¼š<br/>
<img src="media/14957005904776/14957017137688.jpg" alt=""/><br/>
ç‚¹å‡»â€œQuitâ€æŒ‰é’®æˆ–è€…çª—å£çš„â€œxâ€ç»“æŸç¨‹åºã€‚</p>

<h2 id="toc_2">è¾“å…¥æ–‡æœ¬</h2>

<p>æˆ‘ä»¬å†å¯¹è¿™ä¸ªGUIç¨‹åºæ”¹è¿›ä¸€ä¸‹ï¼ŒåŠ å…¥ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œè®©ç”¨æˆ·å¯ä»¥è¾“å…¥æ–‡æœ¬ï¼Œç„¶åç‚¹æŒ‰é’®åï¼Œå¼¹å‡ºæ¶ˆæ¯å¯¹è¯æ¡†ã€‚</p>

<pre><code class="language-py">from Tkinter import *
import tkMessageBox


class Applicition(Frame):

    def __init__(self, master=None):
        Frame.__init__(self, master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.nameInput = Entry(self)
        self.nameInput.pack()
        self.alertButton = Button(self, text=&#39;Hello&#39;, command=self.hello)
        self.alertButton.pack()

    def hello(self):
        name = self.nameInput.get() or &#39;world&#39;
        tkMessageBox.showinfo(&#39;Message&#39;, &#39;Hello,%s&#39; % name)

app = Applicition()
app.master.title(&#39;Hello World&#39;)
app.mainloop()

</code></pre>

<p><img src="media/14957005904776/14957022189285.jpg" alt=""/></p>

<p>å½“ç”¨æˆ·ç‚¹å‡»æŒ‰é’®æ—¶ï¼Œè§¦å‘<code>hello()</code>ï¼Œé€šè¿‡<code>self.nameInput.get()</code>è·å¾—ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬åï¼Œä½¿ç”¨<code>tkMessageBox.showinfo()</code>å¯ä»¥å¼¹å‡ºæ¶ˆæ¯å¯¹è¯æ¡†ã€‚</p>

<h2 id="toc_3">å°ç»“</h2>

<p>Pythonå†…ç½®çš„Tkinterå¯ä»¥æ»¡è¶³åŸºæœ¬çš„GUIç¨‹åºçš„è¦æ±‚ï¼Œå¦‚æœæ˜¯éå¸¸å¤æ‚çš„GUIç¨‹åºï¼Œå»ºè®®ç”¨æ“ä½œç³»ç»ŸåŸç”Ÿæ”¯æŒçš„è¯­è¨€å’Œåº“æ¥ç¼–å†™ã€‚</p>

<p>æºç å‚è€ƒï¼š<a href="https://github.com/michaelliao/learn-python/tree/master/gui">https://github.com/michaelliao/learn-python/tree/master/gui</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HTMLParser]]></title>
    <link href="https://lockxmonk.github.io/14956930097908.html"/>
    <updated>2017-05-25T14:16:49+08:00</updated>
    <id>https://lockxmonk.github.io/14956930097908.html</id>
    <content type="html"><![CDATA[
<p>åœ¨Pythonzä¸­æˆ‘ä»¬æœ‰å¯èƒ½éœ€è¦å»è§£æä¸€ä¸ªçˆ¬ä¸‹æ¥çš„HTML,æˆ‘ä»¬åœ¨Pythonä¸­åº”è¯¥å¦‚ä½•å»è§£æå‘¢?</p>

<p>å¥½åœ¨Pythonæä¾›äº†<code>HTMLParser</code>æ¥éå¸¸æ–¹ä¾¿åœ°è§£æHTMLï¼Œåªéœ€ç®€å•å‡ è¡Œä»£ç ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from HTMLParser import HTMLParser
from htmlentitydefs import name2codepoint

class MyHTMLParser(HTMLParser):
    def handle_starttag(self,tag,attrs):
        print (&#39;&lt;%s&gt;&#39; % tag)

    def handle_endtag(self,tag):
        print (&#39;&lt;/%s&gt;&#39; % tag)
    
    def handle_startendtag(self, tag, attrs):
        print(&#39;&lt;%s/&gt;&#39; % tag)

    def handle_data(self, data):
        print(&#39;data&#39;)

    def handle_comment(self, data):
        print(&#39;&lt;!-- --&gt;&#39;)

    def handle_entityref(self, name):
        print(&#39;&amp;%s;&#39; % name)

    def handle_charref(self, name):
        print(&#39;&amp;#%s;&#39; % name)
parser = MyHTMLParser()
parser.feed(&#39;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;Some &lt;a href=\&quot;#\&quot;&gt;html&lt;/a&gt; tutorial...&lt;br&gt;END&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#39;)

</code></pre>

<p><code>feed()</code>æ–¹æ³•å¯ä»¥å¤šæ¬¡è°ƒç”¨ï¼Œä¹Ÿå°±æ˜¯ä¸ä¸€å®šä¸€æ¬¡æŠŠæ•´ä¸ªHTMLå­—ç¬¦ä¸²éƒ½å¡è¿›å»ï¼Œå¯ä»¥ä¸€éƒ¨åˆ†ä¸€éƒ¨åˆ†å¡è¿›å»ã€‚</p>

<p>ç‰¹æ®Šå­—ç¬¦æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯è‹±æ–‡è¡¨ç¤ºçš„<code>&amp;nbsp;</code>ï¼Œä¸€ç§æ˜¯æ•°å­—è¡¨ç¤ºçš„<code>&amp;#1234;</code>ï¼Œè¿™ä¸¤ç§å­—ç¬¦éƒ½å¯ä»¥é€šè¿‡Parserè§£æå‡ºæ¥ã€‚</p>

<h2 id="toc_0">ç»ƒä¹ </h2>

<p>æ‰¾ä¸€ä¸ªç½‘é¡µï¼Œä¾‹å¦‚<a href="https://www.python.org/events/python-events/">https://www.python.org/events/python-events/</a>ï¼Œç”¨æµè§ˆå™¨æŸ¥çœ‹æºç å¹¶å¤åˆ¶ï¼Œç„¶åå°è¯•è§£æä¸€ä¸‹HTMLï¼Œè¾“å‡ºPythonå®˜ç½‘å‘å¸ƒçš„ä¼šè®®æ—¶é—´ã€åç§°å’Œåœ°ç‚¹ã€‚</p>

<p>è¿™é‡Œæˆ‘ä»¬è¦è§£æHTMLä¹‹å‰,è‚¯å®šè¦å…ˆè·å–è¯¥é¡µé¢å…ƒç´ çš„ä»£ç .æˆ‘ä»¬è¿™é‡Œç”¨åˆ°äº†<code>urllib</code>è¿™ä¸ªåº“,å…·ä½“ç”¨æ³•ä¸º:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import urllib

PythonPage = urllib.urlopen(&#39;https://www.python.org/events/python-events/&#39;)
pyhtml = PythonPage.read()  #è¯»å–è¯¥é¡µé¢ä»£ç .
print pyhtml
</code></pre>

<p><img src="media/14956930097908/14956951701372.jpg" alt=""/><br/>
ä¸Šé¢ä¸ºç»“æœ,è¿™é‡Œåªæ˜¯éƒ¨åˆ†æˆªå›¾.</p>

<p>ä¸‹é¢æˆ‘ä»¬æ¥ç»§ç»­ç ”ç©¶ä¸Šé¢çš„é—®é¢˜:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from HTMLParser import HTMLParser
from htmlentitydefs import name2codepoint
import urllib


class PyHTMLParser(HTMLParser):

    def __init__(self):
        HTMLParser.__init__(self)
        self._count = 0
        self._events = dict()
        self._flag = None

    def handle_starttag(self, tag, attrs):
        if tag == &#39;h3&#39; and attrs.__contains__((&#39;class&#39;, &#39;event-title&#39;)):
            self._count += 1
            self._events[self._count] = dict()
            self._flag = &#39;event-title&#39;
        if tag == &#39;time&#39;:
            self._flag = &#39;time&#39;
        if tag == &#39;span&#39; and attrs.__contains__((&#39;class&#39;, &#39;event-location&#39;)):
            self._flag = &#39;event-location&#39;

    def handle_data(self, data):
        if self._flag == &#39;event-title&#39;:
            self._events[self._count][self._flag] = data
        if self._flag == &#39;time&#39;:
            self._events[self._count][self._flag] = data
        if self._flag == &#39;event-location&#39;:
            self._events[self._count][self._flag] = data
        self._flag = None   #ä¸€å®šè¦è®¾ç½®ä¸ºNone,é˜²æ­¢å…¶å®ƒdataè¯¯å…¥

    def event_list(self):
        print self._events
        print &#39;è¿‘æœŸå…³äºPythonçš„ä¼šè®®æœ‰ï¼š&#39;, self._count, &#39;ä¸ªï¼Œå…·ä½“å¦‚ä¸‹ï¼š&#39;
        for event in self._events.values():
            print event[&#39;event-title&#39;], &#39;\t&#39;, event[&#39;time&#39;], &#39;\t&#39;, event[&#39;event-location&#39;]

PythonPage = urllib.urlopen(&#39;https://www.python.org/events/python-events/&#39;)
pyhtml = PythonPage.read()
parser = PyHTMLParser()
parser.feed(pyhtml)
parser.event_list()
</code></pre>

<p>è¿™é‡Œæˆ‘ä»¬å°†æ‰€é‡åˆ°çš„å±æ€§,è¿›è¡Œäººä¸ºåˆ†ç±»,å°†åŒ…å«<code>&#39;event-title&#39;</code>,<code>&#39;time&#39;</code>,<code>&#39;event-location&#39;</code>å…³é”®å­—çš„å±æ€§èšç±»åˆ°ä¸€èµ·,</p>

<p><img src="media/14956930097908/14956975668058.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[itertools]]></title>
    <link href="https://lockxmonk.github.io/14956760206333.html"/>
    <updated>2017-05-25T09:33:40+08:00</updated>
    <id>https://lockxmonk.github.io/14956760206333.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">chain()</a>
</li>
<li>
<a href="#toc_1">groupby()</a>
</li>
<li>
<a href="#toc_2">imap()</a>
</li>
<li>
<a href="#toc_3">ifilter()</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">å°ç»“</a>


<p>Pythonçš„å†…å»ºæ¨¡å—<code>itertools</code>æä¾›äº†éå¸¸æœ‰ç”¨çš„ç”¨äºæ“ä½œè¿­ä»£å¯¹è±¡çš„å‡½æ•°ã€‚</p>

<p>é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹çœ‹<code>itertools</code>æä¾›çš„å‡ ä¸ªâ€œæ— é™â€è¿­ä»£å™¨ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

natuals = itertools.count(1)
for n in natuals:
    print n
</code></pre>

<p><img src="media/14956760206333/14956780529492.jpg" alt=""/></p>

<p>å› ä¸º<code>count()</code>ä¼šåˆ›å»ºä¸€ä¸ªæ— é™çš„è¿­ä»£å™¨ï¼Œæ‰€ä»¥ä¸Šè¿°ä»£ç ä¼šæ‰“å°å‡ºè‡ªç„¶æ•°åºåˆ—ï¼Œæ ¹æœ¬åœä¸ä¸‹æ¥ï¼Œåªèƒ½æŒ‰<code>Ctrl+C</code>é€€å‡ºã€‚(æœ‰å‡æ­»çš„é£é™©)</p>

<p><code>cycle()</code>ä¼šæŠŠä¼ å…¥çš„ä¸€ä¸ªåºåˆ—æ— é™é‡å¤ä¸‹å»ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

cs = itertools.cycle(&#39;ABC&#39;)
for n in cs:
    print n
</code></pre>

<p><img src="media/14956760206333/14956783779668.jpg" alt=""/><br/>
åŒæ ·åœä¸ä¸‹æ¥ã€‚(å¾ˆå®¹æ˜“ç¨‹åºå‡æ­»....)</p>

<p><code>repeat()</code>è´Ÿè´£æŠŠä¸€ä¸ªå…ƒç´ æ— é™é‡å¤ä¸‹å»ï¼Œä¸è¿‡å¦‚æœæä¾›ç¬¬äºŒä¸ªå‚æ•°å°±å¯ä»¥é™å®šé‡å¤æ¬¡æ•°ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

ns = itertools.repeat(&#39;A&#39;,10)
for n in ns:
    print n
</code></pre>

<p><img src="media/14956760206333/14956789222152.jpg" alt=""/></p>

<p><strong>æ— é™åºåˆ—åªæœ‰åœ¨<code>for</code>è¿­ä»£æ—¶æ‰ä¼šæ— é™åœ°è¿­ä»£ä¸‹å»ï¼Œå¦‚æœåªæ˜¯åˆ›å»ºäº†ä¸€ä¸ªè¿­ä»£å¯¹è±¡ï¼Œå®ƒä¸ä¼šäº‹å…ˆæŠŠæ— é™ä¸ªå…ƒç´ ç”Ÿæˆå‡ºæ¥ï¼Œäº‹å®ä¸Šä¹Ÿä¸å¯èƒ½åœ¨å†…å­˜ä¸­åˆ›å»ºæ— é™å¤šä¸ªå…ƒç´ ã€‚</strong></p>

<p>æ— é™åºåˆ—è™½ç„¶å¯ä»¥æ— é™è¿­ä»£ä¸‹å»ï¼Œä½†æ˜¯é€šå¸¸æˆ‘ä»¬ä¼šé€šè¿‡<code>takewhile()</code>ç­‰å‡½æ•°æ ¹æ®æ¡ä»¶åˆ¤æ–­æ¥æˆªå–å‡ºä¸€ä¸ªæœ‰é™çš„åºåˆ—ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
natuals = itertools.count(1)
ns = itertools.takewhile(lambda x:x&lt;=10, natuals)
for n in ns:
    print n
</code></pre>

<p><img src="media/14956760206333/14956791893037.jpg" alt=""/></p>

<p><code>itertools</code>æä¾›çš„å‡ ä¸ªè¿­ä»£å™¨æ“ä½œå‡½æ•°æ›´åŠ æœ‰ç”¨ï¼š</p>

<h2 id="toc_0">chain()</h2>

<p><code>chain()</code>å¯ä»¥æŠŠä¸€ç»„è¿­ä»£å¯¹è±¡ä¸²è”èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªæ›´å¤§çš„è¿­ä»£å™¨ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for c in itertools.chain(&#39;ABC&#39;, &#39;XYZ&#39;):
    print c
</code></pre>

<p><img src="media/14956760206333/14956795463378.jpg" alt=""/></p>

<h2 id="toc_1">groupby()</h2>

<p><code>groupby()</code>æŠŠè¿­ä»£å™¨ä¸­ç›¸é‚»çš„é‡å¤å…ƒç´ æŒ‘å‡ºæ¥æ”¾åœ¨ä¸€èµ·ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for key,group  in itertools.groupby(&#39;AAAABBBBBCCCAAA&#39;):
    print key ,list(group)
</code></pre>

<p><img src="media/14956760206333/14956799374968.jpg" alt=""/><br/>
<strong>è¿™é‡Œæ³¨æ„åˆ°æ‰“å°groupçš„æ—¶å€™ç”¨çš„æ˜¯list(group),è¿™æ˜¯å› ä¸ºgruupbyè¿”å›ä»ç„¶æ˜¯ä¸€ä¸ªè¿­ä»£å™¨!!,è¿­ä»£å™¨ä¸­çš„å…ƒç´ å¯ä»¥ç”¨list(è¿­ä»£å™¨)æ¥å°†å…ƒç´ æ˜¾ç¤ºå‡ºæ¥,ä½†æ˜¯è¦æ³¨æ„ä¸è¦ç”¨listæ¥æ˜¾ç¤ºé‚£äº›æ— é™å¾ªç¯çš„è¿­ä»£å™¨(ä¼šæ­»æœº....).</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
cs = itertools.repeat(&#39;A&#39;, 10) # æ³¨æ„å­—ç¬¦ä¸²ä¹Ÿæ˜¯åºåˆ—çš„ä¸€ç§
print list(cs)
</code></pre>

<p><img src="media/14956760206333/14956833066876.jpg" alt=""/></p>

<h2 id="toc_2">imap()</h2>

<p><code>imap()</code>å’Œ<code>map()</code>çš„åŒºåˆ«åœ¨äºï¼Œ<code>imap()</code>å¯ä»¥ä½œç”¨äºæ— ç©·åºåˆ—ï¼Œå¹¶ä¸”ï¼Œå¦‚æœä¸¤ä¸ªåºåˆ—çš„é•¿åº¦ä¸ä¸€è‡´ï¼Œä»¥çŸ­çš„é‚£ä¸ªä¸ºå‡†ã€‚</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)):
         print x
</code></pre>

<p><img src="media/14956760206333/14956801958879.jpg" alt=""/></p>

<p>æ³¨æ„<code>imap()</code>è¿”å›ä¸€ä¸ªè¿­ä»£å¯¹è±¡ï¼Œè€Œ<code>map()</code>è¿”å›<code>list</code>ã€‚å½“ä½ è°ƒç”¨<code>map()</code>æ—¶ï¼Œå·²ç»è®¡ç®—å®Œæ¯•,å½“ä½ è°ƒç”¨<code>imap()</code>æ—¶ï¼Œå¹¶æ²¡æœ‰è¿›è¡Œä»»ä½•è®¡ç®—ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = map(lambda x: x*x, [1, 2, 3])
print r     # rå·²ç»è®¡ç®—å‡ºæ¥äº†
n = itertools.imap(lambda x: x*x, [1, 2, 3])
print n     # nåªæ˜¯ä¸€ä¸ªè¿­ä»£å¯¹è±¡

for x in n:
    print x
</code></pre>

<p><img src="media/14956760206333/14956804009136.jpg" alt=""/></p>

<p>å¿…é¡»ç”¨forå¾ªç¯å¯¹rè¿›è¡Œè¿­ä»£ï¼Œæ‰ä¼šåœ¨æ¯æ¬¡å¾ªç¯è¿‡ç¨‹ä¸­è®¡ç®—å‡ºä¸‹ä¸€ä¸ªå…ƒç´ .</p>

<p>è¿™è¯´æ˜<code>imap()</code>å®ç°äº†â€œæƒ°æ€§è®¡ç®—â€ï¼Œä¹Ÿå°±æ˜¯åœ¨éœ€è¦è·å¾—ç»“æœçš„æ—¶å€™æ‰è®¡ç®—ã€‚ç±»ä¼¼<code>imap()</code>è¿™æ ·èƒ½å¤Ÿå®ç°æƒ°æ€§è®¡ç®—çš„å‡½æ•°å°±å¯ä»¥å¤„ç†æ— é™åºåˆ—ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = itertools.imap(lambda x: x*x, itertools.count(1)) 
for n in itertools.takewhile(lambda x: x&lt;100, r):
      print n
</code></pre>

<p><img src="media/14956760206333/14956809524908.jpg" alt=""/></p>

<p>å¦‚æœæŠŠimap()æ¢æˆmap()å»å¤„ç†æ— é™åºåˆ—:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = map(lambda x: x * x, itertools.count(1))
print r
# for n in itertools.takewhile(lambda x: x&lt;100, r):
# print n
</code></pre>

<p>ä¼šé€ æˆç”µè„‘æ­»æœº,ç”±äº<code>map()</code>è¿”å›çš„æ˜¯ä¸€ä¸ªlistï¼Œæ‰€ä»¥å½“ç”¨å®ƒå»å¤„ç†æ— é™åºåˆ—çš„æ—¶å€™ï¼Œå®ƒä¼šå°è¯•è®¡ç®—å®Œä¹‹åæ‰è¿”å›ï¼Œä½†æ˜¯åºåˆ—æ˜¯æ— é™çš„ï¼Œæ‰€ä»¥å®ƒä¼šä¸€ç›´è®¡ç®—ä¸‹å»ï¼Œè‡´ä½¿å…¶å ç”¨çš„ç³»ç»Ÿçš„å†…å­˜è¶Šæ¥è¶Šé«˜ã€‚(å¾ˆå‘çˆ¹....)</p>

<h2 id="toc_3">ifilter()</h2>

<p>ä¸ç”¨å¤šè¯´äº†ï¼Œ<code>ifilter()</code>å°±æ˜¯<code>filter()</code>çš„æƒ°æ€§å®ç°ã€‚</p>

<h1 id="toc_4">å°ç»“</h1>

<p><code>itertools</code>æ¨¡å—æä¾›çš„å…¨éƒ¨æ˜¯å¤„ç†è¿­ä»£åŠŸèƒ½çš„å‡½æ•°ï¼Œå®ƒä»¬çš„è¿”å›å€¼ä¸æ˜¯<code>list</code>ï¼Œè€Œæ˜¯è¿­ä»£å¯¹è±¡ï¼Œåªæœ‰ç”¨<code>for</code>å¾ªç¯è¿­ä»£çš„æ—¶å€™æ‰çœŸæ­£è®¡ç®—ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hashlib]]></title>
    <link href="https://lockxmonk.github.io/14956154840973.html"/>
    <updated>2017-05-24T16:44:44+08:00</updated>
    <id>https://lockxmonk.github.io/14956154840973.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">æ‘˜è¦ç®—æ³•ç®€ä»‹</a>
</li>
</ul>
</li>
<li>
<a href="#toc_1">==MD5æ˜¯æœ€å¸¸è§çš„æ‘˜è¦ç®—æ³•ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œç”Ÿæˆç»“æœæ˜¯å›ºå®šçš„128 bitå­—èŠ‚ï¼Œé€šå¸¸ç”¨ä¸€ä¸ª32ä½çš„16è¿›åˆ¶å­—ç¬¦ä¸²è¡¨ç¤ºã€‚</a>
<ul>
<li>
<a href="#toc_2">æ‘˜è¦ç®—æ³•åº”ç”¨</a>
<ul>
<li>
<a href="#toc_3">ç»ƒä¹ :æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å£ä»¤ï¼Œè®¡ç®—å‡ºå­˜å‚¨åœ¨æ•°æ®åº“ä¸­çš„MD5å£ä»¤ï¼š</a>
</li>
<li>
<a href="#toc_4">ç»ƒä¹ ï¼šè®¾è®¡ä¸€ä¸ªéªŒè¯ç”¨æˆ·ç™»å½•çš„å‡½æ•°ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å£ä»¤æ˜¯å¦æ­£ç¡®ï¼Œè¿”å›Trueæˆ–Falseï¼š</a>
</li>
<li>
<a href="#toc_5">ç»ƒä¹ ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥çš„ç™»å½•åå’Œå£ä»¤æ¨¡æ‹Ÿç”¨æˆ·æ³¨å†Œï¼Œè®¡ç®—æ›´å®‰å…¨çš„MD5ï¼š</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">å°ç»“</a>
</li>
</ul>


<h2 id="toc_0">æ‘˜è¦ç®—æ³•ç®€ä»‹</h2>

<p>Pythonçš„hashlibæä¾›äº†å¸¸è§çš„æ‘˜è¦ç®—æ³•ï¼Œå¦‚MD5ï¼ŒSHA1ç­‰ç­‰ã€‚</p>

<p>ä»€ä¹ˆæ˜¯æ‘˜è¦ç®—æ³•å‘¢ï¼Ÿæ‘˜è¦ç®—æ³•åˆç§°å“ˆå¸Œç®—æ³•ã€æ•£åˆ—ç®—æ³•ã€‚å®ƒé€šè¿‡ä¸€ä¸ªå‡½æ•°ï¼ŒæŠŠä»»æ„é•¿åº¦çš„æ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªé•¿åº¦å›ºå®šçš„æ•°æ®ä¸²ï¼ˆé€šå¸¸ç”¨16è¿›åˆ¶çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼‰ã€‚</p>

<p>ä¸¾ä¸ªä¾‹å­ï¼Œä½ å†™äº†ä¸€ç¯‡æ–‡ç« ï¼Œå†…å®¹æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²<code>&#39;how to use python hashlib - by Michael&#39;</code>ï¼Œå¹¶é™„ä¸Šè¿™ç¯‡æ–‡ç« çš„æ‘˜è¦æ˜¯<code>&#39;2d73d4f15c0db7f5ecb321b6a65e5d6d&#39;</code>ã€‚å¦‚æœæœ‰äººç¯¡æ”¹äº†ä½ çš„æ–‡ç« ï¼Œå¹¶å‘è¡¨ä¸º<code>&#39;how to use python hashlib - by Bob&#39;</code>ï¼Œä½ å¯ä»¥ä¸€ä¸‹å­æŒ‡å‡ºBobç¯¡æ”¹äº†ä½ çš„æ–‡ç« ï¼Œå› ä¸ºæ ¹æ®<code>&#39;how to use python hashlib - by Bob&#39;</code>è®¡ç®—å‡ºçš„æ‘˜è¦ä¸åŒäºåŸå§‹æ–‡ç« çš„æ‘˜è¦ã€‚</p>

<p>å¯è§ï¼Œæ‘˜è¦ç®—æ³•å°±æ˜¯é€šè¿‡æ‘˜è¦å‡½æ•°<code>f()</code>å¯¹ä»»æ„é•¿åº¦çš„æ•°æ®<code>data</code>è®¡ç®—å‡ºå›ºå®šé•¿åº¦çš„æ‘˜è¦<code>digest</code>ï¼Œç›®çš„æ˜¯ä¸ºäº†å‘ç°åŸå§‹æ•°æ®æ˜¯å¦è¢«äººç¯¡æ”¹è¿‡ã€‚</p>

<p><font color=red><strong>æ‘˜è¦ç®—æ³•ä¹‹æ‰€ä»¥èƒ½æŒ‡å‡ºæ•°æ®æ˜¯å¦è¢«ç¯¡æ”¹è¿‡ï¼Œå°±æ˜¯å› ä¸ºæ‘˜è¦å‡½æ•°æ˜¯ä¸€ä¸ªå•å‘å‡½æ•°ï¼Œè®¡ç®—f(data)å¾ˆå®¹æ˜“ï¼Œä½†é€šè¿‡digeståæ¨dataå´éå¸¸å›°éš¾ã€‚è€Œä¸”ï¼Œå¯¹åŸå§‹æ•°æ®åšä¸€ä¸ªbitçš„ä¿®æ”¹ï¼Œéƒ½ä¼šå¯¼è‡´è®¡ç®—å‡ºçš„æ‘˜è¦å®Œå…¨ä¸åŒã€‚</strong></font></p>

<p>æˆ‘ä»¬ä»¥å¸¸è§çš„æ‘˜è¦ç®—æ³•MD5ä¸ºä¾‹ï¼Œè®¡ç®—å‡ºä¸€ä¸ªå­—ç¬¦ä¸²çš„MD5å€¼ï¼š</p>

<pre><code class="language-py">import hashlib

md5 = hashlib.md5()
md5.update(&#39;how to use md5 in python hashlib?&#39;)

print md5.hexdigest()
</code></pre>

<p>ç»“æœä¸º:<br/>
<img src="media/14956154840973/14956157660396.jpg" alt=""/></p>

<p>å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œå¯ä»¥åˆ†å—å¤šæ¬¡è°ƒç”¨<code>update()</code>ï¼Œæœ€åè®¡ç®—çš„ç»“æœæ˜¯ä¸€æ ·çš„ï¼š</p>

<pre><code class="language-py">import hashlib

md5 = hashlib.md5()
md5.update(&#39;how to use md5 in &#39;)
md5.update(&#39;python hashlib?&#39;)
print md5.hexdigest()

</code></pre>

<p>ç»“æœä»ç„¶ä¸º:<br/>
<img src="media/14956154840973/14956158401991.jpg" alt=""/></p>

<p>å½“æ”¹åŠ¨ä¸€ä¸ªå­—æ¯ä¹‹å,ç»“æœä¼šå®Œå…¨ä¸åŒ.</p>

<h1 id="toc_1">==MD5æ˜¯æœ€å¸¸è§çš„æ‘˜è¦ç®—æ³•ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œç”Ÿæˆç»“æœæ˜¯å›ºå®šçš„128 bitå­—èŠ‚ï¼Œé€šå¸¸ç”¨ä¸€ä¸ª32ä½çš„16è¿›åˆ¶å­—ç¬¦ä¸²è¡¨ç¤ºã€‚</h1>

<p>å¦ä¸€ç§å¸¸è§çš„æ‘˜è¦ç®—æ³•æ˜¯<code>SHA1</code>ï¼Œè°ƒç”¨<code>SHA1</code>å’Œè°ƒç”¨<code>MD5</code>å®Œå…¨ç±»ä¼¼ï¼š</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib

sha1 = hashlib.sha1()
sha1.update(&#39;how to use md5 in &#39;)
sha1.update(&#39;python hashlib?&#39;)
print sha1.hexdigest()
</code></pre>

<p><img src="media/14956154840973/14956161682665.jpg" alt=""/></p>

<p>SHA1çš„ç»“æœæ˜¯160 bitå­—èŠ‚ï¼Œé€šå¸¸ç”¨ä¸€ä¸ª40ä½çš„16è¿›åˆ¶å­—ç¬¦ä¸²è¡¨ç¤ºã€‚</p>

<p>æ¯”SHA1æ›´å®‰å…¨çš„ç®—æ³•æ˜¯SHA256å’ŒSHA512ï¼Œä¸è¿‡è¶Šå®‰å…¨çš„ç®—æ³•è¶Šæ…¢ï¼Œè€Œä¸”æ‘˜è¦é•¿åº¦æ›´é•¿ã€‚</p>

<blockquote>
<p>æœ‰æ²¡æœ‰å¯èƒ½ä¸¤ä¸ªä¸åŒçš„æ•°æ®é€šè¿‡æŸä¸ªæ‘˜è¦ç®—æ³•å¾—åˆ°äº†ç›¸åŒçš„æ‘˜è¦ï¼Ÿå®Œå…¨æœ‰å¯èƒ½ï¼Œå› ä¸ºä»»ä½•æ‘˜è¦ç®—æ³•éƒ½æ˜¯æŠŠæ— é™å¤šçš„æ•°æ®é›†åˆæ˜ å°„åˆ°ä¸€ä¸ªæœ‰é™çš„é›†åˆä¸­ã€‚è¿™ç§æƒ…å†µç§°ä¸ºç¢°æ’ï¼Œæ¯”å¦‚Bobè¯•å›¾æ ¹æ®ä½ çš„æ‘˜è¦åæ¨å‡ºä¸€ç¯‡æ–‡ç« &#39;how to learn hashlib in python - by Bob&#39;ï¼Œå¹¶ä¸”è¿™ç¯‡æ–‡ç« çš„æ‘˜è¦æ°å¥½å’Œä½ çš„æ–‡ç« å®Œå…¨ä¸€è‡´ï¼Œè¿™ç§æƒ…å†µä¹Ÿå¹¶éä¸å¯èƒ½å‡ºç°ï¼Œä½†æ˜¯éå¸¸éå¸¸å›°éš¾ã€‚</p>
</blockquote>

<h2 id="toc_2">æ‘˜è¦ç®—æ³•åº”ç”¨</h2>

<p>ä¸€ä¸ªæ­£ç¡®çš„åº”ç”¨æ˜¯åœ¨å­˜å‚¨çš„æ•°æ®åº“è¡¨ä¸­,å°†ç”¨æˆ·ç™»å½•ç”¨çš„å¯†ç è¿›è¡Œè½¬æ¢,é˜²æ­¢æ•°æ®æ³„éœ²å¯¼è‡´é»‘å®¢æˆ–è€…å†…éƒ¨äººå‘˜è·å–.</p>

<p>æ­£ç¡®çš„ä¿å­˜å£ä»¤çš„æ–¹å¼æ˜¯ä¸å­˜å‚¨ç”¨æˆ·çš„æ˜æ–‡å£ä»¤ï¼Œè€Œæ˜¯å­˜å‚¨ç”¨æˆ·å£ä»¤çš„æ‘˜è¦ï¼Œæ¯”å¦‚MD5ï¼š</p>

<table>
<thead>
<tr>
<th>username</th>
<th>password</th>
</tr>
</thead>

<tbody>
<tr>
<td>michael</td>
<td>e10adc3949ba59abbe56e057f20f883e</td>
</tr>
<tr>
<td>bob</td>
<td>878ef96e86145580c38c87f0410ad153</td>
</tr>
<tr>
<td>alice</td>
<td>99b1c2188db85afee403b1536010c2c9</td>
</tr>
</tbody>
</table>

<p>å½“ç”¨æˆ·ç™»å½•æ—¶ï¼Œé¦–å…ˆè®¡ç®—ç”¨æˆ·è¾“å…¥çš„æ˜æ–‡å£ä»¤çš„MD5ï¼Œç„¶åå’Œæ•°æ®åº“å­˜å‚¨çš„MD5å¯¹æ¯”ï¼Œå¦‚æœä¸€è‡´ï¼Œè¯´æ˜å£ä»¤è¾“å…¥æ­£ç¡®ï¼Œå¦‚æœä¸ä¸€è‡´ï¼Œå£ä»¤è‚¯å®šé”™è¯¯ã€‚</p>

<h3 id="toc_3">ç»ƒä¹ :æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å£ä»¤ï¼Œè®¡ç®—å‡ºå­˜å‚¨åœ¨æ•°æ®åº“ä¸­çš„MD5å£ä»¤ï¼š</h3>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib

def calc_md5(password):
    md5 = hashlib.md5()
    md5.update(password)
    return md5.hexdigest()
password_text = raw_input(&#39;è¾“å…¥å¯†ç :&#39;)
print calc_md5(password_text)
</code></pre>

<p><img src="media/14956154840973/14956172486330.jpg" alt=""/></p>

<p>å­˜å‚¨MD5çš„å¥½å¤„æ˜¯å³ä½¿è¿ç»´äººå‘˜èƒ½è®¿é—®æ•°æ®åº“ï¼Œä¹Ÿæ— æ³•è·çŸ¥ç”¨æˆ·çš„æ˜æ–‡å£ä»¤ã€‚</p>

<h3 id="toc_4">ç»ƒä¹ ï¼šè®¾è®¡ä¸€ä¸ªéªŒè¯ç”¨æˆ·ç™»å½•çš„å‡½æ•°ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å£ä»¤æ˜¯å¦æ­£ç¡®ï¼Œè¿”å›Trueæˆ–Falseï¼š</h3>

<pre><code class="language-py">db = {
    &#39;michael&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;,
    &#39;bob&#39;: &#39;878ef96e86145580c38c87f0410ad153&#39;,
    &#39;alice&#39;: &#39;99b1c2188db85afee403b1536010c2c9&#39;
}

def login(user, password):
    pass
</code></pre>

<p>å®ç°ä»£ç ä¸º:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib
db = {
    &#39;michael&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;,
    &#39;bob&#39;: &#39;878ef96e86145580c38c87f0410ad153&#39;,
    &#39;hao&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;
}

def calc_md5(password):
    md5 = hashlib.md5()
    md5.update(password)
    return md5.hexdigest()
def login(user,password):
    md5 = hashlib.md5()
    md5.update(password)
    if user in db:
        if db[user]==md5.hexdigest():
            print &#39;ç™»é™†æˆåŠŸ&#39;
        else :
            print &#39;å¤±è´¥&#39;
    else:
        print&#39;è¯¥ç”¨æˆ·ä¸å­˜åœ¨&#39;
user_text = raw_input(&#39;è¯·è¾“å…¥ç”¨æˆ·å: &#39;)    
password_text = raw_input(&#39;è¾“å…¥å¯†ç : &#39;)

print calc_md5(password_text)
login(user_text, password_text)
</code></pre>

<p><img src="media/14956154840973/14956740702158.jpg" alt=""/></p>

<p>å¯¹äºç”¨æˆ·æ¥è®²ï¼Œå½“ç„¶ä¸è¦ä½¿ç”¨è¿‡äºç®€å•çš„å£ä»¤ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬èƒ½å¦åœ¨ç¨‹åºè®¾è®¡ä¸Šå¯¹ç®€å•å£ä»¤åŠ å¼ºä¿æŠ¤å‘¢ï¼Ÿ</p>

<p>ç”±äºå¸¸ç”¨å£ä»¤çš„MD5å€¼å¾ˆå®¹æ˜“è¢«è®¡ç®—å‡ºæ¥ï¼Œæ‰€ä»¥ï¼Œè¦ç¡®ä¿å­˜å‚¨çš„ç”¨æˆ·å£ä»¤ä¸æ˜¯é‚£äº›å·²ç»è¢«è®¡ç®—å‡ºæ¥çš„å¸¸ç”¨å£ä»¤çš„MD5ï¼Œè¿™ä¸€æ–¹æ³•é€šè¿‡å¯¹åŸå§‹å£ä»¤åŠ ä¸€ä¸ªå¤æ‚å­—ç¬¦ä¸²æ¥å®ç°ï¼Œä¿—ç§°â€œåŠ ç›â€ï¼š</p>

<pre><code class="language-py">def calc_md5(password):
    return get_md5(password + &#39;the-Salt&#39;)
</code></pre>

<p>ç»è¿‡Saltå¤„ç†çš„MD5å£ä»¤ï¼Œåªè¦Saltä¸è¢«é»‘å®¢çŸ¥é“ï¼Œå³ä½¿ç”¨æˆ·è¾“å…¥ç®€å•å£ä»¤ï¼Œä¹Ÿå¾ˆéš¾é€šè¿‡MD5åæ¨æ˜æ–‡å£ä»¤ã€‚</p>

<p><mark>ä½†æ˜¯å¦‚æœæœ‰ä¸¤ä¸ªç”¨æˆ·éƒ½ä½¿ç”¨äº†ç›¸åŒçš„ç®€å•å£ä»¤æ¯”å¦‚123456ï¼Œåœ¨æ•°æ®åº“ä¸­ï¼Œå°†å­˜å‚¨ä¸¤æ¡ç›¸åŒçš„MD5å€¼ï¼Œè¿™è¯´æ˜è¿™ä¸¤ä¸ªç”¨æˆ·çš„å£ä»¤æ˜¯ä¸€æ ·çš„ã€‚æœ‰æ²¡æœ‰åŠæ³•è®©ä½¿ç”¨ç›¸åŒå£ä»¤çš„ç”¨æˆ·å­˜å‚¨ä¸åŒçš„MD5å‘¢ï¼Ÿ</mark></p>

<p><strong>å¦‚æœå‡å®šç”¨æˆ·æ— æ³•ä¿®æ”¹ç™»å½•åï¼Œå°±å¯ä»¥é€šè¿‡æŠŠç™»å½•åä½œä¸ºSaltçš„ä¸€éƒ¨åˆ†æ¥è®¡ç®—MD5ï¼Œä»è€Œå®ç°ç›¸åŒå£ä»¤çš„ç”¨æˆ·ä¹Ÿå­˜å‚¨ä¸åŒçš„MD5ã€‚</strong></p>

<h3 id="toc_5">ç»ƒä¹ ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥çš„ç™»å½•åå’Œå£ä»¤æ¨¡æ‹Ÿç”¨æˆ·æ³¨å†Œï¼Œè®¡ç®—æ›´å®‰å…¨çš„MD5ï¼š</h3>

<pre><code class="language-py">db = {}

def register(username, password):
    db[username] = get_md5(password + username + &#39;the-Salt&#39;)
</code></pre>

<p>ç„¶åï¼Œæ ¹æ®ä¿®æ”¹åçš„MD5ç®—æ³•å®ç°ç”¨æˆ·ç™»å½•çš„éªŒè¯ï¼š</p>

<pre><code class="language-py">def login(username, password):
    pass
</code></pre>

<p>å®ç°ä»£ç :</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib
db = {}
def register(username,password):
    if username in db:
        print &#39;ç”¨æˆ·å·²å­˜åœ¨&#39;
    else:
        db[username] = get_md5(password + username + &#39;the-Slat&#39;) 
def get_md5(text):
    md5 = hashlib.md5()
    md5.update(text)
    return md5.hexdigest()
def login(user,password):
    md5 = hashlib.md5()
    md5.update(password + user + &#39;the-Slat&#39;)
    if user in db:
        if db[user]==md5.hexdigest():
            print &#39;ç™»é™†æˆåŠŸ&#39;
        else :
            print &#39;å¤±è´¥&#39;
    else:
        print&#39;è¯¥ç”¨æˆ·ä¸å­˜åœ¨æˆ–è€…å¯†ç é”™è¯¯&#39;
register(&#39;hao&#39;, &#39;123456&#39;)
register(&#39;hao&#39;, &#39;12212&#39;)    #é‡å¤æ³¨å†Œä¼šæ˜¾ç¤ºå¤±è´¥
user_text = raw_input(&#39;è¯·è¾“å…¥ç”¨æˆ·å: &#39;)    
password_text = raw_input(&#39;è¾“å…¥å¯†ç : &#39;)

login(user_text, password_text)
</code></pre>

<p><img src="media/14956154840973/14956751273389.jpg" alt=""/></p>

<h2 id="toc_6">å°ç»“</h2>

<p>æ‘˜è¦ç®—æ³•åœ¨å¾ˆå¤šåœ°æ–¹éƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚<strong>è¦æ³¨æ„æ‘˜è¦ç®—æ³•ä¸æ˜¯åŠ å¯†ç®—æ³•ï¼Œä¸èƒ½ç”¨äºåŠ å¯†ï¼ˆå› ä¸ºæ— æ³•é€šè¿‡æ‘˜è¦åæ¨æ˜æ–‡ï¼‰</strong>ï¼Œåªèƒ½ç”¨äºé˜²ç¯¡æ”¹ï¼Œä½†æ˜¯å®ƒçš„å•å‘è®¡ç®—ç‰¹æ€§å†³å®šäº†å¯ä»¥åœ¨ä¸å­˜å‚¨æ˜æ–‡å£ä»¤çš„æƒ…å†µä¸‹éªŒè¯ç”¨æˆ·å£ä»¤ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[åœ¨iosä¸­åˆ¤æ–­ä¸€ä¸ªviewcontrolleræ˜¯éƒ½å·²ç»æ­£å¸¸present]]></title>
    <link href="https://lockxmonk.github.io/14956131169957.html"/>
    <updated>2017-05-24T16:05:16+08:00</updated>
    <id>https://lockxmonk.github.io/14956131169957.html</id>
    <content type="html"><![CDATA[
<p>ä¹‹å‰presentä¸€ä¸ªè§†å›¾çš„æ—¶å€™,ä»æ¥æ²¡æœ‰ç ”ç©¶è¿‡å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªè§†å›¾æ˜¯éƒ½å·²ç»æ­£å¸¸å¼¹å‡º.æ¯”å¦‚ä¸‹é¢è¿™ä¸ªæ–¹æ³•æ˜¯å¦æ­£å¸¸æ‰§è¡Œ:</p>

<pre><code class="language-oc">[documentInteractionController presentPreviewAnimated:YES]
</code></pre>

<p>ç„¶è€Œä»Šå¤©æœ‰ä¸€ä¸ªéœ€æ±‚éœ€è¦çŸ¥é“ä¸Šè¿°æ–¹æ³•æ˜¯å¦æ­£å¸¸æ‰§è¡Œ,å¹¶å¼¹å‡ºè§†å›¾.</p>

<p>ä¹‹åé€šè¿‡é˜…è¯»è¯¥æ–¹æ³•çš„æºç åå‘ç°:</p>

<pre><code class="language-oc">// Bypasses the menu and opens the full screen preview window for the item at URL.  Returns NO if the item could not be previewed.
// Note that you must implement the delegate method documentInteractionControllerViewControllerForPreview: to preview the document.
- (BOOL)presentPreviewAnimated:(BOOL)animated;
</code></pre>

<p>è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªBOOLç±»å‹çš„å€¼:å½“æ²¡æœ‰æ­£å¸¸æ‰§è¡Œè¯¥æ–¹æ³•æ—¶,ä¼šè¿”å›NO.æ­£å¸¸è¿”å›YES.</p>

<p>ç±»ä¼¼çš„è¿˜æœ‰å¦‚ä¸‹æ–¹æ³•:</p>

<pre><code class="language-oc">- (BOOL)isBeingPresented NS_AVAILABLE_IOS(5_0);
- (BOOL)isBeingDismissed NS_AVAILABLE_IOS(5_0);

- (BOOL)isMovingToParentViewController NS_AVAILABLE_IOS(5_0);
- (BOOL)isMovingFromParentViewController NS_AVAILABLE_IOS(5_0);
</code></pre>

<p>æ¥åˆ¤æ–­viewControlleræ˜¯æ¶ˆå¤±è¿˜æ˜¯å‡ºç°åœ¨å½“å‰é¡µé¢ä¸­.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Caffeç›®å½•ç»“æ„]]></title>
    <link href="https://lockxmonk.github.io/14955929300450.html"/>
    <updated>2017-05-24T10:28:50+08:00</updated>
    <id>https://lockxmonk.github.io/14955929300450.html</id>
    <content type="html"><![CDATA[
<p>æˆ‘ä»¬è¿™é‡Œå…ˆäº†è§£ä¸€ä¸‹caffeçš„ç›®å½•ç»“æ„:<br/>
<img src="media/14955929300450/14961936456285.jpg" alt=""/><br/>
<img src="media/14955929300450/14961936868967.jpg" alt=""/><br/>
<img src="media/14955929300450/14961937240927.jpg" alt=""/><br/>
å…¶ä¸­ä¸»è¦å…³æ³¨çš„å°±æ˜¯:include/,src/å’Œtools/,ä¸‰ä¸ªç›®å½•,éœ€è¦åˆ†æçš„ä»£ç éƒ½åŒ…å«åœ¨è¿™é‡Œé¢.</p>

<p>åœ¨é˜…è¯»ä»£ç æ—¶,å¦‚ä½•å¿«é€Ÿè¿½è¸ªæŸä¸ªå…³é”®è¯?ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯æ‰“å¼€æŸä¸ªæ–‡ä»¶,ä¹‹åç”¨æŸ¥æ‰¾å‘½ä»¤æ¥æŸ¥æ‰¾å…³é”®è¯.<br/>
è¿™äº†æˆ‘ä»¬ä»‹ç»å¦ä¸€ç§æ–¹æ³•,åˆ©ç”¨<code>grep</code>å‘½ä»¤:</p>

<pre><code>âœ  caffe git:(master) âœ— grep -n -H -R &quot;REGISTER_LAYER_CREATOR&quot; *
</code></pre>

<p><img src="media/14955929300450/14955931415464.jpg" alt=""/><br/>
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°,æ—¥å¿—è¾“å‡ºäº†å¾ˆå¤šæ–‡ä»¶.è¿™ç§æ–¹æ³•æ— éœ€åˆ†åˆ«æ‰“å¼€æ¯ä¸ªæ–‡ä»¶,ä¹Ÿèƒ½ç›´è§‚çš„æ˜¾ç¤ºäº†æ‰€æœ‰åŒ…å«è¿™ä¸ªå®çš„æ–‡ä»¶åå’Œè¡Œå·.</p>

<p>è¿™é‡Œæˆ‘ä»¬ç”¨<code>grep</code>å‘½ä»¤æ¥æœç´¢ä¸€ä¸ªå®è°ƒç”¨:<code>REGISTER_LAYER_CREATOR</code>,<br/>
å‘½ä»¤è¡Œå‚æ•°è§£é‡Šä¸º:</p>

<blockquote>
<p>-n  ---æ˜¾ç¤ºè¡Œå·,ä¾¿äºå®šä½<br/>
-H  ---æ˜¾ç¤ºæ–‡ä»¶å,ä¾¿ä¸å®šä½<br/>
-R  ---é€’å½’æŸ¥æ‰¾æ¯ä¸ªå­ç›®å½•,é€‚åˆå·¥ç¨‹è¾ƒå¤§,åˆ†å¤šä¸ªç›®å½•å­˜æ”¾çš„åœºæ™¯</p>
</blockquote>

<p>åˆ©ç”¨è¿™ç§æ–¹æ³•å¯ä»¥å¾ˆå®¹æ˜“çš„åœ¨caffeæºç ä¸­å®šä½å¾ˆå¤šå†…å®¹.</p>

<h2 id="toc_0">å·åŸºå±‚</h2>

<p>è¿™é‡Œæˆ‘ä»¬å‡å®šå·ç§¯å±‚æœ‰<code>L</code>ä¸ªè¾“å‡ºé€šé“å’Œ<code>K</code>ä¸ªè¾“å…¥é€šé“,äºæ˜¯éœ€è¦æœ‰<code>LK,L=50,K=20</code>ä¸ªå·ç§¯æ ¸å®ç°é€šé“æ•°ç›®çš„è½¬æ¢.è¿™é‡Œæˆ‘ä»¬å‡å®šå·é›†æ ¸å¤§å°ä¸º<code>I*J = 5*5</code>,æ¯ä¸ªè¾“å‡ºé€šé“çš„ç‰¹å¾å›¾å¤§å°ä¸º<code>M*N = 8*8</code>,åˆ™è¯¥å±‚æ¯ä¸ªæ ·æœ¬åšä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶å·ç§¯å±‚è®¡ç®—é‡ä¸º:<br/>
<code>Calculations(MAC) = I*J*M*N*K*L = 5*5*8*8*50*20=1600000MAC</code><br/>
å®é™…ä¸­ä½¿é€å…¥ä¸€æ‰¹æ ·æœ¬(batch),æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œè¿˜éœ€è¦è®¡ç®—é‡ä¹˜ä¸Šæ‰¹é‡å°ºå¯¸.</p>

<p>æˆ‘ä»¬è¿™é‡Œå·ç§¯å±‚çš„å­¦ä¹ å‚æ•°é‡ä¸º:<br/>
<code>Params = I*J*K*L = 25000</code><br/>
æ‰€ä»¥è®¡ç®—é‡-å‚æ•°é‡ä¹‹æ¯”ä¸º<code>CPR=Calculations/Params = M*N = 64</code><br/>
<font color=red><strong>æ‰€ä»¥æˆ‘ä»¬å¾—å‡ºç»“è®ºæ˜¯:å·åŸºå±‚çš„è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸è¶Šå¤§,CPRå€¼è¶Šå¤§,å‚æ•°é‡å¤åˆ©ç”¨ç‡è¶Šé«˜.,è‹¥ä¸€æ¬¡æ€§è¾“å…¥ä¸€æ‰¹æ•°æ®(Bä¸ªæ ·æœ¬),åˆ™CPRå€¼å¯å†æé«˜Bå€.</strong></font></p>

<h2 id="toc_1">å…¨è¿æ¥å±‚</h2>

<p>æ—©åœ¨å·ç§¯ç¥ç»ç½‘ç»œå‡ºç°ä¹‹å‰,æœ€æ—©çš„æ·±åº¦å­¦ä¹ ç½‘ç»œè®¡ç®—ç±»å‹éƒ½æ˜¯å…¨è¿æ¥å½¢å¼çš„.å¦‚ä¸‹æ‰€ç¤º.<br/>
<img src="media/14955929300450/14961962825939.jpg" alt=""/><br/>
æ¯ä¸ªèŠ‚ç‚¹ä¸ç›¸é‚»å±‚çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰è¿æ¥å…³ç³»,è¿™æ˜¯å…¨è¿æ¥å±‚åç§°çš„ç”±æ¥.</p>

<p>å…¨è¿æ¥å±‚çš„ä¸»è¦è®¡ç®—ç±»å‹ä¸ºçŸ©é˜µ-å‘é‡ä¹˜(GEMV).å‡è®¾è¾“å…¥èŠ‚ç‚¹ç»„æˆçš„å‘é‡ä¸ºx,ç»´åº¦ä¸ºD,è¾“å‡ºèŠ‚ç‚¹ç»„æˆçš„å‘é‡ä¸ºy,ç»´åº¦ä¸ºV,åˆ™å…¨è¿æ¥å±‚è®¡ç®—å¯ä»¥è¡¨ç¤ºä¸º:<br/>
\[y=Wx\]<br/>
å…¶ä¸­,Wä¸º<code>V*D</code>ç»´æƒå€¼çŸ©é˜µ.</p>

<p>æˆ‘ä»¬åˆ†æå…¨è¿æ¥å±‚çš„å‚æ•°:<br/>
<img src="media/14955929300450/14961986814721.jpg" alt=""/></p>

<p><img src="media/14955929300450/14961986616938.jpg" alt=""/></p>

<p>å¾—å‡ºè¾“å‡º<code>V=500</code>,è¾“å…¥<code>D=50*4*4 = 800</code>(å…¶ä¸­50æ˜¯è¾“å…¥æ•°é‡,4*4ä½å›¾çš„å°ºå¯¸)</p>

<p>åˆ™å…¨è¿æ¥å±‚å•æ ·æœ¬å‰å‘ä¼ æ’­è®¡ç®—é‡ä¸º:<br/>
\[<br/>
CalculationsMAC = V*D<br/>
                = 800 * 500<br/>
                = 400000<br/>
\]<br/>
å‚æ•°ç»Ÿè®¡é‡ä¸º:<br/>
\[<br/>
Params = V*D<br/>
       = 800 * 500<br/>
       = 400000<br/>
\]<br/>
æ‰€ä»¥CPRå€¼ä¸º1</p>

<p>æ‰€ä»¥å¾—å‡ºç»“è®º,å…¨è¿æ¥å±‚çš„CPRå€¼å§‹ç»ˆä¸º1,ä¸è¾“å…¥,è¾“å‡ºç»´åº¦æ— å…³.æ‰€ä»¥å•æ ·æœ¬å‰å‘å‘ä¼ æ’­è®¡ç®—æ—¶,æƒå€¼é‡å¤åˆ©ç”¨ç‡å¾ˆä½.<br/>
    æˆ‘ä»¬å°†ä¸€æ‰¹(Bä¸ª)æ ·æœ¬é€åˆ—æ‹¼æ¥æˆçŸ©é˜µX,ä¸€æ¬¡æ€§é€šè¿‡å…¨è¿æ¥å±‚,å¾—åˆ°ä¸€æ‰¹è¾“å‡ºå‘é‡æ„æˆçš„çŸ©é˜µY,ç§°ä½œæ‰¹å¤„ç†(çŸ©é˜µ-çŸ©é˜µä¹˜è®¡ç®—GEMM):<br/>
    \[<br/>
    Y=WX<br/>
    \]<br/>
è¿™æ ·å…¨è¿æ¥å±‚å‰å‘è®¡ç®—é‡æé«˜äº†Bå€,è€Œå‚æ•°é‡ä¸å˜,å› æ­¤CPRæé«˜äº†Bå€.</p>

<p>ä¸å·ç§¯å±‚ç›¸æ¯”ï¼Œå…¨è¿æ¥å±‚å‚æ•°é‡æ˜¯å…¶16å€ï¼Œè€Œè®¡ç®—é‡åªæœ‰å…¶25%.å¦‚æœè¾“å‡ºç‰¹å¾å›¾å°ºå¯¸ç›¸åŒ<code>ï¼ˆM*V = V)</code>ï¼Œå·ç§¯å±‚çš„CPRå€¼ä¸ºå…¨è¿æ¥å±‚çš„<code>M*N</code>å€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå·ç§¯å±‚åœ¨è¾“å‡ºç‰¹å¾å›¾ç»´åº¦å®ç°äº†<mark>æƒå€¼å…±äº«</mark>ã€‚è¿™æ˜¯é™ä½å‚æ•°é‡çš„é‡è¦ä¸¾æªã€‚ä¸æ­¤åŒå‹ï¼Œå·æ³å±‚<mark>å±€éƒ¨è¿æ¥</mark>ç‰¹æ€§ (ç›¸æ¯”å…¨è¿æ¥ï¼‰ä¹Ÿå¤§å¹…å‡å°‘äº†å‚æ•°é‡,è¿™ä½¿å¾—CNNç½‘ç»œä¸­å‰å‡ å±‚å·ç§¯å±‚å‚æ•°é‡å æ¯”å°ï¼Œè®¡ç®—é‡å æ¯”å¤§ï¼›è€Œåå‡ å±‚å…¨è¿æ¥å±‚å‚æ•°é‡å æ¯”å¤§ï¼Œè®¡ç®—é‡å æ¯”å°ã€‚å¤§å¤šæ•°CNNæ¨¡å‹éƒ½ç¬¦åˆè¿™ä¸ªç‰¹ç‚¹ã€‚<font color=red><strong>å› æ­¤æˆ‘ä»¬åœ¨è¿›è¡Œè®¡ç®—åŠ é€Ÿä¼˜åŒ–æ—¶,é‡ç‚¹æ”¾åœ¨å·ç§¯å±‚ï¼›è€Œåœ¨è¿›è¡Œå‚æ•°ä¼˜åŒ–ã€æƒå€¼å‰ªè£æ—¶ï¼Œé‡ç‚¹æ”¾åœ¨å…¨è¿æ¥å±‚ï¼Œ</strong></font></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[è¿è¡Œcaffeæ¡†æ¶ä¸­çš„cifar10æ ·ä¾‹]]></title>
    <link href="https://lockxmonk.github.io/14955245049549.html"/>
    <updated>2017-05-23T15:28:24+08:00</updated>
    <id>https://lockxmonk.github.io/14955245049549.html</id>
    <content type="html"><![CDATA[
<p>1.å…ˆè¿è¡Œcaffeç›®å½•ä¸‹çš„data/get_cifar10.shè„šæœ¬.</p>

<pre><code>#!/usr/bin/env sh
# This scripts downloads the CIFAR10 (binary version) data and unzips it.

DIR=&quot;$( cd &quot;$(dirname &quot;$0&quot;)&quot; ; pwd -P )&quot;
cd &quot;$DIR&quot;

echo &quot;Downloading...&quot;

wget --no-check-certificate http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz

echo &quot;Unzipping...&quot;

tar -xf cifar-10-binary.tar.gz &amp;&amp; rm -f cifar-10-binary.tar.gz
mv cifar-10-batches-bin/* . &amp;&amp; rm -rf cifar-10-batches-bin

# Creation is split out because leveldb sometimes causes segfault
# and needs to be re-created.

echo &quot;Done.&quot;
</code></pre>

<p>è·å–æ•°æ®é›†.</p>

<p>ä¹‹åè¿è¡Œexampleä¸‹çš„cifar10/create_cifar10.sh<br/>
ä½†æ˜¯ä¼šé‡åˆ°ä»¥ä¸‹æŠ¥é”™:<br/>
<img src="media/14955245049549/14955247436003.jpg" alt=""/></p>

<p>è¿™é‡Œè¦è¿è¡Œä¸‹é¢è¿™ä¸ªå‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/examples/cifar10/convert_cifar_data.bin
</code></pre>

<p><img src="media/14955245049549/14955249112611.jpg" alt=""/></p>

<p>å†æ¬¡è¿è¡Œ./examples/cifar10/create_cifar10.shååˆä¼šå‡ºç°ä¸€ä¸ªé”™è¯¯:<br/>
<img src="media/14955245049549/14955249879358.jpg" alt=""/><br/>
è¿™é‡Œè¦å†æ¬¡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/tools/compute_image_mean
</code></pre>

<p><img src="media/14955245049549/14955250166086.jpg" alt=""/></p>

<p>ç„¶åå†æ¬¡æ‰§è¡Œ:</p>

<pre><code>./examples/cifar10/create_cifar10.sh
</code></pre>

<p>ç»“æœæˆåŠŸäº†!å¦‚ä¸‹å›¾æ‰€ç¤º:<br/>
<img src="media/14955245049549/14955251049094.jpg" alt=""/></p>

<h1 id="toc_0">Training and Testing the &quot;Quick&quot; Model</h1>

<p>å› ä¸ºä¾‹å­ä¸­å·²ç»ç»™å‡ºå®šä¹‰å¥½çš„protobufå’Œsolver protobufæ–‡ä»¶,æ‰€ä»¥æˆ‘ä»¬ç›´æ¥è¿è¡Œ<code>train_quick.sh</code></p>

<p>è¯¥æ–‡ä»¶å†…å®¹ä¸º:</p>

<pre><code class="language-sh">#!/usr/bin/env sh
set -e

TOOLS=./build/tools

$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver.prototxt $@

# reduce learning rate by factor of 10 after 8 epochs
$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver_lr1.prototxt \
  --snapshot=examples/cifar10/cifar10_quick_iter_4000.solverstate $@
</code></pre>

<p>æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤:</p>

<pre><code>âœ caffe git:(master) âœ— ./examples/cifar10/train_quick.sh
</code></pre>

<p>ç„¶åè¾“å‡ºä¸º:</p>

<pre><code>I0523 15:43:36.608793 2712679360 caffe.cpp:211] Use CPU.
I0523 15:43:36.609737 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 4000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 4000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0523 15:43:36.610075 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.610931 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 15:43:36.610961 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 15:43:36.610966 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.611205 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.611467 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 15:43:36.611524 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.611531 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.611549 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.611565 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.611686 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.617992 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.618022 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.618028 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.618032 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.618041 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.618052 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.618057 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.618063 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.618175 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.618180 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.618185 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 15:43:36.618192 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.618199 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.618202 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.618206 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.618216 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.618219 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618224 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 15:43:36.618228 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.618234 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.618238 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.618242 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.618247 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.618250 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618255 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 15:43:36.618263 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.618273 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.618276 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.618281 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.618585 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.618592 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618597 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 15:43:36.618602 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.618607 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.618609 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.618614 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.618619 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.618623 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618628 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 15:43:36.618630 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.618634 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.618638 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.618643 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.618647 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.618654 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.618662 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 15:43:36.618669 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.618680 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.618685 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.618695 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.619361 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.619372 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619379 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 15:43:36.619385 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.619390 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.619393 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.619398 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.619403 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.619447 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619459 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 15:43:36.619467 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.619477 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.619484 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.619493 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.619505 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.619513 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.619523 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 15:43:36.619529 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.619539 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.619546 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.619555 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.620586 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.620602 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.620607 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 15:43:36.620613 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.620620 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.620625 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.620630 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.620649 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.620656 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.620662 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 15:43:36.620673 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620682 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.620689 2712679360 net.cpp:406] loss &lt;- ip2
I0523 15:43:36.620697 2712679360 net.cpp:406] loss &lt;- label
I0523 15:43:36.620703 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.620730 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620749 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.620756 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.620764 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.620787 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 15:43:36.620795 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.620800 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.620807 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.620813 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.620820 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.620832 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.620851 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.620859 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.620867 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.620875 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.620882 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.620889 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.620896 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.620904 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.620916 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.621170 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.621199 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 15:43:36.621210 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.621821 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.621913 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 15:43:36.621933 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.621943 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.621950 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.621958 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.622017 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.624790 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.624822 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.624830 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624835 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.624840 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 15:43:36.624851 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 15:43:36.624856 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 15:43:36.624862 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 15:43:36.624869 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 15:43:36.624876 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 15:43:36.624878 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624882 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624886 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 15:43:36.624917 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.624927 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.624930 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.624935 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.624987 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.624991 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.624996 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 15:43:36.625002 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.625008 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.625011 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.625015 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.625022 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.625026 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625031 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 15:43:36.625036 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.625041 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.625043 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.625048 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.625053 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.625056 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625061 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 15:43:36.625064 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.625071 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.625074 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.625084 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.625396 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.625402 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625407 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 15:43:36.625412 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.625417 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.625422 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.625425 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.625429 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.625433 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625437 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 15:43:36.625440 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.625445 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.625448 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.625452 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.625458 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.625460 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.625464 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 15:43:36.625468 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.625474 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.625479 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.625483 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.626077 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.626083 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626088 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 15:43:36.626093 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.626098 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.626101 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.626106 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.626111 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.626113 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626117 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 15:43:36.626121 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.626126 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.626129 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.626145 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.626152 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.626154 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.626159 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 15:43:36.626163 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.626168 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.626173 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.626176 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.626969 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.626981 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.626986 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 15:43:36.626992 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.626999 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.627003 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.627008 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.627024 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.627028 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627032 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 15:43:36.627039 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 15:43:36.627046 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 15:43:36.627053 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 15:43:36.627059 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 15:43:36.627068 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 15:43:36.627076 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 15:43:36.627081 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627085 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627089 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 15:43:36.627094 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 15:43:36.627099 2712679360 net.cpp:84] Creating Layer accuracy
I0523 15:43:36.627102 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 15:43:36.627106 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 15:43:36.627110 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 15:43:36.627116 2712679360 net.cpp:122] Setting up accuracy
I0523 15:43:36.627120 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627123 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 15:43:36.627126 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627133 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.627169 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 15:43:36.627178 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 15:43:36.627183 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.627189 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627198 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.627202 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627207 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.627213 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 15:43:36.627215 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.627219 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 15:43:36.627223 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 15:43:36.627228 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.627230 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.627234 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.627321 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.627334 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.627341 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.627348 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.627354 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.627387 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.627394 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.627400 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.627409 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 15:43:36.627418 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.627432 2712679360 net.cpp:242] This network produces output accuracy
I0523 15:43:36.627454 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.627470 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.627553 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 15:43:36.627593 2712679360 caffe.cpp:248] Starting Optimization
I0523 15:43:36.627602 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 15:43:36.627610 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 15:43:36.627933 2712679360 solver.cpp:330] Iteration 0, Testing net (#0)
I0523 15:43:46.157997 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:43:46.542196 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.0865
I0523 15:43:46.542232 2712679360 solver.cpp:397]     Test net output #1: loss = 2.3025 (* 1 = 2.3025 loss)
I0523 15:43:46.784966 2712679360 solver.cpp:218] Iteration 0 (0 iter/s, 10.157s/100 iters), loss = 2.30202
I0523 15:43:46.785002 2712679360 solver.cpp:237]     Train net output #0: loss = 2.30202 (* 1 = 2.30202 loss)
I0523 15:43:46.785009 2712679360 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0523 15:44:08.112608 2712679360 solver.cpp:218] Iteration 100 (4.68889 iter/s, 21.327s/100 iters), loss = 1.67773
I0523 15:44:08.112664 2712679360 solver.cpp:237]     Train net output #0: loss = 1.67773 (* 1 = 1.67773 loss)
I0523 15:44:08.112673 2712679360 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0523 15:44:29.336644 2712679360 solver.cpp:218] Iteration 200 (4.71187 iter/s, 21.223s/100 iters), loss = 1.59886
I0523 15:44:29.336683 2712679360 solver.cpp:237]     Train net output #0: loss = 1.59886 (* 1 = 1.59886 loss)
I0523 15:44:29.336693 2712679360 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0523 15:44:50.573981 2712679360 solver.cpp:218] Iteration 300 (4.70876 iter/s, 21.237s/100 iters), loss = 1.31839
I0523 15:44:50.574038 2712679360 solver.cpp:237]     Train net output #0: loss = 1.31839 (* 1 = 1.31839 loss)
I0523 15:44:50.574044 2712679360 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0523 15:45:12.080576 2712679360 solver.cpp:218] Iteration 400 (4.64987 iter/s, 21.506s/100 iters), loss = 1.24876
I0523 15:45:12.080610 2712679360 solver.cpp:237]     Train net output #0: loss = 1.24876 (* 1 = 1.24876 loss)
I0523 15:45:12.080618 2712679360 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0523 15:45:32.450579 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:33.342396 2712679360 solver.cpp:330] Iteration 500, Testing net (#0)
I0523 15:45:42.732501 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:43.134589 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.5366
I0523 15:45:43.134620 2712679360 solver.cpp:397]     Test net output #1: loss = 1.31952 (* 1 = 1.31952 loss)
I0523 15:45:43.360550 2712679360 solver.cpp:218] Iteration 500 (3.19703 iter/s, 31.279s/100 iters), loss = 1.22391
I0523 15:45:43.360582 2712679360 solver.cpp:237]     Train net output #0: loss = 1.22391 (* 1 = 1.22391 loss)
I0523 15:45:43.360589 2712679360 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0523 15:46:06.734716 2712679360 solver.cpp:218] Iteration 600 (4.27826 iter/s, 23.374s/100 iters), loss = 1.23177
I0523 15:46:06.734771 2712679360 solver.cpp:237]     Train net output #0: loss = 1.23177 (* 1 = 1.23177 loss)
I0523 15:46:06.734779 2712679360 sgd_solver.cpp:105] Iteration 600, lr = 0.001
.......æ•°æ®å½¢å¼åŸºæœ¬ç›¸åŒ æ•…çœç•¥...
I0523 16:00:46.286926 2712679360 solver.cpp:218] Iteration 3900 (4.08731 iter/s, 24.466s/100 iters), loss = 0.557826
I0523 16:00:46.286960 2712679360 solver.cpp:237]     Train net output #0: loss = 0.557826 (* 1 = 0.557826 loss)
I0523 16:00:46.286967 2712679360 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0523 16:01:09.469552 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:10.472170 2712679360 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/cifar10_quick_iter_4000.caffemodel
I0523 16:01:10.475755 2712679360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:10.590515 2712679360 solver.cpp:310] Iteration 4000, loss = 0.641508
I0523 16:01:10.590548 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:21.619536 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:22.054498 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:22.054538 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:22.054548 2712679360 solver.cpp:315] Optimization Done.
I0523 16:01:22.054555 2712679360 caffe.cpp:259] Optimization Done.
I0523 16:01:22.119184 2712679360 caffe.cpp:211] Use CPU.
I0523 16:01:22.120214 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.0001
display: 100
max_iter: 5000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
snapshot_format: HDF5
I0523 16:01:22.120556 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.120817 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 16:01:22.120833 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 16:01:22.120841 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.121104 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.121320 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 16:01:22.121383 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.121393 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.121413 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.121431 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.121585 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.128842 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.128867 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.128875 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.128880 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.128890 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.128902 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.128907 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.128914 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.129009 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.129017 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.129022 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 16:01:22.129030 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.129039 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.129042 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.129047 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.129057 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.129062 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129067 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 16:01:22.129071 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.129078 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.129083 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.129087 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.129093 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.129097 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129102 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 16:01:22.129106 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.129117 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.129120 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.129125 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.129482 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.129487 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129493 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 16:01:22.129500 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.129505 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.129509 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.129514 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.129520 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.129524 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129528 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 16:01:22.129534 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.129537 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.129541 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.129547 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.129554 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.129557 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.129562 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 16:01:22.129566 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.129573 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.129577 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.129585 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.130280 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.130286 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130292 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 16:01:22.130298 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.130304 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.130308 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.130313 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.130318 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.130353 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130360 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 16:01:22.130364 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.130370 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.130374 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.130379 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.130385 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.130389 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.130396 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 16:01:22.130400 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.130409 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.130414 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.130419 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.131337 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.131347 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.131352 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 16:01:22.131358 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.131364 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.131369 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.131374 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.131392 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.131397 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.131400 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 16:01:22.131407 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131413 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.131417 2712679360 net.cpp:406] loss &lt;- ip2
I0523 16:01:22.131422 2712679360 net.cpp:406] loss &lt;- label
I0523 16:01:22.131427 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.131435 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131448 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.131453 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.131458 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.131471 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 16:01:22.131476 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.131495 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.131505 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.131510 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.131515 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.131518 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.131522 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.131527 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.131531 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.131536 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.131541 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.131544 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.131548 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.131552 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.131561 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.131786 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.131814 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:01:22.131826 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.132225 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.132313 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:01:22.132342 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.132356 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.132364 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.132372 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.132438 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.134943 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.134956 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.134963 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.134968 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.134974 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:01:22.134984 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:01:22.135015 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:01:22.135064 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:01:22.135078 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:01:22.135116 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:01:22.135167 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135203 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135241 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:01:22.135313 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.135330 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.135335 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.135342 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.135398 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.135404 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.135411 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:01:22.135418 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.135463 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.135473 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.135514 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.135565 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.135574 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135581 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:01:22.135586 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.135593 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.135598 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.135603 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.135609 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.135613 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135666 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:01:22.135673 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.135681 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.135686 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.135700 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.136068 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.136076 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136081 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:01:22.136088 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.136095 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.136098 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.136103 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.136108 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.136112 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136117 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:01:22.136121 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.136127 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.136132 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.136135 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.136142 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.136147 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.136152 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:01:22.136157 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.136163 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.136168 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.136173 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.136878 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.136888 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136893 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:01:22.136899 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.136904 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.136909 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.136914 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.136919 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.136930 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136961 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:01:22.136968 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.136976 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.137001 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.137008 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.137017 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.137022 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.137027 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:01:22.137032 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.137039 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.137044 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.137050 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.137981 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.137995 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.138002 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:01:22.138008 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.138016 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.138021 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.138027 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.138046 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.138051 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138056 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:01:22.138062 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:01:22.138085 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:01:22.138103 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:01:22.138115 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:01:22.138129 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:01:22.138142 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:01:22.138150 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138160 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138170 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:01:22.138177 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:01:22.138187 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:01:22.138219 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:01:22.138231 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:01:22.138242 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:01:22.138257 2712679360 net.cpp:122] Setting up accuracy
I0523 16:01:22.138264 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138274 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:01:22.138279 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138286 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.138290 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:01:22.138327 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:01:22.138334 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.138342 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138352 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.138357 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138362 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.138368 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:01:22.138372 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.138377 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:01:22.138382 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:01:22.138386 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.138391 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.138396 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.138401 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.138404 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.138408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.138412 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.138417 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.138444 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.138449 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.138454 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.138463 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:01:22.138468 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.138470 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:01:22.138476 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.138485 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.138537 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 16:01:22.138566 2712679360 caffe.cpp:242] Resuming from examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:22.139786 2712679360 sgd_solver.cpp:318] SGDSolver: restoring history
I0523 16:01:22.140019 2712679360 caffe.cpp:248] Starting Optimization
I0523 16:01:22.140027 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 16:01:22.140031 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 16:01:22.140113 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:32.383680 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:32.807214 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:32.807250 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:33.065510 2712679360 solver.cpp:218] Iteration 4000 (366.133 iter/s, 10.925s/100 iters), loss = 0.641508
I0523 16:01:33.065546 2712679360 solver.cpp:237]     Train net output #0: loss = 0.641508 (* 1 = 0.641508 loss)
I0523 16:01:33.065553 2712679360 sgd_solver.cpp:105] Iteration 4000, lr = 0.0001
I0523 16:01:56.950950 2712679360 solver.cpp:218] Iteration 4100 (4.18673 iter/s, 23.885s/100 iters), loss = 0.603556
I0523 16:01:56.951002 2712679360 solver.cpp:237]     Train net output #0: loss = 0.603556 (* 1 = 0.603556 loss)
I0523 16:01:56.951010 2712679360 sgd_solver.cpp:105] Iteration 4100, lr = 0.0001
I0523 16:02:21.127391 2712679360 solver.cpp:218] Iteration 4200 (4.13633 iter/s, 24.176s/100 iters), loss = 0.491505
I0523 16:02:21.127429 2712679360 solver.cpp:237]     Train net output #0: loss = 0.491505 (* 1 = 0.491505 loss)
I0523 16:02:21.127437 2712679360 sgd_solver.cpp:105] Iteration 4200, lr = 0.0001
I0523 16:02:46.283135 2712679360 solver.cpp:218] Iteration 4300 (3.97535 iter/s, 25.155s/100 iters), loss = 0.495313
I0523 16:02:46.283190 2712679360 solver.cpp:237]     Train net output #0: loss = 0.495313 (* 1 = 0.495313 loss)
I0523 16:02:46.283198 2712679360 sgd_solver.cpp:105] Iteration 4300, lr = 0.0001
I0523 16:03:10.841265 2712679360 solver.cpp:218] Iteration 4400 (4.07199 iter/s, 24.558s/100 iters), loss = 0.438567
I0523 16:03:10.841303 2712679360 solver.cpp:237]     Train net output #0: loss = 0.438567 (* 1 = 0.438567 loss)
I0523 16:03:10.841310 2712679360 sgd_solver.cpp:105] Iteration 4400, lr = 0.0001
I0523 16:03:33.942627 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:34.958622 2712679360 solver.cpp:330] Iteration 4500, Testing net (#0)
I0523 16:03:45.910739 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:46.349741 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.752
I0523 16:03:46.349779 2712679360 solver.cpp:397]     Test net output #1: loss = 0.748076 (* 1 = 0.748076 loss)
I0523 16:03:46.589071 2712679360 solver.cpp:218] Iteration 4500 (2.79744 iter/s, 35.747s/100 iters), loss = 0.503921
I0523 16:03:46.589107 2712679360 solver.cpp:237]     Train net output #0: loss = 0.503921 (* 1 = 0.503921 loss)
I0523 16:03:46.589113 2712679360 sgd_solver.cpp:105] Iteration 4500, lr = 0.0001
I0523 16:04:10.851019 2712679360 solver.cpp:218] Iteration 4600 (4.12184 iter/s, 24.261s/100 iters), loss = 0.562534
I0523 16:04:10.851088 2712679360 solver.cpp:237]     Train net output #0: loss = 0.562534 (* 1 = 0.562534 loss)
I0523 16:04:10.851095 2712679360 sgd_solver.cpp:105] Iteration 4600, lr = 0.0001
I0523 16:04:35.547813 2712679360 solver.cpp:218] Iteration 4700 (4.04924 iter/s, 24.696s/100 iters), loss = 0.464102
I0523 16:04:35.547852 2712679360 solver.cpp:237]     Train net output #0: loss = 0.464102 (* 1 = 0.464102 loss)
I0523 16:04:35.547860 2712679360 sgd_solver.cpp:105] Iteration 4700, lr = 0.0001
I0523 16:05:00.517423 2712679360 solver.cpp:218] Iteration 4800 (4.00497 iter/s, 24.969s/100 iters), loss = 0.474584
I0523 16:05:00.517478 2712679360 solver.cpp:237]     Train net output #0: loss = 0.474584 (* 1 = 0.474584 loss)
I0523 16:05:00.517487 2712679360 sgd_solver.cpp:105] Iteration 4800, lr = 0.0001
I0523 16:05:24.429520 2712679360 solver.cpp:218] Iteration 4900 (4.182 iter/s, 23.912s/100 iters), loss = 0.417258
I0523 16:05:24.429554 2712679360 solver.cpp:237]     Train net output #0: loss = 0.417258 (* 1 = 0.417258 loss)
I0523 16:05:24.429563 2712679360 sgd_solver.cpp:105] Iteration 4900, lr = 0.0001
I0523 16:05:47.148733 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:48.086921 2712679360 solver.cpp:457] Snapshotting to HDF5 file examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5
I0523 16:05:48.101351 2712679360 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_quick_iter_5000.solverstate.h5
I0523 16:05:48.215885 2712679360 solver.cpp:310] Iteration 5000, loss = 0.487594
I0523 16:05:48.215921 2712679360 solver.cpp:330] Iteration 5000, Testing net (#0)
I0523 16:05:58.710295 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:59.149840 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.754
I0523 16:05:59.149875 2712679360 solver.cpp:397]     Test net output #1: loss = 0.742307 (* 1 = 0.742307 loss)
I0523 16:05:59.149883 2712679360 solver.cpp:315] Optimization Done.
I0523 16:05:59.149888 2712679360 caffe.cpp:259] Optimization Done.
</code></pre>

<p>è®­ç»ƒå®Œæ¯•.å¹¶ä¸”åœ¨æœ€åå·²ç»åˆ›å»ºå¥½äº†æµ‹è¯•ç½‘ç»œ.</p>

<p>ä¸‹é¢æˆ‘ä»¬ç”¨è®­ç»ƒå¥½çš„cifar10æ¨¡å‹æ¥å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹:</p>

<p>è¿è¡Œå¦‚ä¸‹å‘½ä»¤:</p>

<pre><code>âœ  caffe git:(master) âœ— ./build/tools/caffe.bin test \
-model examples/cifar10/cifar10_quick_train_test.prototxt \
-weights examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5 \
-iterations 100
</code></pre>

<p>å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹:</p>

<pre><code>I0523 16:25:41.234220 2712679360 caffe.cpp:284] Use CPU.
I0523 16:25:41.238044 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:25:41.238080 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:25:41.238523 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:25:41.238731 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:25:41.238788 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:25:41.238796 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:25:41.238816 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:25:41.238834 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:25:41.238957 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:25:41.246219 2712679360 net.cpp:122] Setting up cifar
I0523 16:25:41.246245 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:25:41.246253 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246258 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:25:41.246266 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:25:41.246278 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:25:41.246282 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:25:41.246343 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:25:41.246367 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:25:41.246381 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:25:41.246390 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246400 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246409 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:25:41.246417 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:25:41.246438 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:25:41.246448 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:25:41.246457 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:25:41.246606 2712679360 net.cpp:122] Setting up conv1
I0523 16:25:41.246637 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:25:41.246680 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:25:41.246693 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:25:41.246708 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:25:41.246721 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:25:41.246731 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:25:41.246752 2712679360 net.cpp:122] Setting up pool1
I0523 16:25:41.246781 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246788 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:25:41.246793 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:25:41.246804 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:25:41.246809 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:25:41.246814 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:25:41.246821 2712679360 net.cpp:122] Setting up relu1
I0523 16:25:41.246825 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246830 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:25:41.246834 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:25:41.246841 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:25:41.246846 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:25:41.246851 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:25:41.247228 2712679360 net.cpp:122] Setting up conv2
I0523 16:25:41.247236 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247242 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:25:41.247249 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:25:41.247259 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:25:41.247264 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:25:41.247269 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:25:41.247274 2712679360 net.cpp:122] Setting up relu2
I0523 16:25:41.247278 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247283 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:25:41.247287 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:25:41.247293 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:25:41.247298 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:25:41.247301 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:25:41.247308 2712679360 net.cpp:122] Setting up pool2
I0523 16:25:41.247313 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:25:41.247318 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:25:41.247321 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:25:41.247329 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:25:41.247334 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:25:41.247339 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:25:41.248001 2712679360 net.cpp:122] Setting up conv3
I0523 16:25:41.248008 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248013 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:25:41.248020 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:25:41.248025 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:25:41.248051 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:25:41.248057 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:25:41.248067 2712679360 net.cpp:122] Setting up relu3
I0523 16:25:41.248072 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248077 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:25:41.248081 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:25:41.248085 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:25:41.248090 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:25:41.248095 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:25:41.248102 2712679360 net.cpp:122] Setting up pool3
I0523 16:25:41.248109 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:25:41.248114 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:25:41.248117 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:25:41.248124 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:25:41.248152 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:25:41.248162 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:25:41.248950 2712679360 net.cpp:122] Setting up ip1
I0523 16:25:41.248993 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:25:41.249008 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:25:41.249014 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:25:41.249020 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:25:41.249024 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:25:41.249038 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:25:41.249080 2712679360 net.cpp:122] Setting up ip2
I0523 16:25:41.249097 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249102 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:25:41.249115 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:25:41.249120 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:25:41.249125 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:25:41.249130 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:25:41.249143 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:25:41.249150 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:25:41.249155 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249164 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249171 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:25:41.249174 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:25:41.249183 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:25:41.249187 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:25:41.249191 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:25:41.249195 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:25:41.249202 2712679360 net.cpp:122] Setting up accuracy
I0523 16:25:41.249205 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249209 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:25:41.249214 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249219 2712679360 net.cpp:84] Creating Layer loss
I0523 16:25:41.249223 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:25:41.249236 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:25:41.249241 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:25:41.249249 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249266 2712679360 net.cpp:122] Setting up loss
I0523 16:25:41.249274 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249279 2712679360 net.cpp:132]     with loss weight 1
I0523 16:25:41.249300 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:25:41.249305 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:25:41.249310 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:25:41.249320 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:25:41.249325 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:25:41.249330 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:25:41.249366 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:25:41.249388 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:25:41.249392 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:25:41.249408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:25:41.249413 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:25:41.249416 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:25:41.249420 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:25:41.249424 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:25:41.249428 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:25:41.249431 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:25:41.249436 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:25:41.249439 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:25:41.249444 2712679360 net.cpp:242] This network produces output loss
I0523 16:25:41.249451 2712679360 net.cpp:255] Network initialization done.
I0523 16:25:41.251152 2712679360 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 16:25:41.252013 2712679360 caffe.cpp:290] Running for 100 iterations.
I0523 16:25:41.367466 2712679360 caffe.cpp:313] Batch 0, accuracy = 0.81
I0523 16:25:41.367501 2712679360 caffe.cpp:313] Batch 0, loss = 0.650321
I0523 16:25:41.465518 2712679360 caffe.cpp:313] Batch 1, accuracy = 0.75
I0523 16:25:41.465550 2712679360 caffe.cpp:313] Batch 1, loss = 0.767328
I0523 16:25:41.560680 2712679360 caffe.cpp:313] Batch 2, accuracy = 0.71
I0523 16:25:41.560712 2712679360 caffe.cpp:313] Batch 2, loss = 0.810281
I0523 16:25:41.656878 2712679360 caffe.cpp:313] Batch 3, accuracy = 0.7
I0523 16:25:41.656913 2712679360 caffe.cpp:313] Batch 3, loss = 0.807916
I0523 16:25:41.757275 2712679360 caffe.cpp:313] Batch 4, accuracy = 0.71
I0523 16:25:41.757313 2712679360 caffe.cpp:313] Batch 4, loss = 0.797028
I0523 16:25:41.855583 2712679360 caffe.cpp:313] Batch 5, accuracy = 0.84
I0523 16:25:41.855613 2712679360 caffe.cpp:313] Batch 5, loss = 0.422262
I0523 16:25:41.953912 2712679360 caffe.cpp:313] Batch 6, accuracy = 0.73
I0523 16:25:41.953946 2712679360 caffe.cpp:313] Batch 6, loss = 0.696204
I0523 16:25:42.052671 2712679360 caffe.cpp:313] Batch 7, accuracy = 0.72
I0523 16:25:42.052705 2712679360 caffe.cpp:313] Batch 7, loss = 0.896313
I0523 16:25:42.155107 2712679360 caffe.cpp:313] Batch 8, accuracy = 0.73
I0523 16:25:42.155153 2712679360 caffe.cpp:313] Batch 8, loss = 0.862504
I0523 16:25:42.258592 2712679360 caffe.cpp:313] Batch 9, accuracy = 0.78
I0523 16:25:42.258627 2712679360 caffe.cpp:313] Batch 9, loss = 0.642714
I0523 16:25:42.362510 2712679360 caffe.cpp:313] Batch 10, accuracy = 0.75
I0523 16:25:42.362543 2712679360 caffe.cpp:313] Batch 10, loss = 0.827924
I0523 16:25:42.463922 2712679360 caffe.cpp:313] Batch 11, accuracy = 0.76
I0523 16:25:42.463953 2712679360 caffe.cpp:313] Batch 11, loss = 0.674977
I0523 16:25:42.567791 2712679360 caffe.cpp:313] Batch 12, accuracy = 0.7
I0523 16:25:42.567822 2712679360 caffe.cpp:313] Batch 12, loss = 0.717463
I0523 16:25:42.664435 2712679360 caffe.cpp:313] Batch 13, accuracy = 0.75
I0523 16:25:42.664469 2712679360 caffe.cpp:313] Batch 13, loss = 0.640668
I0523 16:25:42.759980 2712679360 caffe.cpp:313] Batch 14, accuracy = 0.78
I0523 16:25:42.760013 2712679360 caffe.cpp:313] Batch 14, loss = 0.62553
I0523 16:25:42.856386 2712679360 caffe.cpp:313] Batch 15, accuracy = 0.76
I0523 16:25:42.856417 2712679360 caffe.cpp:313] Batch 15, loss = 0.721462
I0523 16:25:42.954746 2712679360 caffe.cpp:313] Batch 16, accuracy = 0.73
I0523 16:25:42.954777 2712679360 caffe.cpp:313] Batch 16, loss = 0.858499
I0523 16:25:43.053562 2712679360 caffe.cpp:313] Batch 17, accuracy = 0.75
I0523 16:25:43.053593 2712679360 caffe.cpp:313] Batch 17, loss = 0.746772
I0523 16:25:43.155479 2712679360 caffe.cpp:313] Batch 18, accuracy = 0.74
I0523 16:25:43.155508 2712679360 caffe.cpp:313] Batch 18, loss = 0.893995
I0523 16:25:43.254688 2712679360 caffe.cpp:313] Batch 19, accuracy = 0.68
I0523 16:25:43.254716 2712679360 caffe.cpp:313] Batch 19, loss = 0.943102
I0523 16:25:43.364045 2712679360 caffe.cpp:313] Batch 20, accuracy = 0.7
I0523 16:25:43.364076 2712679360 caffe.cpp:313] Batch 20, loss = 0.786499
I0523 16:25:43.465351 2712679360 caffe.cpp:313] Batch 21, accuracy = 0.76
I0523 16:25:43.465384 2712679360 caffe.cpp:313] Batch 21, loss = 0.742349
I0523 16:25:43.560330 2712679360 caffe.cpp:313] Batch 22, accuracy = 0.8
I0523 16:25:43.560362 2712679360 caffe.cpp:313] Batch 22, loss = 0.707087
I0523 16:25:43.662050 2712679360 caffe.cpp:313] Batch 23, accuracy = 0.69
I0523 16:25:43.662077 2712679360 caffe.cpp:313] Batch 23, loss = 0.854361
I0523 16:25:43.760444 2712679360 caffe.cpp:313] Batch 24, accuracy = 0.74
I0523 16:25:43.760473 2712679360 caffe.cpp:313] Batch 24, loss = 0.844035
I0523 16:25:43.858397 2712679360 caffe.cpp:313] Batch 25, accuracy = 0.68
I0523 16:25:43.858425 2712679360 caffe.cpp:313] Batch 25, loss = 1.02302
I0523 16:25:43.959595 2712679360 caffe.cpp:313] Batch 26, accuracy = 0.82
I0523 16:25:43.959627 2712679360 caffe.cpp:313] Batch 26, loss = 0.493385
I0523 16:25:44.057914 2712679360 caffe.cpp:313] Batch 27, accuracy = 0.76
I0523 16:25:44.057942 2712679360 caffe.cpp:313] Batch 27, loss = 0.78877
I0523 16:25:44.157359 2712679360 caffe.cpp:313] Batch 28, accuracy = 0.78
I0523 16:25:44.157388 2712679360 caffe.cpp:313] Batch 28, loss = 0.709657
I0523 16:25:44.285976 2712679360 caffe.cpp:313] Batch 29, accuracy = 0.78
I0523 16:25:44.286007 2712679360 caffe.cpp:313] Batch 29, loss = 0.674438
I0523 16:25:44.390980 2712679360 caffe.cpp:313] Batch 30, accuracy = 0.79
I0523 16:25:44.391010 2712679360 caffe.cpp:313] Batch 30, loss = 0.65947
I0523 16:25:44.491211 2712679360 caffe.cpp:313] Batch 31, accuracy = 0.77
I0523 16:25:44.491241 2712679360 caffe.cpp:313] Batch 31, loss = 0.716022
I0523 16:25:44.593423 2712679360 caffe.cpp:313] Batch 32, accuracy = 0.73
I0523 16:25:44.593457 2712679360 caffe.cpp:313] Batch 32, loss = 0.805526
I0523 16:25:44.692994 2712679360 caffe.cpp:313] Batch 33, accuracy = 0.68
I0523 16:25:44.693023 2712679360 caffe.cpp:313] Batch 33, loss = 0.903316
I0523 16:25:44.795087 2712679360 caffe.cpp:313] Batch 34, accuracy = 0.72
I0523 16:25:44.795116 2712679360 caffe.cpp:313] Batch 34, loss = 0.834438
I0523 16:25:44.897828 2712679360 caffe.cpp:313] Batch 35, accuracy = 0.73
I0523 16:25:44.897874 2712679360 caffe.cpp:313] Batch 35, loss = 0.908751
I0523 16:25:44.996119 2712679360 caffe.cpp:313] Batch 36, accuracy = 0.74
I0523 16:25:44.996150 2712679360 caffe.cpp:313] Batch 36, loss = 0.981981
I0523 16:25:45.093991 2712679360 caffe.cpp:313] Batch 37, accuracy = 0.76
I0523 16:25:45.094023 2712679360 caffe.cpp:313] Batch 37, loss = 0.725703
I0523 16:25:45.195551 2712679360 caffe.cpp:313] Batch 38, accuracy = 0.78
I0523 16:25:45.195585 2712679360 caffe.cpp:313] Batch 38, loss = 0.686703
I0523 16:25:45.292881 2712679360 caffe.cpp:313] Batch 39, accuracy = 0.8
I0523 16:25:45.292912 2712679360 caffe.cpp:313] Batch 39, loss = 0.650689
I0523 16:25:45.397084 2712679360 caffe.cpp:313] Batch 40, accuracy = 0.79
I0523 16:25:45.397115 2712679360 caffe.cpp:313] Batch 40, loss = 0.755663
I0523 16:25:45.495128 2712679360 caffe.cpp:313] Batch 41, accuracy = 0.82
I0523 16:25:45.495160 2712679360 caffe.cpp:313] Batch 41, loss = 0.855221
I0523 16:25:45.597597 2712679360 caffe.cpp:313] Batch 42, accuracy = 0.81
I0523 16:25:45.597626 2712679360 caffe.cpp:313] Batch 42, loss = 0.552907
I0523 16:25:45.695441 2712679360 caffe.cpp:313] Batch 43, accuracy = 0.8
I0523 16:25:45.695472 2712679360 caffe.cpp:313] Batch 43, loss = 0.688889
I0523 16:25:45.796842 2712679360 caffe.cpp:313] Batch 44, accuracy = 0.8
I0523 16:25:45.796875 2712679360 caffe.cpp:313] Batch 44, loss = 0.713613
I0523 16:25:45.899427 2712679360 caffe.cpp:313] Batch 45, accuracy = 0.76
I0523 16:25:45.899462 2712679360 caffe.cpp:313] Batch 45, loss = 0.819739
I0523 16:25:46.003129 2712679360 caffe.cpp:313] Batch 46, accuracy = 0.77
I0523 16:25:46.003190 2712679360 caffe.cpp:313] Batch 46, loss = 0.79499
I0523 16:25:46.101080 2712679360 caffe.cpp:313] Batch 47, accuracy = 0.73
I0523 16:25:46.101112 2712679360 caffe.cpp:313] Batch 47, loss = 0.784097
I0523 16:25:46.199532 2712679360 caffe.cpp:313] Batch 48, accuracy = 0.82
I0523 16:25:46.199563 2712679360 caffe.cpp:313] Batch 48, loss = 0.509592
I0523 16:25:46.296840 2712679360 caffe.cpp:313] Batch 49, accuracy = 0.76
I0523 16:25:46.296872 2712679360 caffe.cpp:313] Batch 49, loss = 0.775396
I0523 16:25:46.399880 2712679360 caffe.cpp:313] Batch 50, accuracy = 0.77
I0523 16:25:46.399914 2712679360 caffe.cpp:313] Batch 50, loss = 0.61452
I0523 16:25:46.500458 2712679360 caffe.cpp:313] Batch 51, accuracy = 0.79
I0523 16:25:46.500488 2712679360 caffe.cpp:313] Batch 51, loss = 0.631971
I0523 16:25:46.599107 2712679360 caffe.cpp:313] Batch 52, accuracy = 0.78
I0523 16:25:46.599139 2712679360 caffe.cpp:313] Batch 52, loss = 0.613152
I0523 16:25:46.699442 2712679360 caffe.cpp:313] Batch 53, accuracy = 0.74
I0523 16:25:46.699475 2712679360 caffe.cpp:313] Batch 53, loss = 0.813763
I0523 16:25:46.802717 2712679360 caffe.cpp:313] Batch 54, accuracy = 0.69
I0523 16:25:46.802749 2712679360 caffe.cpp:313] Batch 54, loss = 0.79753
I0523 16:25:46.903400 2712679360 caffe.cpp:313] Batch 55, accuracy = 0.81
I0523 16:25:46.903430 2712679360 caffe.cpp:313] Batch 55, loss = 0.683275
I0523 16:25:47.007345 2712679360 caffe.cpp:313] Batch 56, accuracy = 0.78
I0523 16:25:47.007377 2712679360 caffe.cpp:313] Batch 56, loss = 0.785579
I0523 16:25:47.107044 2712679360 caffe.cpp:313] Batch 57, accuracy = 0.84
I0523 16:25:47.107076 2712679360 caffe.cpp:313] Batch 57, loss = 0.455638
I0523 16:25:47.204998 2712679360 caffe.cpp:313] Batch 58, accuracy = 0.7
I0523 16:25:47.205029 2712679360 caffe.cpp:313] Batch 58, loss = 0.685973
I0523 16:25:47.307816 2712679360 caffe.cpp:313] Batch 59, accuracy = 0.74
I0523 16:25:47.307848 2712679360 caffe.cpp:313] Batch 59, loss = 0.815847
I0523 16:25:47.409512 2712679360 caffe.cpp:313] Batch 60, accuracy = 0.79
I0523 16:25:47.409544 2712679360 caffe.cpp:313] Batch 60, loss = 0.694609
I0523 16:25:47.509786 2712679360 caffe.cpp:313] Batch 61, accuracy = 0.72
I0523 16:25:47.509819 2712679360 caffe.cpp:313] Batch 61, loss = 0.721049
I0523 16:25:47.608265 2712679360 caffe.cpp:313] Batch 62, accuracy = 0.76
I0523 16:25:47.608304 2712679360 caffe.cpp:313] Batch 62, loss = 0.649006
I0523 16:25:47.711271 2712679360 caffe.cpp:313] Batch 63, accuracy = 0.77
I0523 16:25:47.711302 2712679360 caffe.cpp:313] Batch 63, loss = 0.620039
I0523 16:25:47.812440 2712679360 caffe.cpp:313] Batch 64, accuracy = 0.71
I0523 16:25:47.812471 2712679360 caffe.cpp:313] Batch 64, loss = 0.706689
I0523 16:25:47.911661 2712679360 caffe.cpp:313] Batch 65, accuracy = 0.77
I0523 16:25:47.911694 2712679360 caffe.cpp:313] Batch 65, loss = 0.824431
I0523 16:25:48.011318 2712679360 caffe.cpp:313] Batch 66, accuracy = 0.73
I0523 16:25:48.011351 2712679360 caffe.cpp:313] Batch 66, loss = 0.739382
I0523 16:25:48.117573 2712679360 caffe.cpp:313] Batch 67, accuracy = 0.7
I0523 16:25:48.117606 2712679360 caffe.cpp:313] Batch 67, loss = 0.800725
I0523 16:25:48.214515 2712679360 caffe.cpp:313] Batch 68, accuracy = 0.68
I0523 16:25:48.214545 2712679360 caffe.cpp:313] Batch 68, loss = 0.807705
I0523 16:25:48.314254 2712679360 caffe.cpp:313] Batch 69, accuracy = 0.7
I0523 16:25:48.314283 2712679360 caffe.cpp:313] Batch 69, loss = 0.952385
I0523 16:25:48.412657 2712679360 caffe.cpp:313] Batch 70, accuracy = 0.74
I0523 16:25:48.412686 2712679360 caffe.cpp:313] Batch 70, loss = 0.781932
I0523 16:25:48.512931 2712679360 caffe.cpp:313] Batch 71, accuracy = 0.73
I0523 16:25:48.512964 2712679360 caffe.cpp:313] Batch 71, loss = 0.895561
I0523 16:25:48.608669 2712679360 caffe.cpp:313] Batch 72, accuracy = 0.8
I0523 16:25:48.608700 2712679360 caffe.cpp:313] Batch 72, loss = 0.615967
I0523 16:25:48.705847 2712679360 caffe.cpp:313] Batch 73, accuracy = 0.78
I0523 16:25:48.705878 2712679360 caffe.cpp:313] Batch 73, loss = 0.588951
I0523 16:25:48.803540 2712679360 caffe.cpp:313] Batch 74, accuracy = 0.72
I0523 16:25:48.803591 2712679360 caffe.cpp:313] Batch 74, loss = 0.784208
I0523 16:25:48.906528 2712679360 caffe.cpp:313] Batch 75, accuracy = 0.77
I0523 16:25:48.906565 2712679360 caffe.cpp:313] Batch 75, loss = 0.529825
I0523 16:25:49.007186 2712679360 caffe.cpp:313] Batch 76, accuracy = 0.77
I0523 16:25:49.007216 2712679360 caffe.cpp:313] Batch 76, loss = 0.794115
I0523 16:25:49.107000 2712679360 caffe.cpp:313] Batch 77, accuracy = 0.76
I0523 16:25:49.107033 2712679360 caffe.cpp:313] Batch 77, loss = 0.726804
I0523 16:25:49.205263 2712679360 caffe.cpp:313] Batch 78, accuracy = 0.77
I0523 16:25:49.205294 2712679360 caffe.cpp:313] Batch 78, loss = 0.919712
I0523 16:25:49.304277 2712679360 caffe.cpp:313] Batch 79, accuracy = 0.69
I0523 16:25:49.304309 2712679360 caffe.cpp:313] Batch 79, loss = 0.87618
I0523 16:25:49.404642 2712679360 caffe.cpp:313] Batch 80, accuracy = 0.77
I0523 16:25:49.404672 2712679360 caffe.cpp:313] Batch 80, loss = 0.704637
I0523 16:25:49.501708 2712679360 caffe.cpp:313] Batch 81, accuracy = 0.75
I0523 16:25:49.501739 2712679360 caffe.cpp:313] Batch 81, loss = 0.71787
I0523 16:25:49.599267 2712679360 caffe.cpp:313] Batch 82, accuracy = 0.76
I0523 16:25:49.599304 2712679360 caffe.cpp:313] Batch 82, loss = 0.613339
I0523 16:25:49.698971 2712679360 caffe.cpp:313] Batch 83, accuracy = 0.78
I0523 16:25:49.699002 2712679360 caffe.cpp:313] Batch 83, loss = 0.689216
I0523 16:25:49.803320 2712679360 caffe.cpp:313] Batch 84, accuracy = 0.72
I0523 16:25:49.803352 2712679360 caffe.cpp:313] Batch 84, loss = 0.817351
I0523 16:25:49.904433 2712679360 caffe.cpp:313] Batch 85, accuracy = 0.78
I0523 16:25:49.904467 2712679360 caffe.cpp:313] Batch 85, loss = 0.62069
I0523 16:25:50.005846 2712679360 caffe.cpp:313] Batch 86, accuracy = 0.75
I0523 16:25:50.005878 2712679360 caffe.cpp:313] Batch 86, loss = 0.680651
I0523 16:25:50.103121 2712679360 caffe.cpp:313] Batch 87, accuracy = 0.78
I0523 16:25:50.103153 2712679360 caffe.cpp:313] Batch 87, loss = 0.788875
I0523 16:25:50.200103 2712679360 caffe.cpp:313] Batch 88, accuracy = 0.8
I0523 16:25:50.200134 2712679360 caffe.cpp:313] Batch 88, loss = 0.620548
I0523 16:25:50.299957 2712679360 caffe.cpp:313] Batch 89, accuracy = 0.74
I0523 16:25:50.299989 2712679360 caffe.cpp:313] Batch 89, loss = 0.779962
I0523 16:25:50.399699 2712679360 caffe.cpp:313] Batch 90, accuracy = 0.75
I0523 16:25:50.399731 2712679360 caffe.cpp:313] Batch 90, loss = 0.70084
I0523 16:25:50.502117 2712679360 caffe.cpp:313] Batch 91, accuracy = 0.79
I0523 16:25:50.502148 2712679360 caffe.cpp:313] Batch 91, loss = 0.576651
I0523 16:25:50.599150 2712679360 caffe.cpp:313] Batch 92, accuracy = 0.71
I0523 16:25:50.599181 2712679360 caffe.cpp:313] Batch 92, loss = 0.9778
I0523 16:25:50.699782 2712679360 caffe.cpp:313] Batch 93, accuracy = 0.78
I0523 16:25:50.699813 2712679360 caffe.cpp:313] Batch 93, loss = 0.795732
I0523 16:25:50.802847 2712679360 caffe.cpp:313] Batch 94, accuracy = 0.77
I0523 16:25:50.802877 2712679360 caffe.cpp:313] Batch 94, loss = 0.803904
I0523 16:25:50.900668 2712679360 caffe.cpp:313] Batch 95, accuracy = 0.77
I0523 16:25:50.900702 2712679360 caffe.cpp:313] Batch 95, loss = 0.664654
I0523 16:25:50.902439 102174720 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:25:50.999625 2712679360 caffe.cpp:313] Batch 96, accuracy = 0.74
I0523 16:25:50.999656 2712679360 caffe.cpp:313] Batch 96, loss = 0.700099
I0523 16:25:51.100697 2712679360 caffe.cpp:313] Batch 97, accuracy = 0.66
I0523 16:25:51.100728 2712679360 caffe.cpp:313] Batch 97, loss = 0.937044
I0523 16:25:51.201591 2712679360 caffe.cpp:313] Batch 98, accuracy = 0.79
I0523 16:25:51.201622 2712679360 caffe.cpp:313] Batch 98, loss = 0.677679
I0523 16:25:51.299702 2712679360 caffe.cpp:313] Batch 99, accuracy = 0.76
I0523 16:25:51.299736 2712679360 caffe.cpp:313] Batch 99, loss = 0.687144
I0523 16:25:51.299741 2712679360 caffe.cpp:318] Loss: 0.742307
I0523 16:25:51.299762 2712679360 caffe.cpp:330] accuracy = 0.754
I0523 16:25:51.299773 2712679360 caffe.cpp:330] loss = 0.742307 (* 1 = 0.742307 loss)
</code></pre>

<p>å¾—åˆ°æœ€ç»ˆçš„æµ‹è¯•é›†å‡†ç¡®ç‡å¯ä»¥åˆ°è¾¾<code>accuracy = 0.754</code></p>

<p>åˆ°è¿™é‡Œæˆ‘ä»¬å¯¹äºç»ƒä¹  cifar10æ¨¡å‹<br/>
å°±ç»“æŸäº†.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Struct]]></title>
    <link href="https://lockxmonk.github.io/14955029237844.html"/>
    <updated>2017-05-23T09:28:43+08:00</updated>
    <id>https://lockxmonk.github.io/14955029237844.html</id>
    <content type="html"><![CDATA[
<p>å‡†ç¡®åœ°è®²ï¼ŒPythonæ²¡æœ‰ä¸“é—¨å¤„ç†å­—èŠ‚çš„æ•°æ®ç±»å‹ã€‚ä½†ç”±äº<code>str</code>æ—¢æ˜¯å­—ç¬¦ä¸²ï¼Œåˆå¯ä»¥è¡¨ç¤ºå­—èŠ‚ï¼Œæ‰€ä»¥ï¼Œå­—èŠ‚æ•°ç»„ï¼strã€‚è€Œåœ¨Cè¯­è¨€ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿åœ°ç”¨structã€unionæ¥å¤„ç†å­—èŠ‚ï¼Œä»¥åŠå­—èŠ‚å’Œintï¼Œfloatçš„è½¬æ¢ã€‚</p>

<p>åœ¨Pythonä¸­ï¼Œæ¯”æ–¹è¯´è¦æŠŠä¸€ä¸ª32ä½æ— ç¬¦å·æ•´æ•°å˜æˆå­—èŠ‚ï¼Œä¹Ÿå°±æ˜¯4ä¸ªé•¿åº¦çš„strï¼Œä½ å¾—é…åˆä½è¿ç®—ç¬¦æ¥å®Œæˆ.éå¸¸éº»çƒ¦ä¸åˆ©äºæ•ˆç‡å¼€å‘.</p>

<p>å¥½åœ¨Pythonæä¾›äº†ä¸€ä¸ª<code>struct</code>æ¨¡å—æ¥è§£å†³strå’Œå…¶ä»–äºŒè¿›åˆ¶æ•°æ®ç±»å‹çš„è½¬æ¢ã€‚</p>

<p><code>structçš„pack</code>å‡½æ•°æŠŠä»»æ„æ•°æ®ç±»å‹å˜æˆå­—ç¬¦ä¸²ï¼š</p>

<pre><code class="language-py">&gt;&gt;&gt; import struct
&gt;&gt;&gt; struct.pack(&#39;&gt;I&#39;,10240099)
&#39;\x00\x9c@c&#39;
&gt;&gt;&gt;
</code></pre>

<p><code>pack</code>çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¤„ç†æŒ‡ä»¤ï¼Œ<code>&#39;&gt;I&#39;</code>çš„æ„æ€æ˜¯ï¼š</p>

<p><strong><code>&gt;</code>è¡¨ç¤ºå­—èŠ‚é¡ºåºæ˜¯<code>big-endian</code>ï¼Œä¹Ÿå°±æ˜¯ç½‘ç»œåºï¼Œ<code>I</code>è¡¨ç¤º4å­—èŠ‚æ— ç¬¦å·æ•´æ•°ã€‚</strong></p>

<p>åé¢çš„å‚æ•°ä¸ªæ•°è¦å’Œå¤„ç†æŒ‡ä»¤ä¸€è‡´ã€‚</p>

<p><code>unpack</code>æŠŠ<code>str</code>å˜æˆç›¸åº”çš„æ•°æ®ç±»å‹ï¼š</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import struct
print struct.unpack(&#39;&gt;IH&#39;,&#39;\xf0\xf0\xf0\xf0\x80\x80&#39;)
</code></pre>

<p><img src="media/14955029237844/14955050577583.jpg" alt=""/></p>

<p>æ ¹æ®&gt;IHçš„è¯´æ˜ï¼Œåé¢çš„strä¾æ¬¡å˜ä¸ºIï¼š4å­—èŠ‚æ— ç¬¦å·æ•´æ•°å’ŒHï¼š2å­—èŠ‚æ— ç¬¦å·æ•´æ•°ã€‚</p>

<p>æ‰€ä»¥ï¼Œå°½ç®¡Pythonä¸é€‚åˆç¼–å†™åº•å±‚æ“ä½œå­—èŠ‚æµçš„ä»£ç ï¼Œä½†åœ¨å¯¹æ€§èƒ½è¦æ±‚ä¸é«˜çš„åœ°æ–¹ï¼Œåˆ©ç”¨<code>struct</code>å°±æ–¹ä¾¿å¤šäº†ã€‚</p>

<p><code>struct</code>æ¨¡å—å®šä¹‰çš„æ•°æ®ç±»å‹å¯ä»¥å‚è€ƒPythonå®˜æ–¹æ–‡æ¡£ï¼š<a href="https://docs.python.org/2/library/struct.html#format-characters">https://docs.python.org/2/library/struct.html#format-characters</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Base64]]></title>
    <link href="https://lockxmonk.github.io/14955000885655.html"/>
    <updated>2017-05-23T08:41:28+08:00</updated>
    <id>https://lockxmonk.github.io/14955000885655.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">ç»ƒä¹ :</a>
</li>
<li>
<a href="#toc_1">å°ç»“</a>
</li>
</ul>


<p>Base64æ˜¯ä¸€ç§ç”¨64ä¸ªå­—ç¬¦æ¥è¡¨ç¤ºä»»æ„äºŒè¿›åˆ¶æ•°æ®çš„æ–¹æ³•ã€‚</p>

<p>ç”¨è®°äº‹æœ¬æ‰“å¼€<code>exeã€jpgã€pdf</code>è¿™äº›æ–‡ä»¶æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šçœ‹åˆ°ä¸€å¤§å †ä¹±ç ï¼Œå› ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶åŒ…å«å¾ˆå¤šæ— æ³•æ˜¾ç¤ºå’Œæ‰“å°çš„å­—ç¬¦ï¼Œæ‰€ä»¥ï¼Œå¦‚æœè¦è®©è®°äº‹æœ¬è¿™æ ·çš„æ–‡æœ¬å¤„ç†è½¯ä»¶èƒ½å¤„ç†äºŒè¿›åˆ¶æ•°æ®ï¼Œå°±éœ€è¦ä¸€ä¸ªäºŒè¿›åˆ¶åˆ°å­—ç¬¦ä¸²çš„è½¬æ¢æ–¹æ³•ã€‚Base64æ˜¯ä¸€ç§æœ€å¸¸è§çš„äºŒè¿›åˆ¶ç¼–ç æ–¹æ³•ã€‚</p>

<p>Base64çš„åŸç†å¾ˆç®€å•ï¼Œé¦–å…ˆï¼Œå‡†å¤‡ä¸€ä¸ªåŒ…å«64ä¸ªå­—ç¬¦çš„æ•°ç»„ï¼š</p>

<pre><code>[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, ... &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, ... &#39;0&#39;, &#39;1&#39;, ... &#39;+&#39;, &#39;/&#39;]
</code></pre>

<p>ç„¶åï¼Œå¯¹äºŒè¿›åˆ¶æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ¯3ä¸ªå­—èŠ‚ä¸€ç»„ï¼Œä¸€å…±æ˜¯<code>3x8=24bit</code>ï¼Œåˆ’ä¸º4ç»„ï¼Œæ¯ç»„æ­£å¥½6ä¸ªbitï¼š<img src="media/14955000885655/14955010424104.jpg" alt=""/></p>

<p>è¿™æ ·æˆ‘ä»¬å¾—åˆ°4ä¸ªæ•°å­—ä½œä¸ºç´¢å¼•ï¼Œç„¶åæŸ¥è¡¨ï¼Œè·å¾—ç›¸åº”çš„4ä¸ªå­—ç¬¦ï¼Œå°±æ˜¯ç¼–ç åçš„å­—ç¬¦ä¸²ã€‚</p>

<p>æ‰€ä»¥ï¼ŒBase64ç¼–ç ä¼šæŠŠ3å­—èŠ‚çš„äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸º4å­—èŠ‚çš„æ–‡æœ¬æ•°æ®ï¼Œé•¿åº¦å¢åŠ 33%ï¼Œå¥½å¤„æ˜¯ç¼–ç åçš„æ–‡æœ¬æ•°æ®å¯ä»¥åœ¨é‚®ä»¶æ­£æ–‡ã€ç½‘é¡µç­‰ç›´æ¥æ˜¾ç¤ºã€‚</p>

<p>å¦‚æœè¦ç¼–ç çš„äºŒè¿›åˆ¶æ•°æ®ä¸æ˜¯3çš„å€æ•°ï¼Œæœ€åä¼šå‰©ä¸‹1ä¸ªæˆ–2ä¸ªå­—èŠ‚æ€ä¹ˆåŠï¼ŸBase64ç”¨<code>\x00</code>å­—èŠ‚åœ¨æœ«å°¾è¡¥è¶³åï¼Œå†åœ¨ç¼–ç çš„æœ«å°¾åŠ ä¸Š1ä¸ªæˆ–2ä¸ª<code>=</code>å·ï¼Œè¡¨ç¤ºè¡¥äº†å¤šå°‘å­—èŠ‚ï¼Œè§£ç çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨å»æ‰ã€‚</p>

<p>Pythonå†…ç½®çš„<code>base64</code>å¯ä»¥ç›´æ¥è¿›è¡Œbase64çš„ç¼–è§£ç ï¼š</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import base64
print base64.b64encode(&#39;binary\x00string&#39;)
print base64.b64decode(&#39;YmluYXJ5AHN0cmluZw==&#39;)
</code></pre>

<p><img src="media/14955000885655/14955014556004.jpg" alt=""/></p>

<p>ç”±äºæ ‡å‡†çš„Base64ç¼–ç åå¯èƒ½å‡ºç°å­—ç¬¦<code>+</code>å’Œ<code>/</code>ï¼Œåœ¨URLä¸­å°±ä¸èƒ½ç›´æ¥ä½œä¸ºå‚æ•°ï¼Œæ‰€ä»¥åˆæœ‰ä¸€ç§&quot;url safe&quot;çš„base64ç¼–ç ï¼Œå…¶å®å°±æ˜¯æŠŠå­—ç¬¦<code>+</code>å’Œ<code>/</code>åˆ†åˆ«å˜æˆ<code>-</code>å’Œ<code>_</code>ï¼š<br/>
<img src="media/14955000885655/14955018446875.jpg" alt=""/><br/>
è¿˜å¯ä»¥è‡ªå·±å®šä¹‰64ä¸ªå­—ç¬¦çš„æ’åˆ—é¡ºåºï¼Œè¿™æ ·å°±å¯ä»¥è‡ªå®šä¹‰Base64ç¼–ç ï¼Œä¸è¿‡ï¼Œé€šå¸¸æƒ…å†µä¸‹å®Œå…¨æ²¡æœ‰å¿…è¦ã€‚</p>

<p>Base64æ˜¯ä¸€ç§é€šè¿‡æŸ¥è¡¨çš„ç¼–ç æ–¹æ³•ï¼Œä¸èƒ½ç”¨äºåŠ å¯†ï¼Œå³ä½¿ä½¿ç”¨è‡ªå®šä¹‰çš„ç¼–ç è¡¨ä¹Ÿä¸è¡Œã€‚</p>

<p>Base64é€‚ç”¨äºå°æ®µå†…å®¹çš„ç¼–ç ï¼Œæ¯”å¦‚æ•°å­—è¯ä¹¦ç­¾åã€Cookieçš„å†…å®¹ç­‰ã€‚</p>

<p>ç”±äº<code>=</code>å­—ç¬¦ä¹Ÿå¯èƒ½å‡ºç°åœ¨Base64ç¼–ç ä¸­ï¼Œä½†<code>=</code>ç”¨åœ¨URLã€Cookieé‡Œé¢ä¼šé€ æˆæ­§ä¹‰ï¼Œæ‰€ä»¥ï¼Œå¾ˆå¤šBase64ç¼–ç åä¼šæŠŠ<code>=</code>å»æ‰ï¼š</p>

<pre><code># æ ‡å‡†Base64:
&#39;abcd&#39; -&gt; &#39;YWJjZA==&#39;
# è‡ªåŠ¨å»æ‰=:
&#39;abcd&#39; -&gt; &#39;YWJjZA&#39;
</code></pre>

<p>å»æ‰=åæ€ä¹ˆè§£ç å‘¢ï¼Ÿå› ä¸ºBase64æ˜¯æŠŠ3ä¸ªå­—èŠ‚å˜ä¸º4ä¸ªå­—èŠ‚ï¼Œæ‰€ä»¥ï¼ŒBase64ç¼–ç çš„é•¿åº¦æ°¸è¿œæ˜¯4çš„å€æ•°ï¼Œå› æ­¤ï¼Œéœ€è¦åŠ ä¸Š=æŠŠBase64å­—ç¬¦ä¸²çš„é•¿åº¦å˜ä¸º4çš„å€æ•°ï¼Œå°±å¯ä»¥æ­£å¸¸è§£ç äº†ã€‚</p>

<h2 id="toc_0">ç»ƒä¹ :</h2>

<p>è¯·å†™ä¸€ä¸ªèƒ½å»æ‰<code>=</code>çš„base64è§£ç å‡½æ•°:</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import base64
def safe_b64decode(str):
    num = len(str)%4
    if num==0:
        return base64.b64decode(str)
    else :
        for x in range(num):
             str = str + &#39;=&#39;
        return str 
print base64.b64decode(&#39;YWJjZA==&#39;)
#print base64.b64decode(&#39;YWJjZA&#39;)
print safe_b64decode(&#39;YWJjZA&#39;)
</code></pre>

<p><img src="media/14955000885655/14955026953083.jpg" alt=""/></p>

<h2 id="toc_1">å°ç»“</h2>

<p>Base64æ˜¯ä¸€ç§ä»»æ„äºŒè¿›åˆ¶åˆ°æ–‡æœ¬å­—ç¬¦ä¸²çš„ç¼–ç æ–¹æ³•ï¼Œå¸¸ç”¨äºåœ¨URLã€Cookieã€ç½‘é¡µä¸­ä¼ è¾“å°‘é‡äºŒè¿›åˆ¶æ•°æ®ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ä¸‹å­¦æœŸè‹±è¯­è¯¾åç¿»è¯‘æ€»ç»“]]></title>
    <link href="https://lockxmonk.github.io/14948310433460.html"/>
    <updated>2017-05-15T14:50:43+08:00</updated>
    <id>https://lockxmonk.github.io/14948310433460.html</id>
    <content type="html"><![CDATA[
<p>æ®è¯´è¿™äº›å·²ç»è¢«æ”¶é›†çš„çº¸æ¡å¤§çº¦æœ‰ä¸¤ç™¾ä¸‡ä»½ï¼Œç²—ç•¥åˆ†ç±»åæ†ç»‘åˆ°ä¸€èµ·ï¼Œè¿™äº›çº¸æ¡æ— ç–‘å¸ƒæ»¡ç°å°˜(å¹¶ä¸”èµ·çº¿å¤´)ï¼Œæœ‰çš„ç”šè‡³å·²ç»å·æ›²å‘é»„ï¼Œè¿˜æœ‰çš„å› ä¸ºå¹´ä¹…å„¿ç ´ç¢ã€‚</p>

<p>é—®é¢˜ä¹‹ä¸€æ˜¯ï¼šå¿—æ„¿é˜…è¯»è€…å¯¹äºæ‰€è°“â€œæ™®é€šè¯è¯­â€çš„çƒ­æƒ…å…³æ³¨å‘æ¥å°±ä¸é«˜ï¼›ä»–ä»¬æ€»æ˜¯å—åˆ°æŸç§è¯±æƒ‘åŠ›çš„å¸å¼•ï¼Œå¯„æ¥ä¸€äº›ä»¤äººæ„Ÿå…´è¶£çš„è¯æ±‡çš„å¼•è¯­ï¼Œè€Œä¸æ˜¯æ—¥å¸¸è¯æ±‡çš„å¼•è¯­ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[åœ¨RNä¸­åˆ©ç”¨åŸç”ŸDocument Interaction Controlleræ¥é¢„è§ˆå’Œæ‰“å¼€æ–‡æ¡£]]></title>
    <link href="https://lockxmonk.github.io/14948283904445.html"/>
    <updated>2017-05-15T14:06:30+08:00</updated>
    <id>https://lockxmonk.github.io/14948283904445.html</id>
    <content type="html"><![CDATA[
<p>åœ¨react-nativeå¼€å‘ä¸­æœ‰ä¸€ä¸ªåŠŸèƒ½éœ€æ±‚ä¸ºï¼Œæ‰“å¼€ä¹‹å‰å·²ç»ä¸‹è½½åˆ°Documentsä¸­çš„æ–‡ä»¶ã€‚ä¸€å¼€å§‹æœ¬æƒ³ç”¨webviewæ¥åŠ è½½æ–‡ä»¶ã€‚åæ¥å‘ç°æœ‰<code>UIDocumentInteractionController</code>è¿™ä¸ªapiæ¥æ›´å¥½çš„å®Œæˆè¿™ä¸ªåŠŸèƒ½ã€‚</p>

<p>é¦–å…ˆåœ¨RNå¼•ç”¨çš„ç±»ä¸­åŠ å…¥<code>UIDocumentInteractionControllerDelegate</code>æˆ‘ä»¬è¦ç”¨åˆ°è¿™ä¸ªæ¥å‘ˆç°é¢„è§ˆè§†å›¾ï¼š</p>

<p>NativeUtil.h<br/>
```oc</p>

<h1 id="toc_0">import <Foundation/Foundation.h></h1>

<h1 id="toc_1">import &quot;RCTBridgeModule.h&quot;</h1>

<h1 id="toc_2">import &quot;UIView+React.h&quot;       //è¦å¼•å…¥è¿™ä¸ªå¤´æ–‡ä»¶ï¼Œåœ¨rnä¸­åˆ›å»ºå’ŒåŠ è½½åŸå£°è§†å›¾</h1>

<p>@interface NativeUtil : NSObject <RCTBridgeModule,UIDocumentInteractionControllerDelegate></p>

<p>@end<br/>
```</p>

<p>åœ¨NativeUtil.mä¸­å®ç°è¯¥å§”æ‰˜ã€‚å®šä¹‰ä¸€ä¸ªæš´éœ²ç»™RNçš„æ–¹æ³•ï¼Œåœ¨jsä¸­è°ƒç”¨ï¼š</p>

<pre><code class="language-oc">RCT_EXPORT_METHOD(ShowFileChooser: (RCTResponseSenderBlock)callback){
  
    NSString *filePath = @&quot;ceshi007&quot;;           callback(@[[NSNull null],filePath]);    //æµ‹è¯•callbackï¼Œä»nativeå‘rnä¼ å€¼ã€‚


    NSArray *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);           //è·å–åº”ç”¨æœ¬æ²™ç›’å†…çš„ç›®å½•ã€‚
    NSString *documentpath = paths[0];      //è·å–documentçš„è·¯å¾„
    NSString *fileName = [documentpath stringByAppendingPathComponent:@&quot;Screenshot_2016-12-15-21-29-07-50.png&quot;];            //æŒ‡å®šä¸Šè¿°å›¾ç‰‡çš„è·¯å¾„ã€‚
  NSURL *fileURL = [NSURL fileURLWithPath:fileName];
  dispatch_async(dispatch_get_main_queue(), ^{              //è¿è¡Œä¸»çº¿ç¨‹æ¥åŠ è½½ç•Œé¢ï¼Œå¹¶åˆå§‹åŒ–UIDocumentInteractionController
    UIDocumentInteractionController *documentInteractionController = [UIDocumentInteractionController interactionControllerWithURL:fileURL];
    documentInteractionController.delegate =self ;
    [documentInteractionController presentPreviewAnimated:YES];             //presentå‡ºæ¥æ–‡ä»¶çš„é¢„è§ˆç•Œé¢ã€‚
  });
  
}
</code></pre>

<p>ä¹‹åå®šä¹‰<code>(UIViewController *)documentInteractionControllerViewControllerForPreview:(UIDocumentInteractionController *)controller</code>æ–¹æ³•ï¼Œè¦åŠ è½½å‡ºæ¥é¢„è§ˆçš„ç•Œé¢ï¼Œä¸Šè¿°æ–¹æ³•å¿…é¡»å®ç°ï¼Œä¸”è¿”å›ä¸€ä¸ªå½“å‰çš„é¡µé¢çš„ViewControllerã€‚ç”¨äºä½œä¸ºé¢„è§ˆè§†å›¾çš„çˆ¶ViewControlleræ¥å¼¹å‡ºmodalã€‚</p>

<pre><code class="language-oc">- (UIViewController *)documentInteractionControllerViewControllerForPreview:(UIDocumentInteractionController *)controller{
  UIViewController *newVC =[self getPresentedViewController];  //è·å–å½“å‰å±å¹•ä¸­presentå‡ºæ¥çš„viewcontrollerã€‚
   return newVC;
};
</code></pre>

<p>è¿™é‡Œæˆ‘ä»¬æ˜¯åœ¨ä¸€ä¸ªéè§†å›¾ç±»åˆ›å»ºå¹¶åœ¨åŠ è½½ä¸€ä¸ªè§†å›¾ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å…ˆè·å–åˆ°å½“å‰ç•Œé¢çš„<code>ViewController</code>ï¼Œå°†è¢«å±•ç¤ºçš„<code>view</code>åŠ åˆ°å½“å‰<code>view</code>çš„å­è§†å›¾ï¼Œæˆ–ç”¨å½“å‰view <code>presentViewController</code>ï¼Œæˆ–<code>pushViewContrller</code>ã€‚è¿™é‡Œæˆ‘ä»ç½‘ä¸Šæ‰¾åˆ°äº†ä¸¤ä¸ªæ–¹æ³•ï¼š</p>

<pre><code class="language-oc">//è·å–å½“å‰å±å¹•æ˜¾ç¤ºçš„viewcontroller
- (UIViewController *)getCurrentVC  
{  
    UIViewController *result = nil;  
      
    UIWindow * window = [[UIApplication sharedApplication] keyWindow];  
    if (window.windowLevel != UIWindowLevelNormal)  
    {  
        NSArray *windows = [[UIApplication sharedApplication] windows];  
        for(UIWindow * tmpWin in windows)  
        {  
            if (tmpWin.windowLevel == UIWindowLevelNormal)  
            {  
                window = tmpWin;  
                break;  
            }  
        }  
    }  
      
    UIView *frontView = [[window subviews] objectAtIndex:0];  
    id nextResponder = [frontView nextResponder];  
      
    if ([nextResponder isKindOfClass:[UIViewController class]])  
        result = nextResponder;  
    else  
        result = window.rootViewController;  
      
    return result;  
} 
</code></pre>

<pre><code class="language-oc">//è·å–å½“å‰å±å¹•ä¸­presentå‡ºæ¥çš„viewcontroller
- (UIViewController *)getPresentedViewController  
{  
    UIViewController *appRootVC = [UIApplication sharedApplication].keyWindow.rootViewController;  
    UIViewController *topVC = appRootVC;  
    if (topVC.presentedViewController) {  
        topVC = topVC.presentedViewController;  
    }  
      
    return topVC;  
}
</code></pre>

<blockquote>
<p>åœ¨RNä¸­ï¼Œä½¿ç”¨modalç»„ä»¶ä¹‹åï¼Œå¼¹å‡ºçš„modalè§†å›¾çš„viewcontrollerç›¸å½“äºpresentå‡ºæ¥çš„viewcontroller.</p>
</blockquote>

<p>ä¹‹åä½¿ç”¨<code>[self getPresentedViewController]</code>å°±å¯ä»¥è·å¾—<code>viewcontroller</code>æ¥åŠ è½½è§†å›¾ã€‚</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[caffeæ¡†æ¶è¿è¡Œæ‰‹å†™ä½“æ•°å­—è¯†åˆ«ä¾‹å­ï¼ˆMNISTï¼‰]]></title>
    <link href="https://lockxmonk.github.io/14946429015704.html"/>
    <updated>2017-05-13T10:35:01+08:00</updated>
    <id>https://lockxmonk.github.io/14946429015704.html</id>
    <content type="html"><![CDATA[
<p>å…³äºcaffeç¯å¢ƒçš„æ­å»ºæš‚æ—¶ä¸åšè®¨è®ºï¼Œä¹‹åæœ‰æ—¶é—´æ•´ç†ä¸€ä¸‹ï¼ˆmacç³»ç»Ÿä¸Šcpu-onlyçš„caffeç¯å¢ƒæ­å»ºï¼‰ã€‚</p>

<h2 id="toc_0">LeNet-5æ¨¡å‹æè¿°</h2>

<p>caffeæ¡†æ¶ä¸­ç»™çš„LeNet-5æ¨¡å‹ä¸åŸç‰ˆæœ‰æ‰€ä¸åŒï¼Œå…¶ä¸­å°†Sigmoidæ¿€æ´»å‡½æ•°æ¢æˆäº†ReLuï¼Œå®ƒçš„æè¿°æ–‡ä»¶ä¸º<code>examples/mnist/lenet_train_test.prototxt</code><br/>
ï¼Œå®ƒçš„å†…å®¹ä¸»è¦ä¸ºï¼š</p>

<pre><code>name: &quot;LeNet&quot;           //ç½‘ç»œï¼ˆNetï¼‰çš„åç§°ä¸ºLeNet
layer {                 //å®šä¹‰ä¸€ä¸ªå±‚ï¼ˆLayerï¼‰
  name: &quot;mnist&quot;         //å±‚çš„åç§°ä¸ºmnist
  type: &quot;Data&quot;          //å±‚çš„ç±»å‹ä¸ºæ•°æ®å±‚
  top: &quot;data&quot;           //å±‚çš„è¾“å‡ºblobæœ‰ä¸¤ä¸ªï¼šdataå’Œlabel
  top: &quot;label&quot;
  include {
    phase: TRAIN        //è¡¨æ˜è¯¥å±‚å‚æ•°åªåœ¨è®­ç»ƒé˜¶æ®µæœ‰æ•ˆ
  }
  transform_param {
    scale: 0.00390625   //æ•°æ®å˜æ¢ä½¿ç”¨çš„æ•°æ®ç¼©æ”¾å› å­
  }
  data_param {          //æ•°æ®å±‚å‚æ•°
    source: &quot;examples/mnist/mnist_train_lmdb&quot;       //LMDBçš„è·¯å¾„
    batch_size: 64      //æ‰¹é‡æ•°ç›®ï¼Œä¸€æ¬¡æ€§è¯»å–64å¼ å›¾
    backend: LMDB       //æ•°æ®æ ¼å¼ä¸ºLMDB
  }
}
layer {                 //ä¸€ä¸ªæ–°çš„æ•°æ®å±‚ï¼Œåå­—ä¹Ÿå«mnistï¼Œè¾“å‡ºblobä¹Ÿæ˜¯dataå’ŒLabelï¼Œä½†æ˜¯è¿™é‡Œå®šä¹‰çš„å‚æ•°åªåœ¨åˆ†ç±»é˜¶æ®µæœ‰æ•ˆ
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST         //è¡¨æ˜åªåœ¨æµ‹è¯•åˆ†ç±»é˜¶æ®µæœ‰æ•ˆ
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;         //å®šä¹‰ä¸€ä¸ªæ–°çš„å·ç§¯å±‚conv1ï¼Œè¾“å…¥blobä¸ºdataï¼Œè¾“å‡ºblobä¸ºconv1
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1          //æƒå€¼å­¦ä¹ é€Ÿç‡å€ä¹˜å› å­ï¼Œ1è““è¡¨ç¤ºä¸å…¨å±€å‚æ•°ä¸€è‡´
  }
  param {
    lr_mult: 2          //biaså­¦ä¹ é€Ÿç‡å€ä¹˜å› å­ï¼Œæ˜¯å…¨å±€å‚æ•°çš„ä¸¤å€
  }
  convolution_param {   //å·ç§¯è®¡ç®—å‚æ•°
    num_output: 20      //è¾“å‡ºfeature mapæ•°ç›®ä¸º20
    kernel_size: 5      //å·ç§¯æ ¸å°ºå¯¸ï¼Œ5*5
    stride: 1           //å·ç§¯è¾“å‡ºè·³è·ƒé—´éš”ï¼Œ1è¡¨ç¤ºè¿ç»­è¾“å‡ºï¼Œæ— è·³è·ƒ
    weight_filler {     //æƒå€¼ä½¿ç”¨Xavierå¡«å……å™¨
      type: &quot;xavier&quot;
    }
    bias_filler {       //biasä½¿ç”¨å¸¸ç†Ÿå¡«å……å™¨ï¼Œé»˜è®¤ä¸º0
      type: &quot;constant&quot;
    }
  }
}
layer {                 //å®šä¹‰æ–°çš„ä¸‹é‡‡æ ·å±‚pool1ï¼Œè¾“å…¥blobä¸ºconv1ï¼Œè¾“å‡ºblobä¸ºpool1
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {       //ä¸‹é‡‡æ ·å‚æ•°
    pool: MAX           //ä½¿ç”¨æœ€å¤§å€¼ä¸‹é‡‡æ ·æ–¹æ³•
    kernel_size: 2      //ä¸‹é‡‡æ ·çª—å£å°ºå¯¸ä¸º2*2
    stride: 2           //ä¸‹é‡‡æ ·è¾“å‡ºè·³è·ƒé—´éš”2*2
  }
}   
layer {                 //æ–°çš„å·ç§¯å±‚ï¼Œå’Œconv1ç±»ä¼¼
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //æ–°çš„ä¸‹é‡‡æ ·å±‚ï¼Œå’Œpool1ç±»ä¼¼
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {                 //æ–°çš„å…¨è¿æ¥å±‚ï¼Œè¾“å…¥blobä¸ºpool2ï¼Œè¾“å‡ºblobä¸ºip1
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {     //å…¨è¿æ¥å±‚å‚æ•°
    num_output: 500         //è¯¥å±‚è¾“å‡ºå…ƒç´ ä¸ªæ•°ä¸º500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //æ–°çš„éçº¿æ€§å±‚ï¼Œç”¨ReLUæ–¹æ³•
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //åˆ†ç±»å‡†ç¡®ç‡å±‚ï¼Œè¾“å…¥blobä¸ºip2å’ŒLabelï¼Œè¾“å‡ºblobä¸ºaccuracyï¼Œè¯¥å±‚ç”¨äºè®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {             //æŸå¤±å±‚ï¼ŒæŸå¤±å‡½æ•°SoftmaxLossï¼Œè¾“å…¥blobä¸ºip2å’Œlabelï¼Œè¾“å‡ºblobä¸ºloss
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}

</code></pre>

<p>LeNetæ¨¡å‹åŸå›¾ä¸ºï¼š<br/>
<img src="media/14946429015704/14946445265310.jpg" alt=""/></p>

<p><img src="media/14946429015704/14946445430850.jpg" alt=""/></p>

<h2 id="toc_1">è®­ç»ƒè¶…å‚æ•°ï¼š</h2>

<p>ä¸Šé¢å·²è¿‘ç»™å‡ºäº†LeNetæ¨¡å‹ä¸­çš„ç½‘ç»œç»“æ„å›¾å’Œä¸€äº›å‚æ•°å®šä¹‰ï¼Œä¸‹é¢æˆ‘ä»¬æ­£å¼æ¥è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†ç±»å‡†ç¡®ç‡å¯ä»¥è¾¾åˆ°99%ä»¥ä¸Šçš„æ¨¡å‹ã€‚</p>

<p>é¦–å…ˆè¿›å…¥caffeæ‰€åœ¨ç›®å½•ï¼š<br/>
æ‰§è¡Œï¼š<code>examples/mnist/train_lenet.sh</code></p>

<p><code>train_lenet.sh</code>çš„ä»£ç ä¸ºï¼š</p>

<pre><code>#!/usr/bin/env sh
set -e          #æš‚æ—¶ä¸çŸ¥é“å…·ä½“ä½œç”¨

./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt $@

</code></pre>

<p>è¿™é‡Œè°ƒç”¨äº†ä¹‹å‰ç¼–è¯‘å¥½çš„<code>build/tools/caffe.bin</code>äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå‚æ•°ä¸ºï¼š--<code>solver=examples/mnist/lenet_solver.prototxt $@</code>æŒ‡å®šäº†è®­ç»ƒè¶…å‚æ•°æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š</p>

<pre><code class="language-py"># The train/test net protocol buffer definition
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.ï¼ˆè®­ç»ƒæ—¶æ¯è¿­ä»£500æ¬¡ï¼Œè¿›è¡Œä¸€æ¬¡é¢„æµ‹ï¼‰
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.ï¼ˆç½‘ç»œçš„åŸºç¡€å­¦ä¹ é€Ÿç‡ï¼Œå†²é‡å’Œè¡°å‡é‡ï¼‰
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policyï¼ˆå­¦ä¹ é€Ÿç‡çš„è¡°å‡ç­–ç•¥ï¼‰
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterationsï¼ˆæ¯100æ¬¡è¿­ä»£ï¼Œåœ¨å±å¹•ä¸Šæ‰“å°ä¸€æ¬¡è¿è¡Œlogï¼‰
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate resultsï¼ˆæ¯5000æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡å¿«ç…§ï¼‰
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
# solver mode: CPU or GPUï¼ˆæ±‚è§£æ¨¡å¼ä¸ºCPUæ¨¡å¼ï¼Œå› ä¸ºmacæ²¡æœ‰Nå¡ï¼‰
solver_mode: CPU
</code></pre>

<h2 id="toc_2">è®­ç»ƒæ—¥å¿—</h2>

<p>æ‰§è¡Œä¸Šé¢<code>examples/mnist/train_lenet.sh</code>æ–‡ä»¶åä¼šäº§ç”Ÿå¦‚ä¸‹çš„æ—¥å¿—è¾“å‡ºï¼š</p>

<pre><code>//ä½¿ç”¨cpuæ¨¡å¼è¿è¡Œ
I0513 11:18:42.330993 3659862976 caffe.cpp:211] Use CPU.
I0513 11:18:42.331964 3659862976 solver.cpp:44] Initializing solver from parameters:
//æ‰“å°è®­ç»ƒè¶…å‚æ•°æ–‡ä»¶lenet_solver.prototxtä¸­ç»è¿‡è§£æçš„å†…å®¹
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
solver_mode: CPU
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0513 11:18:42.332221 3659862976 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
//è§£æCNNç½‘ç»œæè¿°æ–‡ä»¶ä¸­çš„ç½‘ç»œå‚æ•°ï¼Œåˆ›å»ºè®­ç»ƒç½‘ç»œ
I0513 11:18:42.332438 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0513 11:18:42.332453 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0513 11:18:42.332459 3659862976 net.cpp:51] Initializing net from parameters:
//æ‰“å°è®­ç»ƒç½‘è·¯å‚æ•°æè¿°
name: &quot;LeNet&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_train_lmdb&quot;
    batch_size: 64
    backend: LMDB
  }
}
//........ä¸­é—´çœç•¥
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:18:42.332698 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.332906 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0513 11:18:42.332963 3659862976 net.cpp:84] Creating Layer mnist
//äº§ç”Ÿä¸¤ä¸ªè¾“å‡ºï¼Œdataä¸ºå›¾ç‰‡æ•°æ®ï¼Œlabelä¸ºæ ‡ç­¾æ•°æ®
I0513 11:18:42.332970 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.332989 3659862976 net.cpp:380] mnist -&gt; label
//ä¹‹åæ‰“å¼€äº†è®­ç»ƒé›†LMDBï¼Œdataä¸ºå››ç»´æ•°ç»„ï¼Œåˆç§°blobï¼Œå°ºå¯¸ä¸º64,1,28,28
I0513 11:18:42.333026 3659862976 data_layer.cpp:45] output data size: 64,1,28,28
I0513 11:18:42.337728 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.337738 3659862976 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0513 11:18:42.337759 3659862976 net.cpp:129] Top shape: 64 (64)
//ç»Ÿè®¡å ç”¨å†…å­˜ï¼Œä¼šé€å±‚ç´¯åŠ 
I0513 11:18:42.337762 3659862976 net.cpp:137] Memory required for data: 200960
//ç›–ä¸€æ¥¼ï¼Œconv1
I0513 11:18:42.337769 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.337776 3659862976 net.cpp:84] Creating Layer conv1
//conv1éœ€è¦ä¸€ä¸ªè¾“å…¥dataï¼ˆæ¥è‡ªä¸Šä¸€å±‚mnistï¼‰ï¼Œäº§ç”Ÿä¸€ä¸ªè¾“å‡ºconv1(é€å…¥ä¸‹ä¸€å±‚)
I0513 11:18:42.337780 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.337785 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.337836 3659862976 net.cpp:122] Setting up conv1
//conv1çš„è¾“å‡ºå°ºå¯¸ä¸ºï¼ˆ64,20,24,24ï¼‰
I0513 11:18:42.337842 3659862976 net.cpp:129] Top shape: 64 20 24 24 (737280)
//ç»Ÿè®¡å†…å­˜é€å±‚ç´¯åŠ 
I0513 11:18:42.337847 3659862976 net.cpp:137] Memory required for data: 3150080
I0513 11:18:42.337853 3659862976 layer_factory.hpp:77] Creating layer pool1
//ä¸­é—´å±‚åˆ›å»ºç±»ä¼¼
I0513 11:18:42.337877 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.337882 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.337887 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.337895 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.337899 3659862976 net.cpp:129] Top shape: 64 20 12 12 (184320)
I0513 11:18:42.337904 3659862976 net.cpp:137] Memory required for data: 3887360
I0513 11:18:42.337908 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.337913 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.337916 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.337921 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.338141 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.338146 3659862976 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0513 11:18:42.338162 3659862976 net.cpp:137] Memory required for data: 4706560
I0513 11:18:42.338167 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.338174 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.338178 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.338182 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.338210 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.338215 3659862976 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0513 11:18:42.338220 3659862976 net.cpp:137] Memory required for data: 4911360
I0513 11:18:42.338224 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.338232 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.338235 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.338240 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.341404 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.341413 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341418 3659862976 net.cpp:137] Memory required for data: 5039360
I0513 11:18:42.341424 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.341433 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.341435 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.341440 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.341444 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.341449 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341451 3659862976 net.cpp:137] Memory required for data: 5167360
I0513 11:18:42.341455 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.341470 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.341473 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.341478 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.341531 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.341536 3659862976 net.cpp:129] Top shape: 64 10 (640)
I0513 11:18:42.341539 3659862976 net.cpp:137] Memory required for data: 5169920
//ç›–æœ€åä¸€å±‚loss
I0513 11:18:42.341544 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341550 3659862976 net.cpp:84] Creating Layer loss
//è¯¥å±‚éœ€è¦ä¸¤ä¸ªè¾“å…¥ip2å’Œlabelï¼Œäº§ç”Ÿä¸€ä¸ªè¾“å‡ºloss
I0513 11:18:42.341554 3659862976 net.cpp:406] loss &lt;- ip2
I0513 11:18:42.341557 3659862976 net.cpp:406] loss &lt;- label
I0513 11:18:42.341563 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.341572 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341583 3659862976 net.cpp:122] Setting up loss
//è¾“å‡ºlosså°ºå¯¸ä¸º1ï¼Œloss weightå‚æ•°ä¸º1
I0513 11:18:42.341586 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.341590 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.341598 3659862976 net.cpp:137] Memory required for data: 5169924
//ä»åå¾€å‰ç»Ÿè®¡å“ªäº›å±‚éœ€è¦åšåå‘ä¼ æ’­è®¡ç®—ï¼ˆBPï¼‰
I0513 11:18:42.341601 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.341606 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.341609 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.341614 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.341616 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.341620 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.341624 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.341627 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.341631 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.341655 3659862976 net.cpp:242] This network produces output loss
//ç›–æ¥¼å®Œæ¯•
I0513 11:18:42.341662 3659862976 net.cpp:255] Network initialization done.
//è¿˜éœ€è¦åˆ›å»ºæµ‹è¯•ç½‘ç»œï¼Œåœ¨ç›–ä¸€æ¬¡æ¥¼
I0513 11:18:42.341949 3659862976 solver.cpp:172] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0513 11:18:42.341986 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:18:42.341996 3659862976 net.cpp:51] Initializing net 
from parameters:
//ç±»ä¼¼äºç¬¬ä¸€åº§æ¥¼çš„æƒ…å†µï¼Œåªæ˜¯åœ°åŸºmnistæ”¹äº†ä¸€ä¸‹lmdbæºå’Œè¾“å‡ºå°ºå¯¸ï¼Œé¡¶æ¥¼åŠ äº†ä¸€ä¸ªaccuracyé˜æ¥¼
name: &quot;LeNet&quot;
state {
  phase: TEST
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}

//....ä¸­é—´é‡å¤ï¼Œä¸è¡¨
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
//å…·ä½“ç›–æ¥¼è¿‡ç¨‹ä¸è®­ç»ƒç½‘ç»œç±»ä¼¼
I0513 11:18:42.342216 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.342300 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:18:42.342319 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:18:42.342329 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.342335 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:18:42.342345 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:18:42.343029 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.343037 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:18:42.343057 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343061 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:18:42.343065 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:18:42.343073 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:18:42.343077 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:18:42.343082 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:18:42.343087 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:18:42.343093 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:18:42.343097 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343101 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343106 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:18:42.343109 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.343137 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:18:42.343144 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.343152 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.343175 3659862976 net.cpp:122] Setting up conv1
I0513 11:18:42.343181 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:18:42.343186 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:18:42.343196 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:18:42.343206 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.343214 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.343219 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.343228 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.343232 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:18:42.343236 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:18:42.343240 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.343245 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.343250 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.343253 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.343482 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.343488 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:18:42.343503 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:18:42.343509 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.343513 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.343518 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.343521 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.343526 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.343530 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:18:42.343534 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:18:42.343538 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.343564 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.343569 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.343575 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.346873 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.346884 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346889 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:18:42.346895 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.346901 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.346905 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.346909 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.346915 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.346917 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346921 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:18:42.346925 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.346931 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.346935 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.346938 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.346987 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.346992 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.346997 3659862976 net.cpp:137] Memory required for data: 8078800
//æ³¨æ„è¿™é‡Œï¼Œip2_ip2_0_splitåœ¨ç½‘ç»œæè¿°ä¸­æ²¡æœ‰æ˜¾ç¤ºç»™å‡ºï¼Œæ˜¯caffeè§£æåè‡ªåŠ¨åŠ ä¸Šçš„
I0513 11:18:42.347002 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:18:42.347007 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
//ip2_ip2_0_splitæ¥å—ä¸€ä¸ªè¾“å…¥ip2ï¼Œäº§ç”Ÿä¸¤ä¸ªè¾“å‡ºip2_ip2_0_split_0å’Œip2_ip2_0_split_1ï¼Œæ˜¯å¤åˆ¶å…³ç³»
I0513 11:18:42.347010 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:18:42.347014 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:18:42.347019 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:18:42.347024 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:18:42.347028 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347033 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347036 3659862976 net.cpp:137] Memory required for data: 8086800
//ip2_ip2_0_split_0ç»™äº†accuracyå±‚
I0513 11:18:42.347039 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:18:42.347069 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:18:42.347074 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:18:42.347077 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:18:42.347082 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:18:42.347088 3659862976 net.cpp:122] Setting up accuracy
//accuracyå±‚è¾“å‡ºå°ºå¯¸ä¸º1ï¼Œå³åˆ†ç±»å‡†ç¡®ç‡
I0513 11:18:42.347091 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347095 3659862976 net.cpp:137] Memory required for data: 8086804
//ip2_ip2_0_split_1ç»™äº†losså±‚
I0513 11:18:42.347100 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347103 3659862976 net.cpp:84] Creating Layer loss
I0513 11:18:42.347107 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:18:42.347111 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:18:42.347115 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.347121 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347131 3659862976 net.cpp:122] Setting up loss
I0513 11:18:42.347133 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347137 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.347143 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:18:42.347147 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.347151 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:18:42.347156 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:18:42.347159 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.347162 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.347167 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.347169 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.347173 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.347177 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.347180 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.347184 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:18:42.347189 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.347193 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:18:42.347196 3659862976 net.cpp:242] This network produces output loss
//ç¬¬äºŒåº§æ¥¼ç›–å¥½äº†
I0513 11:18:42.347203 3659862976 net.cpp:255] Network initialization done.
//è£…ä¿®æ–¹æ¡ˆç¡®å®šäº†
I0513 11:18:42.347247 3659862976 solver.cpp:56] Solver scaffolding done.
//å¼€å§‹è£…ä¿®
I0513 11:18:42.347271 3659862976 caffe.cpp:248] Starting Optimization
I0513 11:18:42.347275 3659862976 solver.cpp:272] Solving LeNet
I0513 11:18:42.347278 3659862976 solver.cpp:273] Learning Rate Policy: inv
//å…ˆæµ‹è¯•ä¸€æ¬¡ï¼Œå¾—åˆ°å‡ºäº‹åˆ†ç±»å‡†ç¡®ç‡å’ŒæŸå¤±
I0513 11:18:42.348048 3659862976 solver.cpp:330] Iteration 0, Testing net (#0)
I0513 11:18:44.611253 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:18:44.703907 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.077
I0513 11:18:44.703938 3659862976 solver.cpp:397]     Test net output #1: loss = 2.41516 (* 1 = 2.41516 loss)
//ç°åœ¨åˆ†ç±»æ•ˆæœè‚¯å®šå¾ˆå·®ï¼Œå‡†ç¡®ç‡åªæœ‰0.077ï¼ŒæŸå¤±å€¼çº¦ä¸º2.3
I0513 11:18:44.741230 3659862976 solver.cpp:218] Iteration 0 (0 iter/s, 2.393s/100 iters), loss = 2.42047
//0æ¬¡è¿­ä»£åï¼Œä¾æ—§å¾ˆå·®ï¼Œè®­ç»ƒç½‘ç»œæ²¡æœ‰accuracyè¾“å‡ºï¼Œåªæœ‰lossè¾“å‡º
I0513 11:18:44.741261 3659862976 solver.cpp:237]     Train net output #0: loss = 2.42047 (* 1 = 2.42047 loss)
I0513 11:18:44.741287 3659862976 sgd_solver.cpp:105] Iteration 0, lr = 0.01
//è¿­ä»£100æ¬¡ä¹‹åï¼Œæ•ˆæœå°±å‡ºæ¥äº†ï¼Œlosså·²ç»é™åˆ°0.21ï¼ˆä¹‹å‰æ˜¯2.42ï¼‰
I0513 11:18:47.874459 3659862976 solver.cpp:218] Iteration 100 (31.9183 iter/s, 3.133s/100 iters), loss = 0.215375
I0513 11:18:47.874493 3659862976 solver.cpp:237]     Train net output #0: loss = 0.215375 (* 1 = 0.215375 loss)
I0513 11:18:47.874500 3659862976 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0513 11:18:50.998973 3659862976 solver.cpp:218] Iteration 200 (32.0102 iter/s, 3.124s/100 iters), loss = 0.144389
I0513 11:18:50.999003 3659862976 solver.cpp:237]     Train net output #0: loss = 0.144389 (* 1 = 0.144389 loss)
I0513 11:18:50.999011 3659862976 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0513 11:18:54.100409 3659862976 solver.cpp:218] Iteration 300 (32.2477 iter/s, 3.101s/100 iters), loss = 0.192488
I0513 11:18:54.100476 3659862976 solver.cpp:237]     Train net output #0: loss = 0.192488 (* 1 = 0.192488 loss)
I0513 11:18:54.100483 3659862976 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0513 11:18:57.210686 3659862976 solver.cpp:218] Iteration 400 (32.1543 iter/s, 3.11s/100 iters), loss = 0.0663644
I0513 11:18:57.210728 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0663644 (* 1 = 0.0663644 loss)
I0513 11:18:57.210737 3659862976 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
//è¿­ä»£500æ¬¡ä¹‹åï¼Œè¿›è¡Œä¸€æ¬¡æµ‹è¯•ã€‚
I0513 11:19:00.279249 3659862976 solver.cpp:330] Iteration 500, Testing net (#0)
I0513 11:19:02.608597 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//å‘ç°å‡†ç¡®åº¦accuracyå·²ç»æ˜¾è‘—æå‡åˆ°0.9744äº†ï¼Œlossä¸º0.08
I0513 11:19:02.703658 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9744
I0513 11:19:02.703694 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0836155 (* 1 = 0.0836155 loss)
I0513 11:19:02.735476 3659862976 solver.cpp:218] Iteration 500 (18.1028 iter/s, 5.524s/100 iters), loss = 0.0916289
I0513 11:19:02.735512 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0916288 (* 1 = 0.0916288 loss)
I0513 11:19:02.735520 3659862976 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0513 11:19:05.931562 3659862976 solver.cpp:218] Iteration 600 (31.2891 iter/s, 3.196s/100 iters), loss = 0.0844364
I0513 11:19:05.931597 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0844363 (* 1 = 0.0844363 loss)
I0513 11:19:05.931604 3659862976 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0513 11:19:09.116649 3659862976 solver.cpp:218] Iteration 700 (31.3972 iter/s, 3.185s/100 iters), loss = 0.134004
I0513 11:19:09.116684 3659862976 solver.cpp:237]     Train net output #0: loss = 0.134004 (* 1 = 0.134004 loss)
I0513 11:19:09.116691 3659862976 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
//ä¸­é—´æ˜¯è®­ç»ƒè¿‡ç¨‹ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
I0513 11:22:17.536756 3659862976 solver.cpp:218] Iteration 4800 (19.3311 iter/s, 5.173s/100 iters), loss = 0.0179583
I0513 11:22:17.536806 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0179581 (* 1 = 0.0179581 loss)
I0513 11:22:17.536818 3659862976 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0513 11:22:22.731861 3659862976 solver.cpp:218] Iteration 4900 (19.2493 iter/s, 5.195s/100 iters), loss = 0.00556874
I0513 11:22:22.731927 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00556857 (* 1 = 0.00556857 loss)
I0513 11:22:22.731940 3659862976 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
//æ¯è¿­ä»£åˆ°5000æ¬¡ä¹‹åï¼Œæ‰“å°ä¸€æ¬¡å¿«ç…§ï¼Œä¿å­˜lenet_iter_5000.caffemodelå’Œlenet_iter_5000.solverstate
I0513 11:22:28.143353 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0513 11:22:28.167670 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0513 11:22:28.171842 3659862976 solver.cpp:330] Iteration 5000, Testing net (#0)
I0513 11:22:32.514833 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:22:32.699314 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9888
I0513 11:22:32.699359 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0334435 (* 1 = 0.0334435 loss)
I0513 11:22:32.754936 3659862976 solver.cpp:218] Iteration 5000 (9.97705 iter/s, 10.023s/100 iters), loss = 0.0241056
I0513 11:22:32.754987 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0241055 (* 1 = 0.0241055 loss)
I0513 11:22:32.754999 3659862976 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
//ä¸­é—´ç»§ç»­è®­ç»ƒã€‚ã€‚ã€‚ã€‚ã€‚
I0513 11:26:53.808578 3659862976 solver.cpp:218] Iteration 9900 (21.097 iter/s, 4.74s/100 iters), loss = 0.00466773
I0513 11:26:53.808624 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00466757 (* 1 = 0.00466757 loss)
I0513 11:26:53.808635 3659862976 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
//æœ€åä¸€æ¬¡æ‰“å°å¿«ç…§
I0513 11:26:58.671659 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0513 11:26:58.688323 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0513 11:26:58.715297 3659862976 solver.cpp:310] Iteration 10000, loss = 0.00293942
I0513 11:26:58.715337 3659862976 solver.cpp:330] Iteration 10000, Testing net (#0)
I0513 11:27:02.099313 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//æœ€ç»ˆåˆ†ç±»å‡†ç¡®ç‡ä¸º99%
I0513 11:27:02.230465 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.991
//æœ€ç»ˆlosså€¼ä¸º0.03
I0513 11:27:02.230509 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0304018 (* 1 = 0.0304018 loss)
I0513 11:27:02.230518 3659862976 solver.cpp:315] Optimization Done.
I0513 11:27:02.230525 3659862976 caffe.cpp:259] Optimization Done.
//è£…ä¿®ç»“æŸ
</code></pre>

<h2 id="toc_3">ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹</h2>

<p>ä»ä¸Šé¢çš„è¾“å‡ºç»“æœå¯ä»¥çœ‹åˆ°æœ€ç»ˆè®­ç»ƒçš„æ¨¡å‹æƒå€¼å­˜åœ¨lenet_iter_10000.caffemodalä¸­ï¼Œä¹‹åå¯ä»¥å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤å°±å¯ä»¥äº†ï¼š</p>

<pre><code>âœ  caffe git:(master) âœ— ./build/tools/caffe.bin test \
-model examples/mnist/lenet_train_test.prototxt \
-weights examples/mnist/lenet_iter_10000.caffemodel \
    -iterations 100
</code></pre>

<p>ä¸Šè¿°å‘½ä»¤è§£é‡Šï¼š<br/>
./build/tools/caffe.bin testï¼Œè¡¨ç¤ºåªåšé¢„æµ‹ï¼ˆå‰å‘ä¼ æ’­æ€¥é€Ÿé‚£ï¼‰ï¼Œä¸è¿›è¡Œå‚æ•°æ›´æ–°ï¼ˆBPåå‘ä¼ æ’­è®¡ç®—ï¼‰</p>

<p>-model examples/mnist/lenet_train_test.prototxt ï¼ŒæŒ‡å®šæ¨¡å‹æè¿°æ–‡æœ¬æ–‡ä»¶</p>

<p>-weights examples/mnist/lenet_iter_10000.caffemodel ï¼ŒæŒ‡å®šæ¨¡å‹é¢„å…ˆè®­ç»ƒå¥½çš„æƒå€¼æ–‡ä»¶<br/>
-iterations 100 ï¼Œ æŒ‡å®šæµ‹è¯•è¿­ä»£æ¬¡æ•°ã€‚å‚ä¸æµ‹è¯•çš„æ ·ä¾‹æ•°ç›®ä¸ºï¼ˆiterations*batch_sizeï¼‰,batch_sizeåœ¨model prototxtä¸­è®¾å®šï¼Œä¸º100æ—¶åˆšå¥½è¦†ç›–å…¨éƒ¨10000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚</p>

<p>æˆ‘ä»¬è¿è¡Œä¸Šè¿°å‘½ä»¤å¾—åˆ°ï¼š</p>

<pre><code>I0513 11:37:08.827889 3659862976 caffe.cpp:284] Use CPU.
I0513 11:37:08.830747 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:37:08.830780 3659862976 net.cpp:51] Initializing net from parameters:
name: &quot;LeNet&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:37:08.831130 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:37:08.831360 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:37:08.831418 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:37:08.831425 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:37:08.831444 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:37:08.831480 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:37:08.836457 3659862976 net.cpp:122] Setting up mnist
I0513 11:37:08.836468 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:37:08.836488 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836491 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:37:08.836498 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:37:08.836505 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:37:08.836509 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:37:08.836513 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:37:08.836519 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:37:08.836525 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:37:08.836529 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836534 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836539 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:37:08.836542 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:37:08.836550 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:37:08.836555 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:37:08.836558 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:37:08.836611 3659862976 net.cpp:122] Setting up conv1
I0513 11:37:08.836616 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:37:08.836639 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:37:08.836648 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:37:08.836653 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:37:08.836658 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:37:08.836661 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:37:08.836671 3659862976 net.cpp:122] Setting up pool1
I0513 11:37:08.836675 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:37:08.836680 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:37:08.836683 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:37:08.836691 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:37:08.836695 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:37:08.836700 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:37:08.836917 3659862976 net.cpp:122] Setting up conv2
I0513 11:37:08.836923 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:37:08.836971 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:37:08.837033 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:37:08.837041 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:37:08.837045 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:37:08.837049 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:37:08.837059 3659862976 net.cpp:122] Setting up pool2
I0513 11:37:08.837062 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:37:08.837067 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:37:08.837070 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:37:08.837076 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:37:08.837080 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:37:08.837085 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:37:08.840445 3659862976 net.cpp:122] Setting up ip1
I0513 11:37:08.840461 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840467 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:37:08.840476 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:37:08.840487 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:37:08.840492 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:37:08.840497 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:37:08.840504 3659862976 net.cpp:122] Setting up relu1
I0513 11:37:08.840507 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840512 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:37:08.840517 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:37:08.840523 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:37:08.840528 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:37:08.840533 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:37:08.840591 3659862976 net.cpp:122] Setting up ip2
I0513 11:37:08.840597 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840601 3659862976 net.cpp:137] Memory required for data: 8078800
I0513 11:37:08.840606 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:37:08.840612 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
I0513 11:37:08.840616 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:37:08.840623 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:37:08.840631 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:37:08.840637 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:37:08.840641 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840646 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840649 3659862976 net.cpp:137] Memory required for data: 8086800
I0513 11:37:08.840653 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:37:08.840659 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:37:08.840663 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:37:08.840668 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:37:08.840672 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:37:08.840678 3659862976 net.cpp:122] Setting up accuracy
I0513 11:37:08.840708 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840714 3659862976 net.cpp:137] Memory required for data: 8086804
I0513 11:37:08.840718 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840724 3659862976 net.cpp:84] Creating Layer loss
I0513 11:37:08.840728 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:37:08.840733 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:37:08.840737 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:37:08.840746 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840759 3659862976 net.cpp:122] Setting up loss
I0513 11:37:08.840762 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840767 3659862976 net.cpp:132]     with loss weight 1
I0513 11:37:08.840776 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:37:08.840780 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:37:08.840785 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:37:08.840790 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:37:08.840793 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:37:08.840798 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:37:08.840802 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:37:08.840806 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:37:08.840811 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:37:08.840814 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:37:08.840818 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:37:08.840822 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:37:08.840827 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:37:08.840831 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:37:08.840836 3659862976 net.cpp:242] This network produces output loss
I0513 11:37:08.840843 3659862976 net.cpp:255] Network initialization done.
I0513 11:37:08.843325 3659862976 caffe.cpp:290] Running for 100 iterations.
I0513 11:37:08.871536 3659862976 caffe.cpp:313] Batch 0, accuracy = 1
I0513 11:37:08.871567 3659862976 caffe.cpp:313] Batch 0, loss = 0.0085843
I0513 11:37:08.894382 3659862976 caffe.cpp:313] Batch 1, accuracy = 1
I0513 11:37:08.894414 3659862976 caffe.cpp:313] Batch 1, loss = 0.00573037
I0513 11:37:08.918002 3659862976 caffe.cpp:313] Batch 2, accuracy = 0.99
I0513 11:37:08.918031 3659862976 caffe.cpp:313] Batch 2, loss = 0.0333053
I0513 11:37:08.943091 3659862976 caffe.cpp:313] Batch 3, accuracy = 0.99
I0513 11:37:08.943127 3659862976 caffe.cpp:313] Batch 3, loss = 0.0271862
I0513 11:37:08.967147 3659862976 caffe.cpp:313] Batch 4, accuracy = 0.99
I0513 11:37:08.967177 3659862976 caffe.cpp:313] Batch 4, loss = 0.0571239
I0513 11:37:08.989929 3659862976 caffe.cpp:313] Batch 5, accuracy = 0.99
I0513 11:37:08.989961 3659862976 caffe.cpp:313] Batch 5, loss = 0.0569953
I0513 11:37:09.015426 3659862976 caffe.cpp:313] Batch 6, accuracy = 0.98
I0513 11:37:09.015463 3659862976 caffe.cpp:313] Batch 6, loss = 0.0698283
I0513 11:37:09.039398 3659862976 caffe.cpp:313] Batch 7, accuracy = 0.99
I0513 11:37:09.039432 3659862976 caffe.cpp:313] Batch 7, loss = 0.0349087
I0513 11:37:09.063937 3659862976 caffe.cpp:313] Batch 8, accuracy = 1
I0513 11:37:09.063967 3659862976 caffe.cpp:313] Batch 8, loss = 0.0115442
I0513 11:37:09.086630 3659862976 caffe.cpp:313] Batch 9, accuracy = 0.99
I0513 11:37:09.086663 3659862976 caffe.cpp:313] Batch 9, loss = 0.0361095
I0513 11:37:09.111706 3659862976 caffe.cpp:313] Batch 10, accuracy = 0.98
I0513 11:37:09.111735 3659862976 caffe.cpp:313] Batch 10, loss = 0.0702643
I0513 11:37:09.135445 3659862976 caffe.cpp:313] Batch 11, accuracy = 0.97
I0513 11:37:09.135478 3659862976 caffe.cpp:313] Batch 11, loss = 0.0508112
I0513 11:37:09.159065 3659862976 caffe.cpp:313] Batch 12, accuracy = 0.95
I0513 11:37:09.159097 3659862976 caffe.cpp:313] Batch 12, loss = 0.148118
I0513 11:37:09.181542 3659862976 caffe.cpp:313] Batch 13, accuracy = 0.98
I0513 11:37:09.181607 3659862976 caffe.cpp:313] Batch 13, loss = 0.036772
I0513 11:37:09.205440 3659862976 caffe.cpp:313] Batch 14, accuracy = 1
I0513 11:37:09.205476 3659862976 caffe.cpp:313] Batch 14, loss = 0.00694412
I0513 11:37:09.228198 3659862976 caffe.cpp:313] Batch 15, accuracy = 0.99
I0513 11:37:09.228229 3659862976 caffe.cpp:313] Batch 15, loss = 0.0389514
I0513 11:37:09.251550 3659862976 caffe.cpp:313] Batch 16, accuracy = 0.98
I0513 11:37:09.251581 3659862976 caffe.cpp:313] Batch 16, loss = 0.0298825
I0513 11:37:09.275153 3659862976 caffe.cpp:313] Batch 17, accuracy = 1
I0513 11:37:09.275182 3659862976 caffe.cpp:313] Batch 17, loss = 0.0170967
I0513 11:37:09.298004 3659862976 caffe.cpp:313] Batch 18, accuracy = 0.99
I0513 11:37:09.298035 3659862976 caffe.cpp:313] Batch 18, loss = 0.0189575
I0513 11:37:09.321348 3659862976 caffe.cpp:313] Batch 19, accuracy = 0.99
I0513 11:37:09.321379 3659862976 caffe.cpp:313] Batch 19, loss = 0.0455956
I0513 11:37:09.344025 3659862976 caffe.cpp:313] Batch 20, accuracy = 0.98
I0513 11:37:09.344058 3659862976 caffe.cpp:313] Batch 20, loss = 0.108723
I0513 11:37:09.368069 3659862976 caffe.cpp:313] Batch 21, accuracy = 0.98
I0513 11:37:09.368101 3659862976 caffe.cpp:313] Batch 21, loss = 0.0780955
I0513 11:37:09.390791 3659862976 caffe.cpp:313] Batch 22, accuracy = 0.99
I0513 11:37:09.390823 3659862976 caffe.cpp:313] Batch 22, loss = 0.0368689
I0513 11:37:09.414577 3659862976 caffe.cpp:313] Batch 23, accuracy = 0.97
I0513 11:37:09.414621 3659862976 caffe.cpp:313] Batch 23, loss = 0.0296016
I0513 11:37:09.437597 3659862976 caffe.cpp:313] Batch 24, accuracy = 0.97
I0513 11:37:09.437628 3659862976 caffe.cpp:313] Batch 24, loss = 0.0589915
I0513 11:37:09.460636 3659862976 caffe.cpp:313] Batch 25, accuracy = 0.99
I0513 11:37:09.460669 3659862976 caffe.cpp:313] Batch 25, loss = 0.0754509
I0513 11:37:09.483229 3659862976 caffe.cpp:313] Batch 26, accuracy = 0.99
I0513 11:37:09.483261 3659862976 caffe.cpp:313] Batch 26, loss = 0.118656
I0513 11:37:09.508059 3659862976 caffe.cpp:313] Batch 27, accuracy = 0.98
I0513 11:37:09.508092 3659862976 caffe.cpp:313] Batch 27, loss = 0.0222734
I0513 11:37:09.530911 3659862976 caffe.cpp:313] Batch 28, accuracy = 0.99
I0513 11:37:09.530943 3659862976 caffe.cpp:313] Batch 28, loss = 0.0315118
I0513 11:37:09.555687 3659862976 caffe.cpp:313] Batch 29, accuracy = 0.97
I0513 11:37:09.555721 3659862976 caffe.cpp:313] Batch 29, loss = 0.129427
I0513 11:37:09.579476 3659862976 caffe.cpp:313] Batch 30, accuracy = 1
I0513 11:37:09.579507 3659862976 caffe.cpp:313] Batch 30, loss = 0.0196561
I0513 11:37:09.602957 3659862976 caffe.cpp:313] Batch 31, accuracy = 1
I0513 11:37:09.602993 3659862976 caffe.cpp:313] Batch 31, loss = 0.00242798
I0513 11:37:09.626893 3659862976 caffe.cpp:313] Batch 32, accuracy = 0.99
I0513 11:37:09.626924 3659862976 caffe.cpp:313] Batch 32, loss = 0.0169622
I0513 11:37:09.650236 3659862976 caffe.cpp:313] Batch 33, accuracy = 1
I0513 11:37:09.650270 3659862976 caffe.cpp:313] Batch 33, loss = 0.00425847
I0513 11:37:09.673212 3659862976 caffe.cpp:313] Batch 34, accuracy = 0.99
I0513 11:37:09.673243 3659862976 caffe.cpp:313] Batch 34, loss = 0.0726783
I0513 11:37:09.696039 3659862976 caffe.cpp:313] Batch 35, accuracy = 0.95
I0513 11:37:09.696071 3659862976 caffe.cpp:313] Batch 35, loss = 0.173234
I0513 11:37:09.719209 3659862976 caffe.cpp:313] Batch 36, accuracy = 1
I0513 11:37:09.719241 3659862976 caffe.cpp:313] Batch 36, loss = 0.0126433
I0513 11:37:09.741852 3659862976 caffe.cpp:313] Batch 37, accuracy = 0.99
I0513 11:37:09.741884 3659862976 caffe.cpp:313] Batch 37, loss = 0.0380185
I0513 11:37:09.766039 3659862976 caffe.cpp:313] Batch 38, accuracy = 1
I0513 11:37:09.766072 3659862976 caffe.cpp:313] Batch 38, loss = 0.0161337
I0513 11:37:09.788811 3659862976 caffe.cpp:313] Batch 39, accuracy = 0.98
I0513 11:37:09.788844 3659862976 caffe.cpp:313] Batch 39, loss = 0.0317039
I0513 11:37:09.812556 3659862976 caffe.cpp:313] Batch 40, accuracy = 1
I0513 11:37:09.812587 3659862976 caffe.cpp:313] Batch 40, loss = 0.0283054
I0513 11:37:09.835418 3659862976 caffe.cpp:313] Batch 41, accuracy = 0.98
I0513 11:37:09.835450 3659862976 caffe.cpp:313] Batch 41, loss = 0.0595546
I0513 11:37:09.858765 3659862976 caffe.cpp:313] Batch 42, accuracy = 0.98
I0513 11:37:09.858793 3659862976 caffe.cpp:313] Batch 42, loss = 0.033258
I0513 11:37:09.881479 3659862976 caffe.cpp:313] Batch 43, accuracy = 1
I0513 11:37:09.881510 3659862976 caffe.cpp:313] Batch 43, loss = 0.00560485
I0513 11:37:09.906558 3659862976 caffe.cpp:313] Batch 44, accuracy = 1
I0513 11:37:09.906590 3659862976 caffe.cpp:313] Batch 44, loss = 0.0164246
I0513 11:37:09.932261 3659862976 caffe.cpp:313] Batch 45, accuracy = 0.99
I0513 11:37:09.932294 3659862976 caffe.cpp:313] Batch 45, loss = 0.047733
I0513 11:37:09.957159 3659862976 caffe.cpp:313] Batch 46, accuracy = 1
I0513 11:37:09.957190 3659862976 caffe.cpp:313] Batch 46, loss = 0.00406718
I0513 11:37:09.979852 3659862976 caffe.cpp:313] Batch 47, accuracy = 0.99
I0513 11:37:09.979883 3659862976 caffe.cpp:313] Batch 47, loss = 0.0176224
I0513 11:37:10.003631 3659862976 caffe.cpp:313] Batch 48, accuracy = 0.95
I0513 11:37:10.003666 3659862976 caffe.cpp:313] Batch 48, loss = 0.0918992
I0513 11:37:10.027333 3659862976 caffe.cpp:313] Batch 49, accuracy = 1
I0513 11:37:10.027365 3659862976 caffe.cpp:313] Batch 49, loss = 0.00535747
I0513 11:37:10.050904 3659862976 caffe.cpp:313] Batch 50, accuracy = 1
I0513 11:37:10.050935 3659862976 caffe.cpp:313] Batch 50, loss = 0.000293352
I0513 11:37:10.076280 3659862976 caffe.cpp:313] Batch 51, accuracy = 1
I0513 11:37:10.076314 3659862976 caffe.cpp:313] Batch 51, loss = 0.00675426
I0513 11:37:10.099964 3659862976 caffe.cpp:313] Batch 52, accuracy = 1
I0513 11:37:10.099993 3659862976 caffe.cpp:313] Batch 52, loss = 0.0113504
I0513 11:37:10.123363 3659862976 caffe.cpp:313] Batch 53, accuracy = 1
I0513 11:37:10.123394 3659862976 caffe.cpp:313] Batch 53, loss = 0.00080642
I0513 11:37:10.146338 3659862976 caffe.cpp:313] Batch 54, accuracy = 1
I0513 11:37:10.146368 3659862976 caffe.cpp:313] Batch 54, loss = 0.0119724
I0513 11:37:10.170075 3659862976 caffe.cpp:313] Batch 55, accuracy = 1
I0513 11:37:10.170106 3659862976 caffe.cpp:313] Batch 55, loss = 9.95353e-05
I0513 11:37:10.192754 3659862976 caffe.cpp:313] Batch 56, accuracy = 1
I0513 11:37:10.192785 3659862976 caffe.cpp:313] Batch 56, loss = 0.00792123
I0513 11:37:10.215930 3659862976 caffe.cpp:313] Batch 57, accuracy = 1
I0513 11:37:10.215963 3659862976 caffe.cpp:313] Batch 57, loss = 0.0106224
I0513 11:37:10.238731 3659862976 caffe.cpp:313] Batch 58, accuracy = 1
I0513 11:37:10.238765 3659862976 caffe.cpp:313] Batch 58, loss = 0.00865888
I0513 11:37:10.261700 3659862976 caffe.cpp:313] Batch 59, accuracy = 0.98
I0513 11:37:10.261731 3659862976 caffe.cpp:313] Batch 59, loss = 0.0758659
I0513 11:37:10.284554 3659862976 caffe.cpp:313] Batch 60, accuracy = 1
I0513 11:37:10.284585 3659862976 caffe.cpp:313] Batch 60, loss = 0.00406362
I0513 11:37:10.310072 3659862976 caffe.cpp:313] Batch 61, accuracy = 1
I0513 11:37:10.310102 3659862976 caffe.cpp:313] Batch 61, loss = 0.00472714
I0513 11:37:10.332813 3659862976 caffe.cpp:313] Batch 62, accuracy = 1
I0513 11:37:10.332845 3659862976 caffe.cpp:313] Batch 62, loss = 0.00013836
I0513 11:37:10.356101 3659862976 caffe.cpp:313] Batch 63, accuracy = 1
I0513 11:37:10.356132 3659862976 caffe.cpp:313] Batch 63, loss = 0.000318341
I0513 11:37:10.378556 3659862976 caffe.cpp:313] Batch 64, accuracy = 1
I0513 11:37:10.378587 3659862976 caffe.cpp:313] Batch 64, loss = 0.000235923
I0513 11:37:10.402688 3659862976 caffe.cpp:313] Batch 65, accuracy = 0.94
I0513 11:37:10.402724 3659862976 caffe.cpp:313] Batch 65, loss = 0.174556
I0513 11:37:10.426704 3659862976 caffe.cpp:313] Batch 66, accuracy = 0.98
I0513 11:37:10.426736 3659862976 caffe.cpp:313] Batch 66, loss = 0.0710799
I0513 11:37:10.450608 3659862976 caffe.cpp:313] Batch 67, accuracy = 0.99
I0513 11:37:10.450641 3659862976 caffe.cpp:313] Batch 67, loss = 0.0471492
I0513 11:37:10.474786 3659862976 caffe.cpp:313] Batch 68, accuracy = 1
I0513 11:37:10.474853 3659862976 caffe.cpp:313] Batch 68, loss = 0.00714237
I0513 11:37:10.497565 3659862976 caffe.cpp:313] Batch 69, accuracy = 1
I0513 11:37:10.497596 3659862976 caffe.cpp:313] Batch 69, loss = 0.00141993
I0513 11:37:10.520592 3659862976 caffe.cpp:313] Batch 70, accuracy = 1
I0513 11:37:10.520623 3659862976 caffe.cpp:313] Batch 70, loss = 0.00206052
I0513 11:37:10.543385 3659862976 caffe.cpp:313] Batch 71, accuracy = 1
I0513 11:37:10.543418 3659862976 caffe.cpp:313] Batch 71, loss = 0.000801532
I0513 11:37:10.567934 3659862976 caffe.cpp:313] Batch 72, accuracy = 0.99
I0513 11:37:10.567965 3659862976 caffe.cpp:313] Batch 72, loss = 0.0175235
I0513 11:37:10.591750 3659862976 caffe.cpp:313] Batch 73, accuracy = 1
I0513 11:37:10.591784 3659862976 caffe.cpp:313] Batch 73, loss = 0.000181734
I0513 11:37:10.617092 3659862976 caffe.cpp:313] Batch 74, accuracy = 1
I0513 11:37:10.617122 3659862976 caffe.cpp:313] Batch 74, loss = 0.00376508
I0513 11:37:10.639822 3659862976 caffe.cpp:313] Batch 75, accuracy = 1
I0513 11:37:10.639853 3659862976 caffe.cpp:313] Batch 75, loss = 0.00211647
I0513 11:37:10.664058 3659862976 caffe.cpp:313] Batch 76, accuracy = 1
I0513 11:37:10.664090 3659862976 caffe.cpp:313] Batch 76, loss = 0.000218412
I0513 11:37:10.686815 3659862976 caffe.cpp:313] Batch 77, accuracy = 1
I0513 11:37:10.686847 3659862976 caffe.cpp:313] Batch 77, loss = 0.000203503
I0513 11:37:10.710923 3659862976 caffe.cpp:313] Batch 78, accuracy = 1
I0513 11:37:10.710953 3659862976 caffe.cpp:313] Batch 78, loss = 0.0013391
I0513 11:37:10.733860 3659862976 caffe.cpp:313] Batch 79, accuracy = 1
I0513 11:37:10.733891 3659862976 caffe.cpp:313] Batch 79, loss = 0.00335708
I0513 11:37:10.758643 3659862976 caffe.cpp:313] Batch 80, accuracy = 0.99
I0513 11:37:10.758677 3659862976 caffe.cpp:313] Batch 80, loss = 0.0256179
I0513 11:37:10.781409 3659862976 caffe.cpp:313] Batch 81, accuracy = 1
I0513 11:37:10.781440 3659862976 caffe.cpp:313] Batch 81, loss = 0.0023732
I0513 11:37:10.805886 3659862976 caffe.cpp:313] Batch 82, accuracy = 0.99
I0513 11:37:10.805920 3659862976 caffe.cpp:313] Batch 82, loss = 0.0162458
I0513 11:37:10.828743 3659862976 caffe.cpp:313] Batch 83, accuracy = 1
I0513 11:37:10.828775 3659862976 caffe.cpp:313] Batch 83, loss = 0.00678432
I0513 11:37:10.852507 3659862976 caffe.cpp:313] Batch 84, accuracy = 0.99
I0513 11:37:10.852538 3659862976 caffe.cpp:313] Batch 84, loss = 0.0189542
I0513 11:37:10.875788 3659862976 caffe.cpp:313] Batch 85, accuracy = 0.99
I0513 11:37:10.875819 3659862976 caffe.cpp:313] Batch 85, loss = 0.0198986
I0513 11:37:10.899011 3659862976 caffe.cpp:313] Batch 86, accuracy = 1
I0513 11:37:10.899040 3659862976 caffe.cpp:313] Batch 86, loss = 0.000146087
I0513 11:37:10.921692 3659862976 caffe.cpp:313] Batch 87, accuracy = 1
I0513 11:37:10.921723 3659862976 caffe.cpp:313] Batch 87, loss = 0.000129989
I0513 11:37:10.944453 3659862976 caffe.cpp:313] Batch 88, accuracy = 1
I0513 11:37:10.944484 3659862976 caffe.cpp:313] Batch 88, loss = 4.1275e-05
I0513 11:37:10.968449 3659862976 caffe.cpp:313] Batch 89, accuracy = 1
I0513 11:37:10.968482 3659862976 caffe.cpp:313] Batch 89, loss = 4.4345e-05
I0513 11:37:10.994932 3659862976 caffe.cpp:313] Batch 90, accuracy = 0.97
I0513 11:37:10.994962 3659862976 caffe.cpp:313] Batch 90, loss = 0.0680957
I0513 11:37:11.018280 3659862976 caffe.cpp:313] Batch 91, accuracy = 1
I0513 11:37:11.018312 3659862976 caffe.cpp:313] Batch 91, loss = 2.29651e-05
I0513 11:37:11.044423 3659862976 caffe.cpp:313] Batch 92, accuracy = 1
I0513 11:37:11.044457 3659862976 caffe.cpp:313] Batch 92, loss = 0.000162702
I0513 11:37:11.068132 3659862976 caffe.cpp:313] Batch 93, accuracy = 1
I0513 11:37:11.068163 3659862976 caffe.cpp:313] Batch 93, loss = 0.000582345
I0513 11:37:11.090775 3659862976 caffe.cpp:313] Batch 94, accuracy = 1
I0513 11:37:11.090806 3659862976 caffe.cpp:313] Batch 94, loss = 0.000352066
I0513 11:37:11.115216 3659862976 caffe.cpp:313] Batch 95, accuracy = 1
I0513 11:37:11.115247 3659862976 caffe.cpp:313] Batch 95, loss = 0.00453322
I0513 11:37:11.115762 84811776 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:37:11.137984 3659862976 caffe.cpp:313] Batch 96, accuracy = 0.97
I0513 11:37:11.138017 3659862976 caffe.cpp:313] Batch 96, loss = 0.0792528
I0513 11:37:11.162164 3659862976 caffe.cpp:313] Batch 97, accuracy = 0.98
I0513 11:37:11.162194 3659862976 caffe.cpp:313] Batch 97, loss = 0.106678
I0513 11:37:11.184717 3659862976 caffe.cpp:313] Batch 98, accuracy = 1
I0513 11:37:11.184751 3659862976 caffe.cpp:313] Batch 98, loss = 0.0035934
I0513 11:37:11.208353 3659862976 caffe.cpp:313] Batch 99, accuracy = 0.99
I0513 11:37:11.208385 3659862976 caffe.cpp:313] Batch 99, loss = 0.0180797
I0513 11:37:11.208390 3659862976 caffe.cpp:318] Loss: 0.0304018
I0513 11:37:11.208411 3659862976 caffe.cpp:330] accuracy = 0.991
I0513 11:37:11.208425 3659862976 caffe.cpp:330] loss = 0.0304018 (* 1 = 0.0304018 loss)
</code></pre>

<p>æœ€åaccuracyä¸º0.991ï¼Œlossä¸º0.03</p>

<h2 id="toc_4">æ€»ç»“</h2>

<p>é€šè¿‡ä¸Šè¿°å†…å®¹,æˆ‘ä»¬å¯ä»¥åˆæ­¥äº†è§£ä¸€ä¸ªå®Œæ•´çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿæœ€æ ¸å¿ƒçš„ä¸¤ä¸ªæ–¹é¢:<strong>æ•°æ®å’Œæ¨¡å‹</strong>.æ•°æ®æ˜¯å¸¦æ ‡ç­¾çš„å›¾ç‰‡é›†,åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†;æ¨¡å‹æ˜¯æè¿°CNNç»“æ„çš„æœ‰å‘æ— ç¯å›¾(DAG),è¡¨ç¤ºå¯¹åŸå§‹æ•°æ®çš„å¤„ç†æ–¹å¼.</p>

<p><font color=red>Caffeå¹¶ä¸ç›´æ¥å¤„ç†åŸå§‹æ•°æ®,ç”±é¢„å¤„ç†ç¨‹åºå°†åŸå§‹æ•°æ®å­˜å‚¨ä¸º<code>LMDB</code>æ ¼å¼,æ¥ä¿æŒè¾ƒé«˜çš„IOæ•ˆç‡.æ¨¡å‹é€šå¸¸ç”¨ProtoBufferæ–‡æœ¬æ ¼å¼è¡¨è¿°,è®­ç»ƒç»“æœä¿å­˜ä¸ºProtoBufferäºŒè¿›åˆ¶æ–‡ä»¶æˆ–HDF5æ ¼å¼æ–‡ä»¶.</font><strong>æ·±åº¦å­¦ä¹ çš„è¿‡ç¨‹å°±æ˜¯åˆ©ç”¨è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ,å°†æ•°æ®ä¸­è•´è—çš„å¤§é‡ä¿¡æ¯é€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•ä¸æ–­æ”¶é›†åˆ°æ¨¡å‹ä¸­,åˆ©ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ç°å®ä¸–ç•Œä¸­ç›¸ä¼¼æ•°æ®è¿›è¡Œç‰¹å®šå¤„ç†(å¦‚åˆ†ç±»,è¯†åˆ«,æ£€æµ‹,å®šä½).</strong></p>

]]></content>
  </entry>
  
</feed>
