<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[LZH007]]></title>
  <link href="https://lockxmonk.github.io/atom.xml" rel="self"/>
  <link href="https://lockxmonk.github.io/"/>
  <updated>2017-06-09T16:23:08+08:00</updated>
  <id>https://lockxmonk.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[caffe数据结构描述]]></title>
    <link href="https://lockxmonk.github.io/14970763342670.html"/>
    <updated>2017-06-10T14:32:14+08:00</updated>
    <id>https://lockxmonk.github.io/14970763342670.html</id>
    <content type="html"><![CDATA[
<p>打开caffe目录下的<code>src/caffe/proto/caffe.proto</code>文件,首先讲的就是Blob的描述.</p>

<pre><code class="language-proto">// 该结构描述了 Blob的形状信息
message BlobShape {
  repeated int64 dim = 1 [packed = true];  //只包括若干int64类型值，分别表示Blob每个维度的大小。packed表示这些值在内存中紧密排布，没有空洞
}

//该结构描述Blob在磁盘中序列化后的形态
message BlobProto {
  optional BlobShape shape = 7;    //可选，包括一个BlobShape对象
  repeated float data = 5 [packed = true]; // //包括若千浮点元素，存储数据或权值，元素数目由shape或（num, channels, height, width)确定，这些元素在内存中紧密排布.
  repeated float diff = 6 [packed = true];  ////包括若干浮点元素，用于存储增量信息，维度与data 数组一致
  repeated double double_data = 8 [packed = true];  // 与 data并列，只是类型为double
  repeated double double_diff = 9 [packed = true];  // 与 diff 并列，只是类型为 double

  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}
</code></pre>

<p><strong>这里我们使用protobuffer主要是因为它具有很好的健壮性,将编程最容易出问题的地方加以隐藏，让机器自动处理.</strong></p>

<h2 id="toc_0">Blob的构成</h2>

<p>Blob是一个模板类,声明在<code>include/caffe/blob.hpp中</code>,里面封装了一些基本的Layer,Net,Solver等,还有syncedmem类:</p>

<pre><code class="language-C++">
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;//由protoc生成的头文件，声明了 BlobProto、BlobShape等遵循caffe.proto协议的数据结构 可以在src/caffe/proto文件下运行protoc caffe.proto --cpp_out=./命令生成该头文件.
#include &quot;caffe/syncedmem.hpp&quot;  //CPU/GPU共享内存类，用于数据同步

const int kMaxBlobAxes = 32;    //Blob最大维数目
template &lt;typename Dtype&gt;
class Blob {    //类声明
 public:
    //默认构造函数
  Blob()
       : data_(), diff_(), count_(0), capacity_(0) {}
    //显式构造函数
  explicit Blob(const int num, const int channels, const int height, const int width);
  explicit Blob(const vector&lt;int&gt;&amp; shape);

 //变形函数，报据输入参数重新设置当前Blob形状,必要时重新分配内存
  void Reshape(const int num, const int channels, const int height,
      const int width);
  
  void Reshape(const vector&lt;int&gt;&amp; shape);
  void Reshape(const BlobShape&amp; shape);
  void ReshapeLike(const Blob&amp; other);
  //得到Blob形状字符串用于打印log,见Caffe运行log,类似&quot;Top shape: 100 1 28 28 (78400)&quot;
  inline string shape_string() const {
    ostringstream stream;
    for (int i = 0; i &lt; shape_.size(); ++i) {
      stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;
    }
    stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;
    return stream.str();
  }
  //返回Blob形状
  inline const vector&lt;int&gt;&amp; shape() const { return shape_; }
    //返回某1维度的尺寸
  inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  //返回维度数目
  inline int num_axes() const { return shape_.size(); }
  //返回Blob中元素总数
  inline int count() const { return count_; }
    //返回Blob中某几维子集的元素总数
    inline int count(int start_axis, int end_axis) const {
    CHECK_LE(start_axis, end_axis); //保证 start_axis &lt;= end_axis
    CHECK_GE(start_axis, 0);  // 保证 start_axis &gt;= 0
    CHECK_GE(end_axis, 0);      // 保证 end_axis &gt;= 0
    CHECK_LE(start_axis, num_axes()); //保证start_axis    &lt;=总的维度数目
    CHECK_LE(end_axis, num_axes()); //保证end_axis &lt;=总的维度数目
    int count = 1;
    for (int i = start_axis; i &lt; end_axis; ++i) {
      count *= shape(i);
    }
    return count;
  }
  //计算从某一维度开始的元素总数
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
  //转换坐标轴索引[-N,N)为普通索引[0,N)
  inline int CanonicalAxisIndex(int axis_index) const {
    CHECK_GE(axis_index, -num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    CHECK_LT(axis_index, num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    if (axis_index &lt; 0) {
    //负索引表示从后向前访问，-1表示最后一个个元素，普通索引值为 N-1:同理，-2 =&gt; N-2, -3 =&gt; N-3,…
      return axis_index + num_axes();
    }
    return axis_index;
  }
  //获取某一维的尺寸
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.
  inline int num() const { return LegacyShape(0); }
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.
  inline int channels() const { return LegacyShape(1); }
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.
  inline int height() const { return LegacyShape(2); }
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.
  inline int width() const { return LegacyShape(3); }
  inline int LegacyShape(int index) const {
    CHECK_LE(num_axes(), 4)
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;
    CHECK_LT(index, 4);
    CHECK_GE(index, -4);
    if (index &gt;= num_axes() || index &lt; -num_axes()) {
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse
      // indexing) -- this special case simulates the one-padding used to fill
      // extraneous axes of legacy blobs.
      return 1;
    }
    return shape(index);
  }
  //下面的是计算偏移量的函数
  inline int offset(const int n, const int c = 0, const int h = 0,
      const int w = 0) const {
    CHECK_GE(n, 0);
    CHECK_LE(n, num());
    CHECK_GE(channels(), 0);
    CHECK_LE(c, channels());
    CHECK_GE(height(), 0);
    CHECK_LE(h, height());
    CHECK_GE(width(), 0);
    CHECK_LE(w, width());
    return ((n * channels() + c) * height() + h) * width() + w;
  }

  inline int offset(const vector&lt;int&gt;&amp; indices) const {
    CHECK_LE(indices.size(), num_axes());
    int offset = 0;
    for (int i = 0; i &lt; num_axes(); ++i) {
      offset *= shape(i);
      if (indices.size() &gt; i) {
        CHECK_GE(indices[i], 0);
        CHECK_LT(indices[i], shape(i));
        offset += indices[i];
      }
    }
    return offset;
  }
  //按值拷贝Blob到当前Blob
  void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false);
  
  //下面几个函数是存取器(getter/setter)
  inline Dtype data_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_data()[offset(n, c, h, w)];
  }

  inline Dtype diff_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_diff()[offset(n, c, h, w)];
  }

  inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_data()[offset(index)];
  }

  inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_diff()[offset(index)];
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;
  }
  
  //只读访问cpu_date
  const Dtype* cpu_data() const;
  //设置cpu_date
  void set_cpu_data(Dtype* data);
  const int* gpu_shape() const;
  //只读访问gpu_date
  const Dtype* gpu_data() const;
  //设置gpu_date
  void set_gpu_data(Dtype* data);
  //只读访问cpu_diff
  const Dtype* cpu_diff() const;
  //只读访问gpu_diff
  const Dtype* gpu_diff() const;
  //下面四个是读写访问数据
  Dtype* mutable_cpu_data();
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  void Update();    //Blob更新运算，可简单理解为data与diff的merge过程
  //反序列化函数，从BlobProto中恢复个Blob对象
  void FromProto(const BlobProto&amp; proto, bool reshape = true);
  //序列化函数，将内存中的Blob对象保存到BlobProto中
  void ToProto(BlobProto* proto, bool write_diff = false) const;

  /// @brief Compute the sum of absolute values (L1 norm) of the data.
  Dtype asum_data() const;
  /// @brief Compute the sum of absolute values (L1 norm) of the diff.
  Dtype asum_diff() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the data.
  Dtype sumsq_data() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the diff.
  Dtype sumsq_diff() const;

/// @brief Scale the blob data by a constant factor.
  void scale_data(Dtype scale_factor);
  /// @brief Scale the blob diff by a constant factor.
  void scale_diff(Dtype scale_factor);
 // 共享另一个 Blob 的 diff
  void ShareData(const Blob&amp; other);
  void ShareDiff(const Blob&amp; other);
  
  protected:
  shared_ptr&lt;SyncedMemory&gt; data_;   //存放指向data的指针
  shared_ptr&lt;SyncedMemory&gt; diff_;   //存放指向diff的指针
  shared_ptr&lt;SyncedMemory&gt; shape_data_; 
  vector&lt;int&gt; shape_;   //形状信息
  int count_;   //存放有效元素数目信息
  int capacity_;    //存放Blob容器的容量信息

  DISABLE_COPY_AND_ASSIGN(Blob);    //禁用拷贝构造函数、陚值运算符重载
};  // class Blob

</code></pre>

<p><strong>注意到Caffe类中成员变量名都带有后缀，这样在函数实现中容易区分临时变量和类成员变量。</strong></p>

<p>打幵<code>include/caffe/syncedmem.hpp</code>，査看该类的用法:</p>

<pre><code class="language-c++">#ifndef CAFFE_SYNCEDMEM_HPP_
#define CAFFE_SYNCEDMEM_HPP_

#include &lt;cstdlib&gt;

#ifdef USE_MKL
  #include &quot;mkl.h&quot;
#endif

#include &quot;caffe/common.hpp&quot;

namespace caffe {

//如果在GPU模式，且CUDA使能，那么主机内存会以页锁定内存方式分配（使用cudaMallocHostU函数。对f-单GPU的性能提升不明显，但多GPU会非常明显)
inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) {
#ifndef CPU_ONLY
  if (Caffe::mode() == Caffe::GPU) {
    CUDA_CHECK(cudaMallocHost(ptr, size));
    *use_cuda = true;
    return;
  }
#endif
#ifdef USE_MKL
  *ptr = mkl_malloc(size ? size:1, 64);
#else
  *ptr = malloc(size);
#endif
  *use_cuda = false;
  CHECK(*ptr) &lt;&lt; &quot;host allocation of size &quot; &lt;&lt; size &lt;&lt; &quot; failed&quot;;
}
// 与CaffeMallocHost对应
inline void CaffeFreeHost(void* ptr, bool use_cuda) {
#ifndef CPU_ONLY
  if (use_cuda) {
    CUDA_CHECK(cudaFreeHost(ptr));
    return;
  }
#endif
#ifdef USE_MKL
  mkl_free(ptr);
#else
  free(ptr);
#endif
}

//该类负责存储分配以及主机和设备间同步
class SyncedMemory {
 public:
 //构造函数
  SyncedMemory();
  //显式构造函数
  explicit SyncedMemory(size_t size);
  //析构函数
  ~SyncedMemory();
  const void* cpu_data();       //只读获取cpu data
  void set_cpu_data(void* data);    //设置cpu data
  const void* gpu_data();       //只读获取gpu data
  void set_gpu_data(void* data);    //设置gpu data
  void* mutable_cpu_data();     // 读写获取 cpu data
  void* mutable_gpu_data();     // 读写获取 gpu data
  //状态机变量，表示4种状态：术初始化、CPU数据奋效、GPU数据有效、己同步
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  //获得当前状态机变量值
  SyncedHead head() { return head_; }
  //获得当前存储空间尺寸
  size_t size() { return size_; }

#ifndef CPU_ONLY
  void async_gpu_push(const cudaStream_t&amp; stream);
#endif

 private:
  void check_device();

  void to_cpu();    //数据同步至CPU
  void to_gpu();    //数据同步至GPU
  void* cpu_ptr_;   //位于CPU的数据指针
  void* gpu_ptr_;   //位于GPU的数据指针
  size_t size_;     //存储空间大小
  SyncedHead head_; //状态机变量
  bool own_cpu_data_;   //标志是否拥有CPU数据所有权（否，即从别的对象共享)
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;   ////标志是否拥有GPU数据所有权
  int device_;      //设备号

  DISABLE_COPY_AND_ASSIGN(SyncedMemory);
};  // class SyncedMemory

}  // namespace caffe

#endif  // CAFFE_SYNCEDMEM_HPP_

</code></pre>

<p>Blob类实现的源码位于<code>src/caffe/blob.cpp</code>中，内容如下:</p>

<pre><code class="language-c++">
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {
//变维函数，将（num, channels, height, width}参数转换为vector&lt;int&gt;，然后调用重载的变维函数void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape)
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
//真正变维函数
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes); //保证vector维度&lt;=kMaxBlobAxes
  count_ = 1;   //用于计算元素总数=num * channels * height * width 
  shape_.resize(shape.size());  //成员变量维度也被重罝
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);  // 保证每维度尺寸都&gt;=0
    if (count_ != 0) {
    //证count_不溢出
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];     //count_累乘
    shape_[i] = shape[i];   //为成员变量赋值
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {     //如果新的count_大于当前己分f配空间容量
    capacity_ = count_;         //扩容，重新分配data_和dif f_空间
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[caffe数据结构]]></title>
    <link href="https://lockxmonk.github.io/14969765744787.html"/>
    <updated>2017-06-09T10:49:34+08:00</updated>
    <id>https://lockxmonk.github.io/14969765744787.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Blob</a>
</li>
<li>
<a href="#toc_1">Blob的基本用法</a>
</li>
</ul>


<p>一个CNN网络是由多个Layer堆叠而成的.如图所示:<br/>
<img src="media/14966518170972/14966520513325.jpg" alt=""/></p>

<p>caffe按照我们设计的图纸(prototxt),用Blob这些砖块建成一层层(Layer)楼房,最后通过方法SGD方法(Solver)进行简装修(Train),精装修(Finetune)实现的.我们这里就是学习这些基本概念.</p>

<h2 id="toc_0">Blob</h2>

<p>Caffe使用称为Blob的4维数组用于存储和交换数据.Blob提供了统一的存储器接口,持有一批图像或其它数据,权值,权值更新值. 其它机器学习框架也有类似的数据结构.</p>

<p><strong>Blob在内存中为4维数组,分别为<code>(width_,height_,channels_,num_)</code>,width_和height_表示图像的宽和高,channel_表示颜色通道RGB,num_表示第几帧,用于存储数据或权值(data)和权值增量(diff),在进行网路计算时,每层的输入,输出都需要Blob对象缓冲.Blob是Caffe的基本存储单元.</strong></p>

<h2 id="toc_1">Blob的基本用法</h2>

<p>Blob是一个模板类,所以创建对象时需要制定模板参数.我们这里写一个简单的测试程序<code>blob_demo.cpp</code>将它放在<code>caffe</code>的安装目录下:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>上面代码首先创建了整型Blob对象a,打印其维度信息,然后调用其<code>Reshape()</code>方法,再次打印其维度信息.</p>

<p>使用如下命令来编译上面的文件.</p>

<pre><code>g++ -o app blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe
</code></pre>

<p>生成了可执行程序<code>app</code></p>

<p>这个时候运行<code>app</code>的话可能会遇到下面这个错误:<br/>
<img src="media/14966518170972/14968048615065.jpg" alt=""/><br/>
这个因为<code>app</code>没有链接到这个动态库文件,执行下边这个命令链接:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app
</code></pre>

<p><code>/usr/local/Cellar/caffe/build/lib/</code>为<code>@rpath/libcaffe.so.1.0.0</code>动态库的路径.</p>

<p>执行后,再次运行会遇到错误:<br/>
<img src="media/14966518170972/14968049981988.jpg" alt=""/></p>

<p>与上面类似,这是因为没有链接到<code>@rpath/libhdf5_hl.10.dylib</code><br/>
执行下面这个命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/lib/libcaffe.so.1.0.0
</code></pre>

<p>其中<code>/Users/liangzhonghao/anaconda2/lib</code>包含这个库文件.</p>

<p>再次执行app,终于成功了!<br/>
<img src="media/14966518170972/14968051024802.jpg" alt=""/></p>

<p>创建了Blob对象之后,我们可以通过<code>mutable_cpu[gpu]_data[diff]</code>函数来修改其内部数值:</p>

<p>代码为:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    for(int i=0;i&lt;a.count();i++){
        p[i]=i;
    }
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    return 0;
}
</code></pre>

<p>跟上面一样继续编译和执行,这里按照上面的命令继续来编译的话,遇到了一个错误:<br/>
<img src="media/14966518170972/14968061663863.jpg" alt=""/></p>

<p>之后换成下边的命令执行后成功:</p>

<pre><code>g++ -o app2 blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe -lglog -lboost_system -lprotobuf
</code></pre>

<p>差别在于,后边加上了<code>-lglog -lboost_system -lprotobuf</code>命令,具体作用后续将研究(暂时不理解),继续运行后,又出现了错误:<br/>
<img src="media/14966518170972/14968063024883.jpg" alt=""/></p>

<p>同样是动态库的连接问题:<br/>
运行命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app2
</code></pre>

<p>执行命令,然后运行<code>app2</code>.得到输出:<br/>
<img src="media/14966518170972/14968063915684.jpg" alt=""/></p>

<p><strong>可见,Blob下标的访问与c/c++高维数组几乎一致,而Blob好处在于可以直接同步CPU/GPU上的数据.</strong></p>

<p>Blob还支持计算所有元素的绝对值之和(L1-范数),平方和(L2-范数):</p>

<pre><code>cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
</code></pre>

<p>输出结果为:</p>

<pre><code>ASUM = 276
SUMSQ = 4324

</code></pre>

<p>除了data,我们还可以改diff部分,与data的操作基本一致:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //将data初始化为1,2,3....
        q[i]= a.count()-1-i;   //将diff初始化为23,22,21,...
    }
    
    a.Update();         //执行update操作,将diff与data融合,这也是CNN权值更新步骤的最终实施者
   
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
    
    return 0;
}

</code></pre>

<p>然后执行以下命令编译,链接库文件:</p>

<pre><code>g++ -o app blob_demo_diff.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe  -lglog -lboost_system -lprotobuf

install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/LZHcaffe/./app
</code></pre>

<p><img src="media/14969765744787/14969953318311.jpg" alt=""/></p>

<p>运行.app,结果为:<br/>
<img src="media/14969765744787/14969953744774.jpg" alt=""/></p>

<p>上面表明,在<code>Update()</code>函数中,实现了<code>data = data -diff</code>操作,这个主要是在CNN权值更新时会用到,后面继续学习.</p>

<p>将Blob内部值保存到硬盘,或者冲硬盘载入到内存,可以分别通过<code>ToProto(),FromProto()</code>实现:</p>

<pre><code class="language-c++">
#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
#include&lt;caffe/util/io.hpp&gt;   //需要包含这个头文件
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //将data初始化为1,2,3....
        q[i]= a.count()-1-i;   //将diff初始化为23,22,21,...
    }
    
    a.Update();         //执行update操作,将diff与data融合,这也是CNN权值更新步骤的最终实施者
   
    BlobProto bp;          //构造一个BlobProto对象
    a.ToProto(&amp;bp,true);    //将a序列化,连同diff(默认不带)
    WriteProtoToBinaryFile(bp,&quot;a.blob&quot;);     //写入磁盘文件&quot;a.blob&quot;
    BlobProto bp2;           //构造一个新的BlobProto对象
    ReadProtoFromBinaryFileOrDie(&quot;a.blob&quot;,&amp;bp2);    //读取磁盘文件
    Blob&lt;float&gt; b;          //新建一个Blob对象b
    b.FromProto(bp2,true);  //从序列化对象bp2中克隆b(连同形状)
    
    for(int u=0;u&lt;b.num();u++){
        for(int v=0;v&lt;b.channels();v++){
            for(int w=0;w&lt;b.height();w++){
                for(int x=0;x&lt;b.width();x++){
                    cout&lt;&lt;&quot;b[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;b.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;b.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;b.sumsq_data()&lt;&lt;endl;
    
    
    return 0;
}

</code></pre>

<p>编译,连接库文件后(注意编译时末尾加入<code>&quot;-lglog -lboost_system -lprotobuf&quot;</code>选项),输出如下:</p>

<p><img src="media/14969765744787/14969964533804.jpg" alt=""/></p>

<p>可以发现与上面没有差别,只是在文件夹中多了一个<code>Blob.a</code>文件,<strong>所以<code>BlobProto</code>对象实现了硬盘与内存之间的数据通信.可以帮助保存中间权值和数据</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[激活函数]]></title>
    <link href="https://lockxmonk.github.io/14964511552671.html"/>
    <updated>2017-06-03T08:52:35+08:00</updated>
    <id>https://lockxmonk.github.io/14964511552671.html</id>
    <content type="html"><![CDATA[
<p>深度神经网络之所以具有丰富的表达能力，除了有深层次的网络之外，还有一个重要因素即非线性处理单元,称为激活函数（Activation Function)或挤压函数（Squashing Function).<strong>所以我们必须要关注怎么在caffe中实现这些函数.</strong></p>

<p>下图是一个神经元模型.\(\varphi(.)\)为激活函数.主要作用是将上一层的输入线性组合结果\(u_k\)动态范围压缩到特定值域(例如[-1,1]).一般来说具备非线性处理单元的深度神经网络(大于等于3层),理论上可以逼近任意函数.<br/>
<img src="media/14964511552671/14964521741597.jpg" alt=""/></p>

<p>其中几个常用的激活函数如下:<br/>
1.Sigmoid函数,值域为(0,1)<br/>
\[<br/>
\varphi(x) = \frac{1}{1+e^{-ax}}<br/>
\]<br/>
<img src="media/14964511552671/14964523348860.jpg" alt=""/></p>

<p>2.tanh函数,值域为(-1,1):<br/>
\[<br/>
\varphi(x) = \frac{1-e^{-2x}}{1+e^{-2x}}<br/>
\]<br/>
<img src="media/14964511552671/14964526906602.jpg" alt=""/></p>

<p>3.ReLu(Rectified Linear Unit，规整化线性单元)函数,值域为\([0,+ \infty)\),是一种非饱和激活函数.<br/>
\[<br/>
\varphi(x) = max(0,x)<br/>
\]<br/>
<img src="media/14964511552671/14964530576767.jpg" alt=""/></p>

<p>远不止上面这些激活函数,随着发展,陆续又出现了很多激活函数.这里不多介绍.后面还要自学很多这类相关知识.</p>

<p>神经网络中最大的问题是梯度消失问题（Gradient Vanishing Problem),这在使用 <code>Sigmoid、tanh</code>等饱和激活函数情况下尤为严重(神经网络进行误差反向传播时，各层都要乘以激活函数的一阶导数\(G=e\cdot \varphi&#39;(x) \cdot x\)),梯度每传递一层都会衰减一次,网络层数较多时,梯度G就会不停的衰减至消失),使得训练网络时收敛极慢,而ReLU这类非饱和激活函数收敛速度就快很多.所以学习网络模型中一般都会选用类似ReLu这种死活函数.</p>

<p>接下来我们学习在caffe用代码实现对应层的计算,包括前向传播计算和反向传播计算.Caffe的所有与激活函数相关的Layer类声明在<code>include/caffe/layers</code>文件夹中分别为<code>sigmoid_layer.hpp,relu_layer.hpp,tanh_layer.hpp</code>,我们将它们统称为<strong>非线性层</strong>,我们重点关注<code>ReLULayer,SigmoidLayer和TanHLayer</code>这三类.</p>

<p>在前面我们测试的LeNet-5模型中使用了ReLu层,我们在<code>example/mnist/lenet_train_test.prototxt</code>中找到描述:</p>

<pre><code>layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
</code></pre>

<p>与卷积层、全连接层最大的不同,就是没有权值相关的参数，描述相对简单。另外两种层没有实际样例，怎么办呢？这时按照我们的Caffe源码阅读方法论.从<code>src/caffe/proto/caffe.proto</code>中获得灵感。</p>

<pre><code class="language-c++">// ReLU层参数
message ReLUParameter {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  // Leaky ReLU参数，我们暂不关心
  optional float negative_slope = 1 [default = 0];
  enum Engine {     //计算引擎选择
    DEFAULT = 0;
    CAFFE = 1;      // Caffe 实现
    CUDNN = 2;      // CUDNN 实现
  }
  optional Engine engine = 2 [default = DEFAULT];
}
</code></pre>

<pre><code class="language-c++">// Sigmoid层参数
message SigmoidParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}

</code></pre>

<pre><code class="language-c++">//  tanh 层参数
message TanHParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}
</code></pre>

<p>非线性层的共同特点就是对前一层blob中的数值逐一进行非线性变换，并放回原blob中。激活函数的类声明如下:</p>

<pre><code class="language-c++">namespace caffe {
//非线性层的鼻祖NeuronLayer，派生于Layer类，特点是输出blob(y)与输入blob(x)尺寸相同

/**
 * @brief An interface for layers that take one blob as input (@f$ x @f$)
 *        and produce one equally-sized blob as output (@f$ y @f$), where
 *        each element of the output depends only on the corresponding input
 *        element.
 */
template &lt;typename Dtype&gt;
class NeuronLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit NeuronLayer(const LayerParameter&amp; param)
     : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline int ExactNumBottomBlobs() const { return 1; }
  virtual inline int ExactNumTopBlobs() const { return 1; }
};

}  // namespace caffe

#endif  // CAFFE_NEURON_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// ReLULayer，派生于NeuronLayer，实现了ReLu激活函数计算

/**
 * @brief Rectified Linear Unit non-linearity @f$ y = \max(0, x) @f$.
 *        The simple max is fast to compute, and the function does not saturate.
 */
template &lt;typename Dtype&gt;
class ReLULayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
 
  /**
   * @param param provides ReLUParameter relu_param,
   *     with ReLULayer options:
   *   - negative_slope (\b optional, default 0).
   *     the value @f$ \nu @f$ by which negative values are multiplied.
   */
  explicit ReLULayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;ReLU&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \max(0, x)
   *      @f$ by default.  If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed outputs are @f$ y = \max(0, x) + \nu \min(0, x) @f$.
   */
   //前向传波函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the ReLU inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            0 &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$ if propagate_down[0], by default.
   *      If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed gradients are @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            \nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$.
   */
   
   //反向传波函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_RELU_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// SigmoidLayer,派生于NeuronLayer，实现了Sigmoid激活函数的计算
/**
 * @brief Sigmoid function non-linearity @f$
 *         y = (1 + \exp(-x))^{-1}
 *     @f$, a classic choice in neural networks.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class SigmoidLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
  explicit SigmoidLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;Sigmoid&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = (1 + \exp(-x))^{-1}
   *      @f$
   */
   
   //前向传播函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y} y (1 - y)
   *      @f$ if propagate_down[0]
   */
   
   //反向传播函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_SIGMOID_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// TanHLayer，派生于NeuronLayer，实现了tanh激活函数计算
/**
 * @brief TanH hyperbolic tangent non-linearity @f$
 *         y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
 *     @f$, popular in auto-encoders.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class TanHLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //显式构造函数
  explicit TanHLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//返回类名字符串
  virtual inline const char* type() const { return &quot;TanH&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
   *      @f$
   */
   
   //前向传播函数
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y}
   *              \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
   *            = \frac{\partial E}{\partial y} (1 - y^2)
   *      @f$ if propagate_down[0]
   */
   
   //反向传播函数
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_TANH_LAYER_HPP_
</code></pre>

<p>上面类的声明比较简单,各自声明了Forward和Backward函数.下面对这些函数的实现进行解析.我们首先看下<code>src/caffe/layers/relu_layer.cpp</code>中前向传播函数的实现代码。</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // (只读) 获得输人blob的data指针
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  // (读写）获得输出blob的data指针
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  //获得输入blob元素个数
  const int count = bottom[0]-&gt;count();
  // Leaky ReLU参数，从layer_param中获得，默认为0，即普通ReLU
  Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
  //执行ReLU操作我们姑且认为negative_slop值为0,不考虑Leaky ReLU
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = std::max(bottom_data[i], Dtype(0))
        + negative_slope * std::min(bottom_data[i], Dtype(0));
  }
}
</code></pre>

<p>不出所料，用一层for循环就搞定了,下面我们来看<strong>反向传播函数</strong>的实现代码.</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // 如果需要做反向传播计算
  if (propagate_down[0]) {
    //(只读）获得前一层的data指针
    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
    //(只读) 获得后一层的diff指针
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    //(读写) 获得前一层的diff指针
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    //获得要参计算的元素总数
    const int count = bottom[0]-&gt;count();
    // Leaky ReLU参数，姑且认为是0
    Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
    for (int i = 0; i &lt; count; ++i) {
    // ReLU的导函数就是（bottom_data[i] &gt; 0)，根据求导链式法则，后一层的误差乘以导函数得到前一层的误差
      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)
          + negative_slope * (bottom_data[i] &lt;= 0));
    }
  }
}
</code></pre>

<p>到这里可以看到ReLu计算非常简单(目前如此)</p>

<p>其它激活函数源码,之后也许用的比较少,这里不做多的介绍.</p>

<p>所以,非线性层虽然公式表示较为复杂,但代码实现都非常简洁、直观，只要掌握了基本求导技巧，同样可以推导出非线性层其他类的反向传播公式.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UDP编程]]></title>
    <link href="https://lockxmonk.github.io/14957648679184.html"/>
    <updated>2017-05-26T10:14:27+08:00</updated>
    <id>https://lockxmonk.github.io/14957648679184.html</id>
    <content type="html"><![CDATA[
<p>TCP是建立可靠连接，并且通信双方都可以以流的形式发送数据。相对TCP，UDP则是<strong>面向无连接</strong>的协议。</p>

<p><font color=red>使用UDP协议时，不需要建立连接，只需要知道对方的IP地址和端口号，就可以直接发数据包。但是，能不能到达就不知道了。</p>

<p>虽然用UDP传输数据不可靠，但它的优点是和TCP比，速度快，对于不要求可靠到达的数据，就可以使用UDP协议。</font></p>

<p>我们来看看如何通过UDP协议传输数据。和TCP类似，使用<mark>UDP的通信双方也分为客户端和服务器</mark>。服务器首先需要绑定端口：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# 绑定端口:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>创建Socket时，<code>SOCK_DGRAM</code>指定了这个Socket的类型是UDP。绑定端口和TCP一样，但是不需要调用<code>listen()</code>方法，而是直接接收来自任何客户端的数据：</p>

<pre><code class="language-py">print &#39;Bind UDP on 9999...&#39;
while True:
    # 接收数据:
    data, addr = s.recvfrom(1024)
    print &#39;Received from %s:%s.&#39; % addr
    s.sendto(&#39;Hello, %s!&#39; % data, addr)
</code></pre>

<p><code>recvfrom()</code>方法返回数据和客户端的地址与端口，这样，服务器收到数据后，直接调用<code>sendto()</code>就可以把数据用UDP发给客户端。</p>

<p>注意这里省掉了多线程，因为这个例子很简单。</p>

<p>客户端使用UDP时，首先仍然创建基于UDP的Socket，然后，不需要调用<code>connect()</code>，直接通过<code>sendto()</code>给服务器发数据：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # 发送数据:
    s.sendto(data, (&#39;127.0.0.1&#39;, 9999))
    # 接收数据:
    print s.recv(1024)
s.close()
</code></pre>

<p>客户端:<br/>
<img src="media/14957648679184/14957678758780.jpg" alt=""/><br/>
服务器:<br/>
<img src="media/14957648679184/14957678951579.jpg" alt=""/><br/>
客户端从服务器接收数据仍然调用<code>recv()</code>方法。</p>

<h2 id="toc_0">小结</h2>

<p>UDP的使用与TCP类似，但是不需要建立连接。此外，服务器绑定UDP端口和TCP端口互不冲突，也就是说，UDP的9999端口与TCP的9999端口可以各自绑定。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[网络编程]]></title>
    <link href="https://lockxmonk.github.io/14957594179111.html"/>
    <updated>2017-05-26T08:43:37+08:00</updated>
    <id>https://lockxmonk.github.io/14957594179111.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">TCP/IP</a>
</li>
<li>
<a href="#toc_1">TCP编程</a>
<ul>
<li>
<a href="#toc_2">客户端</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">服务器</a>
</li>
<li>
<a href="#toc_4">小结</a>
</li>
</ul>


<h2 id="toc_0">TCP/IP</h2>

<p>现在的网络编程基本都是在一个统一的通用写一下来进行的,<code>互联网协议簇（Internet Protocol Suite）</code>就是通用协议标准。Internet是由inter和net两个单词组合起来的，原意就是连接“网络”的网络，有了Internet，任何私有网络，只要支持这个协议，就可以联入互联网。</p>

<p>因为互联网协议包含了上百种协议标准，但是最重要的两个协议是TCP和IP协议，所以，大家把互联网的协议简称<strong>TCP/IP</strong>协议。</p>

<p>通信的时候，双方必须知道对方的标识，好比发邮件必须知道对方的邮件地址。互联网上每个计算机的唯一标识就是IP地址，类似123.123.123.123。如果一台计算机同时接入到两个或更多的网络，比如路由器，它就会有两个或多个IP地址，所以，IP地址对应的实际上是计算机的网络接口，通常是网卡。</p>

<p><strong>IP协议负责把数据从一台计算机通过网络发送到另一台计算机。数据被分割成一小块一小块，然后通过IP包发送出去。</strong>由于互联网链路复杂，两台计算机之间经常有多条线路，因此，<strong>路由器就负责决定如何把一个IP包转发出去。IP包的特点是按块发送，途径多个路由，但不保证能到达，也不保证顺序到达。</strong></p>

<p><font color=red>TCP协议则是建立在IP协议之上的。TCP协议负责在两台计算机之间建立可靠连接，保证数据包按顺序到达。TCP协议会通过握手建立连接，然后，对每个IP包编号，确保对方按顺序收到，如果包丢掉了，就自动重发。</font></p>

<p>许多常用的更高级的协议都是建立在TCP协议基础上的，比如用于浏览器的HTTP协议、发送邮件的SMTP协议等。</p>

<p>一个IP包除了包含要传输的数据外，还包含源IP地址和目标IP地址，源端口和目标端口。</p>

<p>端口有什么作用？在两台计算机通信时，只发IP地址是不够的，因为同一台计算机上跑着多个网络程序。一个IP包来了之后，到底是交给浏览器还是QQ，就需要<strong>端口号</strong>来区分。每个网络程序都向操作系统申请唯一的端口号，这样，两个进程在两台计算机之间建立网络连接就需要各自的IP地址和各自的端口号。(在进行tomcat设置的时候,类似也是需要设置端口号.)</p>

<p>一个进程也可能同时与多个计算机建立链接，因此它会申请很多端口。</p>

<h2 id="toc_1">TCP编程</h2>

<p>Socket是网络编程的一个抽象概念。通常我们用一个Socket表示“打开了一个网络链接”，而打开一个Socket需要知道目标计算机的IP地址和端口号，再指定协议类型即可。</p>

<h3 id="toc_2">客户端</h3>

<p>大多数连接都是可靠的TCP连接。创建TCP连接时，主动发起连接的叫客户端，被动响应连接的叫服务器。</p>

<p>我们要创建一个基于TCP连接的Socket，可以这样做：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import socket

s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

s = connect((&#39;www.sina.com.cn&#39;,80))
</code></pre>

<p>创建Socket时，<code>AF_INET</code>指定使用<code>IPv4</code>协议，如果要用更先进的<code>IPv6</code>，就指定为<code>AF_INET6</code>。<code>SOCK_STREAM</code>指定使用面向流的TCP协议，这样，一个<code>Socket</code>对象就创建成功，但是还没有建立连接。</p>

<p>客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号。新浪网站的IP地址可以用域名<code>www.sina.com.cn</code>自动转换到IP地址，但是怎么知道新浪服务器的端口号呢？</p>

<p>答案是作为服务器，提供什么样的服务，端口号就必须固定下来。由于我们想要访问网页，因此新浪提供网页服务的服务器必须把端口号固定在<code>80</code>端口，因为<code>80</code>端口是Web服务的标准端口。其他服务都有对应的标准端口号，例如SMTP服务是25端口，FTP服务是21端口，等等。端口号小于1024的是Internet标准服务的端口，端口号大于1024的，可以任意使用。</p>

<p>因此，我们连接新浪服务器的代码如下：</p>

<pre><code class="language-py">s.connect((&#39;www.sina.com.cn&#39;, 80))
</code></pre>

<p><strong>注意参数是一个tuple，包含地址和端口号。</strong></p>

<p>建立TCP连接后，我们就可以向新浪服务器发送请求，要求返回首页的内容：</p>

<pre><code class="language-py">s.send(&#39;GET / HTTP/1.1\r\nHost: www.sina.com.cn\r\nConnection: close\r\n\r\n&#39;)
</code></pre>

<p>TCP连接创建的是双向通道，双方都可以同时给对方发数据。但是谁先发谁后发，怎么协调，要根据具体的协议来决定。例如，HTTP协议规定客户端必须先发请求给服务器，服务器收到后才发数据给客户端。</p>

<p>发送的文本格式必须符合HTTP标准，如果格式没问题，接下来就可以接收新浪服务器返回的数据了：</p>

<pre><code class="language-py"># 接收数据:
buffer = []
while True:
    # 每次最多接收1k字节:
    d = s.recv(1024)
    if d:
        buffer.append(d)
    else:
        break
data = &#39;&#39;.join(buffer)
</code></pre>

<p>接收数据时，调用<code>recv(max)</code>方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到<code>recv()</code>返回空数据，表示接收完毕，退出循环。</p>

<p>当我们接收完数据后，调用<code>close()</code>方法关闭<code>Socket</code>，这样，一次完整的网络通信就结束了：</p>

<pre><code class="language-py"># 关闭连接:
s.close()
</code></pre>

<p>接收到的数据包括<code>HTTP头</code>和<code>网页本身</code>，我们只需要把HTTP头和网页分离一下，把<code>HTTP头</code>打印出来，网页内容保存到文件：</p>

<pre><code class="language-py">header, html = data.split(&#39;\r\n\r\n&#39;, 1)
print header
# 把接收的数据写入文件:
with open(&#39;sina.html&#39;, &#39;wb&#39;) as f:
    f.write(html)
</code></pre>

<p>现在，只需要在浏览器中打开这个<code>sina.html</code>文件，就可以看到新浪的首页了。</p>

<h2 id="toc_3">服务器</h2>

<p>和客户端编程相比，服务器编程就要复杂一些。</p>

<p>服务器进程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了。</p>

<p>所以，服务器会打开固定端口（比如80）监听，每来一个客户端连接，就创建该Socket连接。由于服务器会有大量来自客户端的连接，所以，服务器要能够区分一个Socket连接是和哪个客户端绑定的。一个Socket依赖4项：服务器地址、服务器端口、客户端地址、客户端端口来唯一确定一个Socket。</p>

<p>但是服务器还需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。</p>

<p>我们来编写一个简单的服务器程序，它接收客户端连接，把客户端发过来的字符串加上<code>Hello</code>再发回去。</p>

<p>首先，创建一个基于IPv4和TCP协议的Socket：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
</code></pre>

<p>然后，我们要绑定监听的地址和端口。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用<code>0.0.0.0</code>绑定到所有的网络地址，还可以用<code>127.0.0.1</code>绑定到本机地址。<code>127.0.0.1</code>是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。</p>

<p>端口号需要预先指定。因为我们写的这个服务不是标准服务，所以用<code>9999</code>这个端口号。请注意，小于<code>1024</code>的端口号必须要有管理员权限才能绑定：</p>

<pre><code class="language-py"># 监听端口:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>紧接着，调用listen()方法开始监听端口，传入的参数指定等待连接的最大数量：<br/>
<code>py<br/>
s.listen(5)<br/>
print &#39;Waiting for connection...&#39;<br/>
</code></p>

<p>接下来，服务器程序通过一个永久循环来接受来自客户端的连接，<code>accept()</code>会等待并返回一个客户端的连接:</p>

<pre><code class="language-py">while True:
    # 接受一个新连接:
    sock, addr = s.accept()
    # 创建新线程来处理TCP连接:
    t = threading.Thread(target=tcplink, args=(sock, addr))
    t.start()
</code></pre>

<p>每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接：</p>

<pre><code class="language-py">def tcplink(sock, addr):
    print &#39;Accept new connection from %s:%s...&#39; % addr
    sock.send(&#39;Welcome!&#39;)
    while True:
        data = sock.recv(1024)
        time.sleep(1)
        if data == &#39;exit&#39; or not data:
            break
        sock.send(&#39;Hello, %s!&#39; % data)
    sock.close()
    print &#39;Connection from %s:%s closed.&#39; % addr
</code></pre>

<p>连接建立后，服务器首先发一条欢迎消息，然后等待客户端数据，并加上<code>Hello</code>再发送给客户端。如果客户端发送了<code>exit</code>字符串，就直接关闭连接。</p>

<p>要测试这个服务器程序，我们还需要编写一个客户端程序：</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# 建立连接:
s.connect((&#39;127.0.0.1&#39;, 9999))
# 接收欢迎消息:
print s.recv(1024)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # 发送数据:
    s.send(data)
    print s.recv(1024)
s.send(&#39;exit&#39;)
s.close()
</code></pre>

<p>我们需要打开两个命令行窗口，一个运行服务器程序，另一个运行客户端程序，就可以看到效果了：<br/>
(先运行服务器端程序)<br/>
执行了两次客户端程序:<br/>
<img src="media/14957594179111/14957645115702.jpg" alt=""/><br/>
服务端:<br/>
<img src="media/14957594179111/14957645546966.jpg" alt=""/><br/>
需要注意的是，客户端程序运行完毕就退出了，而服务器程序会永远运行下去，必须按Ctrl+C退出程序。</p>

<h2 id="toc_4">小结</h2>

<p>用TCP协议进行Socket编程在Python中十分简单，对于客户端，要主动连接服务器的IP和指定端口，对于服务器，要首先监听指定端口，然后，对每一个新的连接，创建一个线程或进程来处理。通常，服务器程序会无限运行下去。</p>

<p>同一个端口，被一个Socket绑定了以后，就不能被别的Socket绑定了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图形界面]]></title>
    <link href="https://lockxmonk.github.io/14957005904776.html"/>
    <updated>2017-05-25T16:23:10+08:00</updated>
    <id>https://lockxmonk.github.io/14957005904776.html</id>
    <content type="html"><![CDATA[
<p>Python支持多种图形界面的第三方库，包括：<br/>
* Tk<br/>
* wxWidgets<br/>
* Qt<br/>
* GTK<br/>
等等.</p>

<p>但是Python自带的库是支持Tk的Tkinter，使用Tkinter，无需安装任何包，就可以直接使用。本章简单介绍如何使用Tkinter进行GUI编程。</p>

<h2 id="toc_0">Tkinter</h2>

<p>我们来梳理一下概念：</p>

<p>我们编写的Python代码会调用内置的Tkinter，Tkinter封装了访问Tk的接口；</p>

<p>Tk是一个图形库，支持多个操作系统，使用Tcl语言开发；</p>

<p>Tk会调用操作系统提供的本地GUI接口，完成最终的GUI。</p>

<p>所以，我们的代码只需要调用Tkinter提供的接口就可以了。</p>

<h2 id="toc_1">第一个GUI程序</h2>

<p>使用Tkinter十分简单，我们来编写一个GUI版本的“Hello, world!”。</p>

<p>第一步是导入Tkinter包的所有内容：</p>

<pre><code class="language-py">from Tkinter import *
</code></pre>

<p>第二步是从<code>Frame</code>派生一个<code>Application</code>类，这是所有Widget的父容器：</p>

<pre><code class="language-py">
class Applicition(Frame):
    def __init__(self, master=None):
        Frame.__init__(self,master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.helloLabel = Label(self,text = &#39;Hello,world&#39;)
        self.helloLabel.pack()
        self.quitButton = Button(self, text = &#39;Quit&#39;,command = self.quit)
        self.quitButton.pack()

</code></pre>

<p>在GUI中，每个Button、Label、输入框等，都是一个Widget。Frame则是可以容纳其他Widget的Widget，所有的Widget组合起来就是一棵树。</p>

<p><code>pack()</code>方法把Widget加入到父容器中，并实现布局。<code>pack()</code>是最简单的布局，<code>grid()</code>可以实现更复杂的布局。</p>

<p>在<code>createWidgets()</code>方法中，我们创建一个<code>Label</code>和一个<code>Button</code>，当Button被点击时，触发<code>self.quit()</code>使程序退出。</p>

<p>第三步，实例化<code>Application</code>，并启动消息循环：</p>

<pre><code class="language-py">app = Application()
# 设置窗口标题:
app.master.title(&#39;Hello World&#39;)
# 主消息循环:
app.mainloop()
</code></pre>

<p>GUI程序的主线程负责监听来自操作系统的消息，并依次处理每一条消息。因此，如果消息处理非常耗时，就需要在新线程中处理。</p>

<p>运行这个GUI程序，可以看到下面的窗口：<br/>
<img src="media/14957005904776/14957017137688.jpg" alt=""/><br/>
点击“Quit”按钮或者窗口的“x”结束程序。</p>

<h2 id="toc_2">输入文本</h2>

<p>我们再对这个GUI程序改进一下，加入一个文本框，让用户可以输入文本，然后点按钮后，弹出消息对话框。</p>

<pre><code class="language-py">from Tkinter import *
import tkMessageBox


class Applicition(Frame):

    def __init__(self, master=None):
        Frame.__init__(self, master)
        self.pack()
        self.creatWidgets()

    def creatWidgets(self):
        self.nameInput = Entry(self)
        self.nameInput.pack()
        self.alertButton = Button(self, text=&#39;Hello&#39;, command=self.hello)
        self.alertButton.pack()

    def hello(self):
        name = self.nameInput.get() or &#39;world&#39;
        tkMessageBox.showinfo(&#39;Message&#39;, &#39;Hello,%s&#39; % name)

app = Applicition()
app.master.title(&#39;Hello World&#39;)
app.mainloop()

</code></pre>

<p><img src="media/14957005904776/14957022189285.jpg" alt=""/></p>

<p>当用户点击按钮时，触发<code>hello()</code>，通过<code>self.nameInput.get()</code>获得用户输入的文本后，使用<code>tkMessageBox.showinfo()</code>可以弹出消息对话框。</p>

<h2 id="toc_3">小结</h2>

<p>Python内置的Tkinter可以满足基本的GUI程序的要求，如果是非常复杂的GUI程序，建议用操作系统原生支持的语言和库来编写。</p>

<p>源码参考：<a href="https://github.com/michaelliao/learn-python/tree/master/gui">https://github.com/michaelliao/learn-python/tree/master/gui</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HTMLParser]]></title>
    <link href="https://lockxmonk.github.io/14956930097908.html"/>
    <updated>2017-05-25T14:16:49+08:00</updated>
    <id>https://lockxmonk.github.io/14956930097908.html</id>
    <content type="html"><![CDATA[
<p>在Pythonz中我们有可能需要去解析一个爬下来的HTML,我们在Python中应该如何去解析呢?</p>

<p>好在Python提供了<code>HTMLParser</code>来非常方便地解析HTML，只需简单几行代码：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from HTMLParser import HTMLParser
from htmlentitydefs import name2codepoint

class MyHTMLParser(HTMLParser):
    def handle_starttag(self,tag,attrs):
        print (&#39;&lt;%s&gt;&#39; % tag)

    def handle_endtag(self,tag):
        print (&#39;&lt;/%s&gt;&#39; % tag)
    
    def handle_startendtag(self, tag, attrs):
        print(&#39;&lt;%s/&gt;&#39; % tag)

    def handle_data(self, data):
        print(&#39;data&#39;)

    def handle_comment(self, data):
        print(&#39;&lt;!-- --&gt;&#39;)

    def handle_entityref(self, name):
        print(&#39;&amp;%s;&#39; % name)

    def handle_charref(self, name):
        print(&#39;&amp;#%s;&#39; % name)
parser = MyHTMLParser()
parser.feed(&#39;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;Some &lt;a href=\&quot;#\&quot;&gt;html&lt;/a&gt; tutorial...&lt;br&gt;END&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#39;)

</code></pre>

<p><code>feed()</code>方法可以多次调用，也就是不一定一次把整个HTML字符串都塞进去，可以一部分一部分塞进去。</p>

<p>特殊字符有两种，一种是英文表示的<code>&amp;nbsp;</code>，一种是数字表示的<code>&amp;#1234;</code>，这两种字符都可以通过Parser解析出来。</p>

<h2 id="toc_0">练习</h2>

<p>找一个网页，例如<a href="https://www.python.org/events/python-events/">https://www.python.org/events/python-events/</a>，用浏览器查看源码并复制，然后尝试解析一下HTML，输出Python官网发布的会议时间、名称和地点。</p>

<p>这里我们要解析HTML之前,肯定要先获取该页面元素的代码.我们这里用到了<code>urllib</code>这个库,具体用法为:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import urllib

PythonPage = urllib.urlopen(&#39;https://www.python.org/events/python-events/&#39;)
pyhtml = PythonPage.read()  #读取该页面代码.
print pyhtml
</code></pre>

<p><img src="media/14956930097908/14956951701372.jpg" alt=""/><br/>
上面为结果,这里只是部分截图.</p>

<p>下面我们来继续研究上面的问题:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from HTMLParser import HTMLParser
from htmlentitydefs import name2codepoint
import urllib


class PyHTMLParser(HTMLParser):

    def __init__(self):
        HTMLParser.__init__(self)
        self._count = 0
        self._events = dict()
        self._flag = None

    def handle_starttag(self, tag, attrs):
        if tag == &#39;h3&#39; and attrs.__contains__((&#39;class&#39;, &#39;event-title&#39;)):
            self._count += 1
            self._events[self._count] = dict()
            self._flag = &#39;event-title&#39;
        if tag == &#39;time&#39;:
            self._flag = &#39;time&#39;
        if tag == &#39;span&#39; and attrs.__contains__((&#39;class&#39;, &#39;event-location&#39;)):
            self._flag = &#39;event-location&#39;

    def handle_data(self, data):
        if self._flag == &#39;event-title&#39;:
            self._events[self._count][self._flag] = data
        if self._flag == &#39;time&#39;:
            self._events[self._count][self._flag] = data
        if self._flag == &#39;event-location&#39;:
            self._events[self._count][self._flag] = data
        self._flag = None   #一定要设置为None,防止其它data误入

    def event_list(self):
        print self._events
        print &#39;近期关于Python的会议有：&#39;, self._count, &#39;个，具体如下：&#39;
        for event in self._events.values():
            print event[&#39;event-title&#39;], &#39;\t&#39;, event[&#39;time&#39;], &#39;\t&#39;, event[&#39;event-location&#39;]

PythonPage = urllib.urlopen(&#39;https://www.python.org/events/python-events/&#39;)
pyhtml = PythonPage.read()
parser = PyHTMLParser()
parser.feed(pyhtml)
parser.event_list()
</code></pre>

<p>这里我们将所遇到的属性,进行人为分类,将包含<code>&#39;event-title&#39;</code>,<code>&#39;time&#39;</code>,<code>&#39;event-location&#39;</code>关键字的属性聚类到一起,</p>

<p><img src="media/14956930097908/14956975668058.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[itertools]]></title>
    <link href="https://lockxmonk.github.io/14956760206333.html"/>
    <updated>2017-05-25T09:33:40+08:00</updated>
    <id>https://lockxmonk.github.io/14956760206333.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">chain()</a>
</li>
<li>
<a href="#toc_1">groupby()</a>
</li>
<li>
<a href="#toc_2">imap()</a>
</li>
<li>
<a href="#toc_3">ifilter()</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">小结</a>


<p>Python的内建模块<code>itertools</code>提供了非常有用的用于操作迭代对象的函数。</p>

<p>首先，我们看看<code>itertools</code>提供的几个“无限”迭代器：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

natuals = itertools.count(1)
for n in natuals:
    print n
</code></pre>

<p><img src="media/14956760206333/14956780529492.jpg" alt=""/></p>

<p>因为<code>count()</code>会创建一个无限的迭代器，所以上述代码会打印出自然数序列，根本停不下来，只能按<code>Ctrl+C</code>退出。(有假死的风险)</p>

<p><code>cycle()</code>会把传入的一个序列无限重复下去：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

cs = itertools.cycle(&#39;ABC&#39;)
for n in cs:
    print n
</code></pre>

<p><img src="media/14956760206333/14956783779668.jpg" alt=""/><br/>
同样停不下来。(很容易程序假死....)</p>

<p><code>repeat()</code>负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

ns = itertools.repeat(&#39;A&#39;,10)
for n in ns:
    print n
</code></pre>

<p><img src="media/14956760206333/14956789222152.jpg" alt=""/></p>

<p><strong>无限序列只有在<code>for</code>迭代时才会无限地迭代下去，如果只是创建了一个迭代对象，它不会事先把无限个元素生成出来，事实上也不可能在内存中创建无限多个元素。</strong></p>

<p>无限序列虽然可以无限迭代下去，但是通常我们会通过<code>takewhile()</code>等函数根据条件判断来截取出一个有限的序列：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
natuals = itertools.count(1)
ns = itertools.takewhile(lambda x:x&lt;=10, natuals)
for n in ns:
    print n
</code></pre>

<p><img src="media/14956760206333/14956791893037.jpg" alt=""/></p>

<p><code>itertools</code>提供的几个迭代器操作函数更加有用：</p>

<h2 id="toc_0">chain()</h2>

<p><code>chain()</code>可以把一组迭代对象串联起来，形成一个更大的迭代器：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for c in itertools.chain(&#39;ABC&#39;, &#39;XYZ&#39;):
    print c
</code></pre>

<p><img src="media/14956760206333/14956795463378.jpg" alt=""/></p>

<h2 id="toc_1">groupby()</h2>

<p><code>groupby()</code>把迭代器中相邻的重复元素挑出来放在一起：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for key,group  in itertools.groupby(&#39;AAAABBBBBCCCAAA&#39;):
    print key ,list(group)
</code></pre>

<p><img src="media/14956760206333/14956799374968.jpg" alt=""/><br/>
<strong>这里注意到打印group的时候用的是list(group),这是因为gruupby返回仍然是一个迭代器!!,迭代器中的元素可以用list(迭代器)来将元素显示出来,但是要注意不要用list来显示那些无限循环的迭代器(会死机....).</strong></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
cs = itertools.repeat(&#39;A&#39;, 10) # 注意字符串也是序列的一种
print list(cs)
</code></pre>

<p><img src="media/14956760206333/14956833066876.jpg" alt=""/></p>

<h2 id="toc_2">imap()</h2>

<p><code>imap()</code>和<code>map()</code>的区别在于，<code>imap()</code>可以作用于无穷序列，并且，如果两个序列的长度不一致，以短的那个为准。</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools
for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)):
         print x
</code></pre>

<p><img src="media/14956760206333/14956801958879.jpg" alt=""/></p>

<p>注意<code>imap()</code>返回一个迭代对象，而<code>map()</code>返回<code>list</code>。当你调用<code>map()</code>时，已经计算完毕,当你调用<code>imap()</code>时，并没有进行任何计算：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = map(lambda x: x*x, [1, 2, 3])
print r     # r已经计算出来了
n = itertools.imap(lambda x: x*x, [1, 2, 3])
print n     # n只是一个迭代对象

for x in n:
    print x
</code></pre>

<p><img src="media/14956760206333/14956804009136.jpg" alt=""/></p>

<p>必须用for循环对r进行迭代，才会在每次循环过程中计算出下一个元素.</p>

<p>这说明<code>imap()</code>实现了“惰性计算”，也就是在需要获得结果的时候才计算。类似<code>imap()</code>这样能够实现惰性计算的函数就可以处理无限序列：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = itertools.imap(lambda x: x*x, itertools.count(1)) 
for n in itertools.takewhile(lambda x: x&lt;100, r):
      print n
</code></pre>

<p><img src="media/14956760206333/14956809524908.jpg" alt=""/></p>

<p>如果把imap()换成map()去处理无限序列:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
import itertools

r = map(lambda x: x * x, itertools.count(1))
print r
# for n in itertools.takewhile(lambda x: x&lt;100, r):
# print n
</code></pre>

<p>会造成电脑死机,由于<code>map()</code>返回的是一个list，所以当用它去处理无限序列的时候，它会尝试计算完之后才返回，但是序列是无限的，所以它会一直计算下去，致使其占用的系统的内存越来越高。(很坑爹....)</p>

<h2 id="toc_3">ifilter()</h2>

<p>不用多说了，<code>ifilter()</code>就是<code>filter()</code>的惰性实现。</p>

<h1 id="toc_4">小结</h1>

<p><code>itertools</code>模块提供的全部是处理迭代功能的函数，它们的返回值不是<code>list</code>，而是迭代对象，只有用<code>for</code>循环迭代的时候才真正计算。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hashlib]]></title>
    <link href="https://lockxmonk.github.io/14956154840973.html"/>
    <updated>2017-05-24T16:44:44+08:00</updated>
    <id>https://lockxmonk.github.io/14956154840973.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">摘要算法简介</a>
</li>
</ul>
</li>
<li>
<a href="#toc_1">==MD5是最常见的摘要算法，速度很快，生成结果是固定的128 bit字节，通常用一个32位的16进制字符串表示。</a>
<ul>
<li>
<a href="#toc_2">摘要算法应用</a>
<ul>
<li>
<a href="#toc_3">练习:根据用户输入的口令，计算出存储在数据库中的MD5口令：</a>
</li>
<li>
<a href="#toc_4">练习：设计一个验证用户登录的函数，根据用户输入的口令是否正确，返回True或False：</a>
</li>
<li>
<a href="#toc_5">练习：根据用户输入的登录名和口令模拟用户注册，计算更安全的MD5：</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">小结</a>
</li>
</ul>


<h2 id="toc_0">摘要算法简介</h2>

<p>Python的hashlib提供了常见的摘要算法，如MD5，SHA1等等。</p>

<p>什么是摘要算法呢？摘要算法又称哈希算法、散列算法。它通过一个函数，把任意长度的数据转换为一个长度固定的数据串（通常用16进制的字符串表示）。</p>

<p>举个例子，你写了一篇文章，内容是一个字符串<code>&#39;how to use python hashlib - by Michael&#39;</code>，并附上这篇文章的摘要是<code>&#39;2d73d4f15c0db7f5ecb321b6a65e5d6d&#39;</code>。如果有人篡改了你的文章，并发表为<code>&#39;how to use python hashlib - by Bob&#39;</code>，你可以一下子指出Bob篡改了你的文章，因为根据<code>&#39;how to use python hashlib - by Bob&#39;</code>计算出的摘要不同于原始文章的摘要。</p>

<p>可见，摘要算法就是通过摘要函数<code>f()</code>对任意长度的数据<code>data</code>计算出固定长度的摘要<code>digest</code>，目的是为了发现原始数据是否被人篡改过。</p>

<p><font color=red><strong>摘要算法之所以能指出数据是否被篡改过，就是因为摘要函数是一个单向函数，计算f(data)很容易，但通过digest反推data却非常困难。而且，对原始数据做一个bit的修改，都会导致计算出的摘要完全不同。</strong></font></p>

<p>我们以常见的摘要算法MD5为例，计算出一个字符串的MD5值：</p>

<pre><code class="language-py">import hashlib

md5 = hashlib.md5()
md5.update(&#39;how to use md5 in python hashlib?&#39;)

print md5.hexdigest()
</code></pre>

<p>结果为:<br/>
<img src="media/14956154840973/14956157660396.jpg" alt=""/></p>

<p>如果数据量很大，可以分块多次调用<code>update()</code>，最后计算的结果是一样的：</p>

<pre><code class="language-py">import hashlib

md5 = hashlib.md5()
md5.update(&#39;how to use md5 in &#39;)
md5.update(&#39;python hashlib?&#39;)
print md5.hexdigest()

</code></pre>

<p>结果仍然为:<br/>
<img src="media/14956154840973/14956158401991.jpg" alt=""/></p>

<p>当改动一个字母之后,结果会完全不同.</p>

<h1 id="toc_1">==MD5是最常见的摘要算法，速度很快，生成结果是固定的128 bit字节，通常用一个32位的16进制字符串表示。</h1>

<p>另一种常见的摘要算法是<code>SHA1</code>，调用<code>SHA1</code>和调用<code>MD5</code>完全类似：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib

sha1 = hashlib.sha1()
sha1.update(&#39;how to use md5 in &#39;)
sha1.update(&#39;python hashlib?&#39;)
print sha1.hexdigest()
</code></pre>

<p><img src="media/14956154840973/14956161682665.jpg" alt=""/></p>

<p>SHA1的结果是160 bit字节，通常用一个40位的16进制字符串表示。</p>

<p>比SHA1更安全的算法是SHA256和SHA512，不过越安全的算法越慢，而且摘要长度更长。</p>

<blockquote>
<p>有没有可能两个不同的数据通过某个摘要算法得到了相同的摘要？完全有可能，因为任何摘要算法都是把无限多的数据集合映射到一个有限的集合中。这种情况称为碰撞，比如Bob试图根据你的摘要反推出一篇文章&#39;how to learn hashlib in python - by Bob&#39;，并且这篇文章的摘要恰好和你的文章完全一致，这种情况也并非不可能出现，但是非常非常困难。</p>
</blockquote>

<h2 id="toc_2">摘要算法应用</h2>

<p>一个正确的应用是在存储的数据库表中,将用户登录用的密码进行转换,防止数据泄露导致黑客或者内部人员获取.</p>

<p>正确的保存口令的方式是不存储用户的明文口令，而是存储用户口令的摘要，比如MD5：</p>

<table>
<thead>
<tr>
<th>username</th>
<th>password</th>
</tr>
</thead>

<tbody>
<tr>
<td>michael</td>
<td>e10adc3949ba59abbe56e057f20f883e</td>
</tr>
<tr>
<td>bob</td>
<td>878ef96e86145580c38c87f0410ad153</td>
</tr>
<tr>
<td>alice</td>
<td>99b1c2188db85afee403b1536010c2c9</td>
</tr>
</tbody>
</table>

<p>当用户登录时，首先计算用户输入的明文口令的MD5，然后和数据库存储的MD5对比，如果一致，说明口令输入正确，如果不一致，口令肯定错误。</p>

<h3 id="toc_3">练习:根据用户输入的口令，计算出存储在数据库中的MD5口令：</h3>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib

def calc_md5(password):
    md5 = hashlib.md5()
    md5.update(password)
    return md5.hexdigest()
password_text = raw_input(&#39;输入密码:&#39;)
print calc_md5(password_text)
</code></pre>

<p><img src="media/14956154840973/14956172486330.jpg" alt=""/></p>

<p>存储MD5的好处是即使运维人员能访问数据库，也无法获知用户的明文口令。</p>

<h3 id="toc_4">练习：设计一个验证用户登录的函数，根据用户输入的口令是否正确，返回True或False：</h3>

<pre><code class="language-py">db = {
    &#39;michael&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;,
    &#39;bob&#39;: &#39;878ef96e86145580c38c87f0410ad153&#39;,
    &#39;alice&#39;: &#39;99b1c2188db85afee403b1536010c2c9&#39;
}

def login(user, password):
    pass
</code></pre>

<p>实现代码为:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib
db = {
    &#39;michael&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;,
    &#39;bob&#39;: &#39;878ef96e86145580c38c87f0410ad153&#39;,
    &#39;hao&#39;: &#39;e10adc3949ba59abbe56e057f20f883e&#39;
}

def calc_md5(password):
    md5 = hashlib.md5()
    md5.update(password)
    return md5.hexdigest()
def login(user,password):
    md5 = hashlib.md5()
    md5.update(password)
    if user in db:
        if db[user]==md5.hexdigest():
            print &#39;登陆成功&#39;
        else :
            print &#39;失败&#39;
    else:
        print&#39;该用户不存在&#39;
user_text = raw_input(&#39;请输入用户名: &#39;)    
password_text = raw_input(&#39;输入密码: &#39;)

print calc_md5(password_text)
login(user_text, password_text)
</code></pre>

<p><img src="media/14956154840973/14956740702158.jpg" alt=""/></p>

<p>对于用户来讲，当然不要使用过于简单的口令。但是，我们能否在程序设计上对简单口令加强保护呢？</p>

<p>由于常用口令的MD5值很容易被计算出来，所以，要确保存储的用户口令不是那些已经被计算出来的常用口令的MD5，这一方法通过对原始口令加一个复杂字符串来实现，俗称“加盐”：</p>

<pre><code class="language-py">def calc_md5(password):
    return get_md5(password + &#39;the-Salt&#39;)
</code></pre>

<p>经过Salt处理的MD5口令，只要Salt不被黑客知道，即使用户输入简单口令，也很难通过MD5反推明文口令。</p>

<p><mark>但是如果有两个用户都使用了相同的简单口令比如123456，在数据库中，将存储两条相同的MD5值，这说明这两个用户的口令是一样的。有没有办法让使用相同口令的用户存储不同的MD5呢？</mark></p>

<p><strong>如果假定用户无法修改登录名，就可以通过把登录名作为Salt的一部分来计算MD5，从而实现相同口令的用户也存储不同的MD5。</strong></p>

<h3 id="toc_5">练习：根据用户输入的登录名和口令模拟用户注册，计算更安全的MD5：</h3>

<pre><code class="language-py">db = {}

def register(username, password):
    db[username] = get_md5(password + username + &#39;the-Salt&#39;)
</code></pre>

<p>然后，根据修改后的MD5算法实现用户登录的验证：</p>

<pre><code class="language-py">def login(username, password):
    pass
</code></pre>

<p>实现代码:</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

import hashlib
db = {}
def register(username,password):
    if username in db:
        print &#39;用户已存在&#39;
    else:
        db[username] = get_md5(password + username + &#39;the-Slat&#39;) 
def get_md5(text):
    md5 = hashlib.md5()
    md5.update(text)
    return md5.hexdigest()
def login(user,password):
    md5 = hashlib.md5()
    md5.update(password + user + &#39;the-Slat&#39;)
    if user in db:
        if db[user]==md5.hexdigest():
            print &#39;登陆成功&#39;
        else :
            print &#39;失败&#39;
    else:
        print&#39;该用户不存在或者密码错误&#39;
register(&#39;hao&#39;, &#39;123456&#39;)
register(&#39;hao&#39;, &#39;12212&#39;)    #重复注册会显示失败
user_text = raw_input(&#39;请输入用户名: &#39;)    
password_text = raw_input(&#39;输入密码: &#39;)

login(user_text, password_text)
</code></pre>

<p><img src="media/14956154840973/14956751273389.jpg" alt=""/></p>

<h2 id="toc_6">小结</h2>

<p>摘要算法在很多地方都有广泛的应用。<strong>要注意摘要算法不是加密算法，不能用于加密（因为无法通过摘要反推明文）</strong>，只能用于防篡改，但是它的单向计算特性决定了可以在不存储明文口令的情况下验证用户口令。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在ios中判断一个viewcontroller是都已经正常present]]></title>
    <link href="https://lockxmonk.github.io/14956131169957.html"/>
    <updated>2017-05-24T16:05:16+08:00</updated>
    <id>https://lockxmonk.github.io/14956131169957.html</id>
    <content type="html"><![CDATA[
<p>之前present一个视图的时候,从来没有研究过如何判断一个视图是都已经正常弹出.比如下面这个方法是否正常执行:</p>

<pre><code class="language-oc">[documentInteractionController presentPreviewAnimated:YES]
</code></pre>

<p>然而今天有一个需求需要知道上述方法是否正常执行,并弹出视图.</p>

<p>之后通过阅读该方法的源码后发现:</p>

<pre><code class="language-oc">// Bypasses the menu and opens the full screen preview window for the item at URL.  Returns NO if the item could not be previewed.
// Note that you must implement the delegate method documentInteractionControllerViewControllerForPreview: to preview the document.
- (BOOL)presentPreviewAnimated:(BOOL)animated;
</code></pre>

<p>该方法返回一个BOOL类型的值:当没有正常执行该方法时,会返回NO.正常返回YES.</p>

<p>类似的还有如下方法:</p>

<pre><code class="language-oc">- (BOOL)isBeingPresented NS_AVAILABLE_IOS(5_0);
- (BOOL)isBeingDismissed NS_AVAILABLE_IOS(5_0);

- (BOOL)isMovingToParentViewController NS_AVAILABLE_IOS(5_0);
- (BOOL)isMovingFromParentViewController NS_AVAILABLE_IOS(5_0);
</code></pre>

<p>来判断viewController是消失还是出现在当前页面中.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Caffe目录结构]]></title>
    <link href="https://lockxmonk.github.io/14955929300450.html"/>
    <updated>2017-05-24T10:28:50+08:00</updated>
    <id>https://lockxmonk.github.io/14955929300450.html</id>
    <content type="html"><![CDATA[
<p>我们这里先了解一下caffe的目录结构:<br/>
<img src="media/14955929300450/14961936456285.jpg" alt=""/><br/>
<img src="media/14955929300450/14961936868967.jpg" alt=""/><br/>
<img src="media/14955929300450/14961937240927.jpg" alt=""/><br/>
其中主要关注的就是:include/,src/和tools/,三个目录,需要分析的代码都包含在这里面.</p>

<p>在阅读代码时,如何快速追踪某个关键词?传统的方法是打开某个文件,之后用查找命令来查找关键词.<br/>
这了我们介绍另一种方法,利用<code>grep</code>命令:</p>

<pre><code>➜  caffe git:(master) ✗ grep -n -H -R &quot;REGISTER_LAYER_CREATOR&quot; *
</code></pre>

<p><img src="media/14955929300450/14955931415464.jpg" alt=""/><br/>
我们可以看到,日志输出了很多文件.这种方法无需分别打开每个文件,也能直观的显示了所有包含这个宏的文件名和行号.</p>

<p>这里我们用<code>grep</code>命令来搜索一个宏调用:<code>REGISTER_LAYER_CREATOR</code>,<br/>
命令行参数解释为:</p>

<blockquote>
<p>-n  ---显示行号,便于定位<br/>
-H  ---显示文件名,便与定位<br/>
-R  ---递归查找每个子目录,适合工程较大,分多个目录存放的场景</p>
</blockquote>

<p>利用这种方法可以很容易的在caffe源码中定位很多内容.</p>

<h2 id="toc_0">卷基层</h2>

<p>这里我们假定卷积层有<code>L</code>个输出通道和<code>K</code>个输入通道,于是需要有<code>LK,L=50,K=20</code>个卷积核实现通道数目的转换.这里我们假定卷集核大小为<code>I*J = 5*5</code>,每个输出通道的特征图大小为<code>M*N = 8*8</code>,则该层每个样本做一次前向传播时卷积层计算量为:<br/>
<code>Calculations(MAC) = I*J*M*N*K*L = 5*5*8*8*50*20=1600000MAC</code><br/>
实际中使送入一批样本(batch),所以我们这里还需要计算量乘上批量尺寸.</p>

<p>我们这里卷积层的学习参数量为:<br/>
<code>Params = I*J*K*L = 25000</code><br/>
所以计算量-参数量之比为<code>CPR=Calculations/Params = M*N = 64</code><br/>
<font color=red><strong>所以我们得出结论是:卷基层的输出特征图尺寸越大,CPR值越大,参数重复利用率越高.,若一次性输入一批数据(B个样本),则CPR值可再提高B倍.</strong></font></p>

<h2 id="toc_1">全连接层</h2>

<p>早在卷积神经网络出现之前,最早的深度学习网络计算类型都是全连接形式的.如下所示.<br/>
<img src="media/14955929300450/14961962825939.jpg" alt=""/><br/>
每个节点与相邻层的所有节点都有连接关系,这是全连接层名称的由来.</p>

<p>全连接层的主要计算类型为矩阵-向量乘(GEMV).假设输入节点组成的向量为x,维度为D,输出节点组成的向量为y,维度为V,则全连接层计算可以表示为:<br/>
\[y=Wx\]<br/>
其中,W为<code>V*D</code>维权值矩阵.</p>

<p>我们分析全连接层的参数:<br/>
<img src="media/14955929300450/14961986814721.jpg" alt=""/></p>

<p><img src="media/14955929300450/14961986616938.jpg" alt=""/></p>

<p>得出输出<code>V=500</code>,输入<code>D=50*4*4 = 800</code>(其中50是输入数量,4*4位图的尺寸)</p>

<p>则全连接层单样本前向传播计算量为:<br/>
\[<br/>
CalculationsMAC = V*D<br/>
                = 800 * 500<br/>
                = 400000<br/>
\]<br/>
参数统计量为:<br/>
\[<br/>
Params = V*D<br/>
       = 800 * 500<br/>
       = 400000<br/>
\]<br/>
所以CPR值为1</p>

<p>所以得出结论,全连接层的CPR值始终为1,与输入,输出维度无关.所以单样本前向向传播计算时,权值重复利用率很低.<br/>
    我们将一批(B个)样本逐列拼接成矩阵X,一次性通过全连接层,得到一批输出向量构成的矩阵Y,称作批处理(矩阵-矩阵乘计算GEMM):<br/>
    \[<br/>
    Y=WX<br/>
    \]<br/>
这样全连接层前向计算量提高了B倍,而参数量不变,因此CPR提高了B倍.</p>

<p>与卷积层相比，全连接层参数量是其16倍，而计算量只有其25%.如果输出特征图尺寸相同<code>（M*V = V)</code>，卷积层的CPR值为全连接层的<code>M*N</code>倍。也就是说，卷积层在输出特征图维度实现了<mark>权值共享</mark>。这是降低参数量的重要举措。与此同吋，卷枳层<mark>局部连接</mark>特性 (相比全连接）也大幅减少了参数量,这使得CNN网络中前几层卷积层参数量占比小，计算量占比大；而后几层全连接层参数量占比大，计算量占比小。大多数CNN模型都符合这个特点。<font color=red><strong>因此我们在进行计算加速优化时,重点放在卷积层；而在进行参数优化、权值剪裁时，重点放在全连接层，</strong></font></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[运行caffe框架中的cifar10样例]]></title>
    <link href="https://lockxmonk.github.io/14955245049549.html"/>
    <updated>2017-05-23T15:28:24+08:00</updated>
    <id>https://lockxmonk.github.io/14955245049549.html</id>
    <content type="html"><![CDATA[
<p>1.先运行caffe目录下的data/get_cifar10.sh脚本.</p>

<pre><code>#!/usr/bin/env sh
# This scripts downloads the CIFAR10 (binary version) data and unzips it.

DIR=&quot;$( cd &quot;$(dirname &quot;$0&quot;)&quot; ; pwd -P )&quot;
cd &quot;$DIR&quot;

echo &quot;Downloading...&quot;

wget --no-check-certificate http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz

echo &quot;Unzipping...&quot;

tar -xf cifar-10-binary.tar.gz &amp;&amp; rm -f cifar-10-binary.tar.gz
mv cifar-10-batches-bin/* . &amp;&amp; rm -rf cifar-10-batches-bin

# Creation is split out because leveldb sometimes causes segfault
# and needs to be re-created.

echo &quot;Done.&quot;
</code></pre>

<p>获取数据集.</p>

<p>之后运行example下的cifar10/create_cifar10.sh<br/>
但是会遇到以下报错:<br/>
<img src="media/14955245049549/14955247436003.jpg" alt=""/></p>

<p>这里要运行下面这个命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/examples/cifar10/convert_cifar_data.bin
</code></pre>

<p><img src="media/14955245049549/14955249112611.jpg" alt=""/></p>

<p>再次运行./examples/cifar10/create_cifar10.sh后又会出现一个错误:<br/>
<img src="media/14955245049549/14955249879358.jpg" alt=""/><br/>
这里要再次执行以下命令:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/tools/compute_image_mean
</code></pre>

<p><img src="media/14955245049549/14955250166086.jpg" alt=""/></p>

<p>然后再次执行:</p>

<pre><code>./examples/cifar10/create_cifar10.sh
</code></pre>

<p>结果成功了!如下图所示:<br/>
<img src="media/14955245049549/14955251049094.jpg" alt=""/></p>

<h1 id="toc_0">Training and Testing the &quot;Quick&quot; Model</h1>

<p>因为例子中已经给出定义好的protobuf和solver protobuf文件,所以我们直接运行<code>train_quick.sh</code></p>

<p>该文件内容为:</p>

<pre><code class="language-sh">#!/usr/bin/env sh
set -e

TOOLS=./build/tools

$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver.prototxt $@

# reduce learning rate by factor of 10 after 8 epochs
$TOOLS/caffe train \
  --solver=examples/cifar10/cifar10_quick_solver_lr1.prototxt \
  --snapshot=examples/cifar10/cifar10_quick_iter_4000.solverstate $@
</code></pre>

<p>执行如下命令:</p>

<pre><code>➜ caffe git:(master) ✗ ./examples/cifar10/train_quick.sh
</code></pre>

<p>然后输出为:</p>

<pre><code>I0523 15:43:36.608793 2712679360 caffe.cpp:211] Use CPU.
I0523 15:43:36.609737 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 4000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 4000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0523 15:43:36.610075 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.610931 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 15:43:36.610961 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 15:43:36.610966 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.611205 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.611467 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 15:43:36.611524 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.611531 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.611549 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.611565 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.611686 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.617992 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.618022 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.618028 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.618032 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.618041 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.618052 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.618057 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.618063 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.618175 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.618180 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.618185 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 15:43:36.618192 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.618199 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.618202 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.618206 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.618216 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.618219 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618224 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 15:43:36.618228 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.618234 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.618238 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.618242 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.618247 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.618250 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618255 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 15:43:36.618263 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.618273 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.618276 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.618281 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.618585 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.618592 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618597 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 15:43:36.618602 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.618607 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.618609 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.618614 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.618619 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.618623 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.618628 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 15:43:36.618630 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.618634 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.618638 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.618643 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.618647 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.618654 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.618662 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 15:43:36.618669 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.618680 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.618685 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.618695 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.619361 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.619372 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619379 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 15:43:36.619385 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.619390 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.619393 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.619398 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.619403 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.619447 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.619459 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 15:43:36.619467 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.619477 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.619484 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.619493 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.619505 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.619513 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.619523 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 15:43:36.619529 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.619539 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.619546 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.619555 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.620586 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.620602 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.620607 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 15:43:36.620613 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.620620 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.620625 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.620630 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.620649 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.620656 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.620662 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 15:43:36.620673 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620682 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.620689 2712679360 net.cpp:406] loss &lt;- ip2
I0523 15:43:36.620697 2712679360 net.cpp:406] loss &lt;- label
I0523 15:43:36.620703 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.620730 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.620749 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.620756 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.620764 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.620787 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 15:43:36.620795 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.620800 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.620807 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.620813 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.620820 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.620832 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.620851 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.620859 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.620867 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.620875 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.620882 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.620889 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.620896 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.620904 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.620916 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.621170 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 15:43:36.621199 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 15:43:36.621210 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 15:43:36.621821 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 15:43:36.621913 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 15:43:36.621933 2712679360 net.cpp:84] Creating Layer cifar
I0523 15:43:36.621943 2712679360 net.cpp:380] cifar -&gt; data
I0523 15:43:36.621950 2712679360 net.cpp:380] cifar -&gt; label
I0523 15:43:36.621958 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 15:43:36.622017 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 15:43:36.624790 2712679360 net.cpp:122] Setting up cifar
I0523 15:43:36.624822 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 15:43:36.624830 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624835 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 15:43:36.624840 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 15:43:36.624851 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 15:43:36.624856 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 15:43:36.624862 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 15:43:36.624869 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 15:43:36.624876 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 15:43:36.624878 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624882 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 15:43:36.624886 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 15:43:36.624917 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 15:43:36.624927 2712679360 net.cpp:84] Creating Layer conv1
I0523 15:43:36.624930 2712679360 net.cpp:406] conv1 &lt;- data
I0523 15:43:36.624935 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 15:43:36.624987 2712679360 net.cpp:122] Setting up conv1
I0523 15:43:36.624991 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 15:43:36.624996 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 15:43:36.625002 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 15:43:36.625008 2712679360 net.cpp:84] Creating Layer pool1
I0523 15:43:36.625011 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 15:43:36.625015 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 15:43:36.625022 2712679360 net.cpp:122] Setting up pool1
I0523 15:43:36.625026 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625031 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 15:43:36.625036 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 15:43:36.625041 2712679360 net.cpp:84] Creating Layer relu1
I0523 15:43:36.625043 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 15:43:36.625048 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 15:43:36.625053 2712679360 net.cpp:122] Setting up relu1
I0523 15:43:36.625056 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625061 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 15:43:36.625064 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 15:43:36.625071 2712679360 net.cpp:84] Creating Layer conv2
I0523 15:43:36.625074 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 15:43:36.625084 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 15:43:36.625396 2712679360 net.cpp:122] Setting up conv2
I0523 15:43:36.625402 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625407 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 15:43:36.625412 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 15:43:36.625417 2712679360 net.cpp:84] Creating Layer relu2
I0523 15:43:36.625422 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 15:43:36.625425 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 15:43:36.625429 2712679360 net.cpp:122] Setting up relu2
I0523 15:43:36.625433 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 15:43:36.625437 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 15:43:36.625440 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 15:43:36.625445 2712679360 net.cpp:84] Creating Layer pool2
I0523 15:43:36.625448 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 15:43:36.625452 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 15:43:36.625458 2712679360 net.cpp:122] Setting up pool2
I0523 15:43:36.625460 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 15:43:36.625464 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 15:43:36.625468 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 15:43:36.625474 2712679360 net.cpp:84] Creating Layer conv3
I0523 15:43:36.625479 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 15:43:36.625483 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 15:43:36.626077 2712679360 net.cpp:122] Setting up conv3
I0523 15:43:36.626083 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626088 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 15:43:36.626093 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 15:43:36.626098 2712679360 net.cpp:84] Creating Layer relu3
I0523 15:43:36.626101 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 15:43:36.626106 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 15:43:36.626111 2712679360 net.cpp:122] Setting up relu3
I0523 15:43:36.626113 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 15:43:36.626117 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 15:43:36.626121 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 15:43:36.626126 2712679360 net.cpp:84] Creating Layer pool3
I0523 15:43:36.626129 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 15:43:36.626145 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 15:43:36.626152 2712679360 net.cpp:122] Setting up pool3
I0523 15:43:36.626154 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 15:43:36.626159 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 15:43:36.626163 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 15:43:36.626168 2712679360 net.cpp:84] Creating Layer ip1
I0523 15:43:36.626173 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 15:43:36.626176 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 15:43:36.626969 2712679360 net.cpp:122] Setting up ip1
I0523 15:43:36.626981 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 15:43:36.626986 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 15:43:36.626992 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 15:43:36.626999 2712679360 net.cpp:84] Creating Layer ip2
I0523 15:43:36.627003 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 15:43:36.627008 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 15:43:36.627024 2712679360 net.cpp:122] Setting up ip2
I0523 15:43:36.627028 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627032 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 15:43:36.627039 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 15:43:36.627046 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 15:43:36.627053 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 15:43:36.627059 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 15:43:36.627068 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 15:43:36.627076 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 15:43:36.627081 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627085 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 15:43:36.627089 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 15:43:36.627094 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 15:43:36.627099 2712679360 net.cpp:84] Creating Layer accuracy
I0523 15:43:36.627102 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 15:43:36.627106 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 15:43:36.627110 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 15:43:36.627116 2712679360 net.cpp:122] Setting up accuracy
I0523 15:43:36.627120 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627123 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 15:43:36.627126 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627133 2712679360 net.cpp:84] Creating Layer loss
I0523 15:43:36.627169 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 15:43:36.627178 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 15:43:36.627183 2712679360 net.cpp:380] loss -&gt; loss
I0523 15:43:36.627189 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 15:43:36.627198 2712679360 net.cpp:122] Setting up loss
I0523 15:43:36.627202 2712679360 net.cpp:129] Top shape: (1)
I0523 15:43:36.627207 2712679360 net.cpp:132]     with loss weight 1
I0523 15:43:36.627213 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 15:43:36.627215 2712679360 net.cpp:198] loss needs backward computation.
I0523 15:43:36.627219 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 15:43:36.627223 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 15:43:36.627228 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 15:43:36.627230 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 15:43:36.627234 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 15:43:36.627321 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 15:43:36.627334 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 15:43:36.627341 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 15:43:36.627348 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 15:43:36.627354 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 15:43:36.627387 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 15:43:36.627394 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 15:43:36.627400 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 15:43:36.627409 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 15:43:36.627418 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 15:43:36.627432 2712679360 net.cpp:242] This network produces output accuracy
I0523 15:43:36.627454 2712679360 net.cpp:242] This network produces output loss
I0523 15:43:36.627470 2712679360 net.cpp:255] Network initialization done.
I0523 15:43:36.627553 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 15:43:36.627593 2712679360 caffe.cpp:248] Starting Optimization
I0523 15:43:36.627602 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 15:43:36.627610 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 15:43:36.627933 2712679360 solver.cpp:330] Iteration 0, Testing net (#0)
I0523 15:43:46.157997 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:43:46.542196 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.0865
I0523 15:43:46.542232 2712679360 solver.cpp:397]     Test net output #1: loss = 2.3025 (* 1 = 2.3025 loss)
I0523 15:43:46.784966 2712679360 solver.cpp:218] Iteration 0 (0 iter/s, 10.157s/100 iters), loss = 2.30202
I0523 15:43:46.785002 2712679360 solver.cpp:237]     Train net output #0: loss = 2.30202 (* 1 = 2.30202 loss)
I0523 15:43:46.785009 2712679360 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0523 15:44:08.112608 2712679360 solver.cpp:218] Iteration 100 (4.68889 iter/s, 21.327s/100 iters), loss = 1.67773
I0523 15:44:08.112664 2712679360 solver.cpp:237]     Train net output #0: loss = 1.67773 (* 1 = 1.67773 loss)
I0523 15:44:08.112673 2712679360 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0523 15:44:29.336644 2712679360 solver.cpp:218] Iteration 200 (4.71187 iter/s, 21.223s/100 iters), loss = 1.59886
I0523 15:44:29.336683 2712679360 solver.cpp:237]     Train net output #0: loss = 1.59886 (* 1 = 1.59886 loss)
I0523 15:44:29.336693 2712679360 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0523 15:44:50.573981 2712679360 solver.cpp:218] Iteration 300 (4.70876 iter/s, 21.237s/100 iters), loss = 1.31839
I0523 15:44:50.574038 2712679360 solver.cpp:237]     Train net output #0: loss = 1.31839 (* 1 = 1.31839 loss)
I0523 15:44:50.574044 2712679360 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0523 15:45:12.080576 2712679360 solver.cpp:218] Iteration 400 (4.64987 iter/s, 21.506s/100 iters), loss = 1.24876
I0523 15:45:12.080610 2712679360 solver.cpp:237]     Train net output #0: loss = 1.24876 (* 1 = 1.24876 loss)
I0523 15:45:12.080618 2712679360 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0523 15:45:32.450579 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:33.342396 2712679360 solver.cpp:330] Iteration 500, Testing net (#0)
I0523 15:45:42.732501 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 15:45:43.134589 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.5366
I0523 15:45:43.134620 2712679360 solver.cpp:397]     Test net output #1: loss = 1.31952 (* 1 = 1.31952 loss)
I0523 15:45:43.360550 2712679360 solver.cpp:218] Iteration 500 (3.19703 iter/s, 31.279s/100 iters), loss = 1.22391
I0523 15:45:43.360582 2712679360 solver.cpp:237]     Train net output #0: loss = 1.22391 (* 1 = 1.22391 loss)
I0523 15:45:43.360589 2712679360 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0523 15:46:06.734716 2712679360 solver.cpp:218] Iteration 600 (4.27826 iter/s, 23.374s/100 iters), loss = 1.23177
I0523 15:46:06.734771 2712679360 solver.cpp:237]     Train net output #0: loss = 1.23177 (* 1 = 1.23177 loss)
I0523 15:46:06.734779 2712679360 sgd_solver.cpp:105] Iteration 600, lr = 0.001
.......数据形式基本相同 故省略...
I0523 16:00:46.286926 2712679360 solver.cpp:218] Iteration 3900 (4.08731 iter/s, 24.466s/100 iters), loss = 0.557826
I0523 16:00:46.286960 2712679360 solver.cpp:237]     Train net output #0: loss = 0.557826 (* 1 = 0.557826 loss)
I0523 16:00:46.286967 2712679360 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0523 16:01:09.469552 978944 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:10.472170 2712679360 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/cifar10_quick_iter_4000.caffemodel
I0523 16:01:10.475755 2712679360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:10.590515 2712679360 solver.cpp:310] Iteration 4000, loss = 0.641508
I0523 16:01:10.590548 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:21.619536 1515520 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:22.054498 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:22.054538 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:22.054548 2712679360 solver.cpp:315] Optimization Done.
I0523 16:01:22.054555 2712679360 caffe.cpp:259] Optimization Done.
I0523 16:01:22.119184 2712679360 caffe.cpp:211] Use CPU.
I0523 16:01:22.120214 2712679360 solver.cpp:44] Initializing solver from parameters:
test_iter: 100
test_interval: 500
base_lr: 0.0001
display: 100
max_iter: 5000
lr_policy: &quot;fixed&quot;
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: &quot;examples/cifar10/cifar10_quick&quot;
solver_mode: CPU
net: &quot;examples/cifar10/cifar10_quick_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
snapshot_format: HDF5
I0523 16:01:22.120556 2712679360 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.120817 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0523 16:01:22.120833 2712679360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 16:01:22.120841 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.121104 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.121320 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0523 16:01:22.121383 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.121393 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.121413 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.121431 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.121585 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.128842 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.128867 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.128875 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.128880 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.128890 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.128902 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.128907 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.128914 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.129009 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.129017 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.129022 2712679360 net.cpp:137] Memory required for data: 14336400
I0523 16:01:22.129030 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.129039 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.129042 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.129047 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.129057 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.129062 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129067 2712679360 net.cpp:137] Memory required for data: 17613200
I0523 16:01:22.129071 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.129078 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.129083 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.129087 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.129093 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.129097 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129102 2712679360 net.cpp:137] Memory required for data: 20890000
I0523 16:01:22.129106 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.129117 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.129120 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.129125 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.129482 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.129487 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129493 2712679360 net.cpp:137] Memory required for data: 24166800
I0523 16:01:22.129500 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.129505 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.129509 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.129514 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.129520 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.129524 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.129528 2712679360 net.cpp:137] Memory required for data: 27443600
I0523 16:01:22.129534 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.129537 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.129541 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.129547 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.129554 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.129557 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.129562 2712679360 net.cpp:137] Memory required for data: 28262800
I0523 16:01:22.129566 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.129573 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.129577 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.129585 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.130280 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.130286 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130292 2712679360 net.cpp:137] Memory required for data: 29901200
I0523 16:01:22.130298 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.130304 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.130308 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.130313 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.130318 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.130353 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.130360 2712679360 net.cpp:137] Memory required for data: 31539600
I0523 16:01:22.130364 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.130370 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.130374 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.130379 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.130385 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.130389 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.130396 2712679360 net.cpp:137] Memory required for data: 31949200
I0523 16:01:22.130400 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.130409 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.130414 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.130419 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.131337 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.131347 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.131352 2712679360 net.cpp:137] Memory required for data: 31974800
I0523 16:01:22.131358 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.131364 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.131369 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.131374 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.131392 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.131397 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.131400 2712679360 net.cpp:137] Memory required for data: 31978800
I0523 16:01:22.131407 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131413 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.131417 2712679360 net.cpp:406] loss &lt;- ip2
I0523 16:01:22.131422 2712679360 net.cpp:406] loss &lt;- label
I0523 16:01:22.131427 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.131435 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.131448 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.131453 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.131458 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.131471 2712679360 net.cpp:137] Memory required for data: 31978804
I0523 16:01:22.131476 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.131495 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.131505 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.131510 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.131515 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.131518 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.131522 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.131527 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.131531 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.131536 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.131541 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.131544 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.131548 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.131552 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.131561 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.131786 2712679360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0523 16:01:22.131814 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:01:22.131826 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:01:22.132225 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:01:22.132313 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:01:22.132342 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:01:22.132356 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:01:22.132364 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:01:22.132372 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:01:22.132438 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:01:22.134943 2712679360 net.cpp:122] Setting up cifar
I0523 16:01:22.134956 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:01:22.134963 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.134968 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:01:22.134974 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:01:22.134984 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:01:22.135015 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:01:22.135064 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:01:22.135078 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:01:22.135116 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:01:22.135167 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135203 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:01:22.135241 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:01:22.135313 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:01:22.135330 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:01:22.135335 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:01:22.135342 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:01:22.135398 2712679360 net.cpp:122] Setting up conv1
I0523 16:01:22.135404 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:01:22.135411 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:01:22.135418 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:01:22.135463 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:01:22.135473 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:01:22.135514 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:01:22.135565 2712679360 net.cpp:122] Setting up pool1
I0523 16:01:22.135574 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135581 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:01:22.135586 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:01:22.135593 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:01:22.135598 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:01:22.135603 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:01:22.135609 2712679360 net.cpp:122] Setting up relu1
I0523 16:01:22.135613 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.135666 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:01:22.135673 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:01:22.135681 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:01:22.135686 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:01:22.135700 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:01:22.136068 2712679360 net.cpp:122] Setting up conv2
I0523 16:01:22.136076 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136081 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:01:22.136088 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:01:22.136095 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:01:22.136098 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:01:22.136103 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:01:22.136108 2712679360 net.cpp:122] Setting up relu2
I0523 16:01:22.136112 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:01:22.136117 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:01:22.136121 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:01:22.136127 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:01:22.136132 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:01:22.136135 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:01:22.136142 2712679360 net.cpp:122] Setting up pool2
I0523 16:01:22.136147 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:01:22.136152 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:01:22.136157 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:01:22.136163 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:01:22.136168 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:01:22.136173 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:01:22.136878 2712679360 net.cpp:122] Setting up conv3
I0523 16:01:22.136888 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136893 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:01:22.136899 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:01:22.136904 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:01:22.136909 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:01:22.136914 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:01:22.136919 2712679360 net.cpp:122] Setting up relu3
I0523 16:01:22.136930 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:01:22.136961 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:01:22.136968 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:01:22.136976 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:01:22.137001 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:01:22.137008 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:01:22.137017 2712679360 net.cpp:122] Setting up pool3
I0523 16:01:22.137022 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:01:22.137027 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:01:22.137032 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:01:22.137039 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:01:22.137044 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:01:22.137050 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:01:22.137981 2712679360 net.cpp:122] Setting up ip1
I0523 16:01:22.137995 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:01:22.138002 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:01:22.138008 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:01:22.138016 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:01:22.138021 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:01:22.138027 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:01:22.138046 2712679360 net.cpp:122] Setting up ip2
I0523 16:01:22.138051 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138056 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:01:22.138062 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:01:22.138085 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:01:22.138103 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:01:22.138115 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:01:22.138129 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:01:22.138142 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:01:22.138150 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138160 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:01:22.138170 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:01:22.138177 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:01:22.138187 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:01:22.138219 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:01:22.138231 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:01:22.138242 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:01:22.138257 2712679360 net.cpp:122] Setting up accuracy
I0523 16:01:22.138264 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138274 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:01:22.138279 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138286 2712679360 net.cpp:84] Creating Layer loss
I0523 16:01:22.138290 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:01:22.138327 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:01:22.138334 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:01:22.138342 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:01:22.138352 2712679360 net.cpp:122] Setting up loss
I0523 16:01:22.138357 2712679360 net.cpp:129] Top shape: (1)
I0523 16:01:22.138362 2712679360 net.cpp:132]     with loss weight 1
I0523 16:01:22.138368 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:01:22.138372 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:01:22.138377 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:01:22.138382 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:01:22.138386 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:01:22.138391 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:01:22.138396 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:01:22.138401 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:01:22.138404 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:01:22.138408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:01:22.138412 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:01:22.138417 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:01:22.138444 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:01:22.138449 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:01:22.138454 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:01:22.138463 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:01:22.138468 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:01:22.138470 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:01:22.138476 2712679360 net.cpp:242] This network produces output loss
I0523 16:01:22.138485 2712679360 net.cpp:255] Network initialization done.
I0523 16:01:22.138537 2712679360 solver.cpp:56] Solver scaffolding done.
I0523 16:01:22.138566 2712679360 caffe.cpp:242] Resuming from examples/cifar10/cifar10_quick_iter_4000.solverstate
I0523 16:01:22.139786 2712679360 sgd_solver.cpp:318] SGDSolver: restoring history
I0523 16:01:22.140019 2712679360 caffe.cpp:248] Starting Optimization
I0523 16:01:22.140027 2712679360 solver.cpp:272] Solving CIFAR10_quick
I0523 16:01:22.140031 2712679360 solver.cpp:273] Learning Rate Policy: fixed
I0523 16:01:22.140113 2712679360 solver.cpp:330] Iteration 4000, Testing net (#0)
I0523 16:01:32.383680 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:01:32.807214 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.7119
I0523 16:01:32.807250 2712679360 solver.cpp:397]     Test net output #1: loss = 0.848064 (* 1 = 0.848064 loss)
I0523 16:01:33.065510 2712679360 solver.cpp:218] Iteration 4000 (366.133 iter/s, 10.925s/100 iters), loss = 0.641508
I0523 16:01:33.065546 2712679360 solver.cpp:237]     Train net output #0: loss = 0.641508 (* 1 = 0.641508 loss)
I0523 16:01:33.065553 2712679360 sgd_solver.cpp:105] Iteration 4000, lr = 0.0001
I0523 16:01:56.950950 2712679360 solver.cpp:218] Iteration 4100 (4.18673 iter/s, 23.885s/100 iters), loss = 0.603556
I0523 16:01:56.951002 2712679360 solver.cpp:237]     Train net output #0: loss = 0.603556 (* 1 = 0.603556 loss)
I0523 16:01:56.951010 2712679360 sgd_solver.cpp:105] Iteration 4100, lr = 0.0001
I0523 16:02:21.127391 2712679360 solver.cpp:218] Iteration 4200 (4.13633 iter/s, 24.176s/100 iters), loss = 0.491505
I0523 16:02:21.127429 2712679360 solver.cpp:237]     Train net output #0: loss = 0.491505 (* 1 = 0.491505 loss)
I0523 16:02:21.127437 2712679360 sgd_solver.cpp:105] Iteration 4200, lr = 0.0001
I0523 16:02:46.283135 2712679360 solver.cpp:218] Iteration 4300 (3.97535 iter/s, 25.155s/100 iters), loss = 0.495313
I0523 16:02:46.283190 2712679360 solver.cpp:237]     Train net output #0: loss = 0.495313 (* 1 = 0.495313 loss)
I0523 16:02:46.283198 2712679360 sgd_solver.cpp:105] Iteration 4300, lr = 0.0001
I0523 16:03:10.841265 2712679360 solver.cpp:218] Iteration 4400 (4.07199 iter/s, 24.558s/100 iters), loss = 0.438567
I0523 16:03:10.841303 2712679360 solver.cpp:237]     Train net output #0: loss = 0.438567 (* 1 = 0.438567 loss)
I0523 16:03:10.841310 2712679360 sgd_solver.cpp:105] Iteration 4400, lr = 0.0001
I0523 16:03:33.942627 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:34.958622 2712679360 solver.cpp:330] Iteration 4500, Testing net (#0)
I0523 16:03:45.910739 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:03:46.349741 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.752
I0523 16:03:46.349779 2712679360 solver.cpp:397]     Test net output #1: loss = 0.748076 (* 1 = 0.748076 loss)
I0523 16:03:46.589071 2712679360 solver.cpp:218] Iteration 4500 (2.79744 iter/s, 35.747s/100 iters), loss = 0.503921
I0523 16:03:46.589107 2712679360 solver.cpp:237]     Train net output #0: loss = 0.503921 (* 1 = 0.503921 loss)
I0523 16:03:46.589113 2712679360 sgd_solver.cpp:105] Iteration 4500, lr = 0.0001
I0523 16:04:10.851019 2712679360 solver.cpp:218] Iteration 4600 (4.12184 iter/s, 24.261s/100 iters), loss = 0.562534
I0523 16:04:10.851088 2712679360 solver.cpp:237]     Train net output #0: loss = 0.562534 (* 1 = 0.562534 loss)
I0523 16:04:10.851095 2712679360 sgd_solver.cpp:105] Iteration 4600, lr = 0.0001
I0523 16:04:35.547813 2712679360 solver.cpp:218] Iteration 4700 (4.04924 iter/s, 24.696s/100 iters), loss = 0.464102
I0523 16:04:35.547852 2712679360 solver.cpp:237]     Train net output #0: loss = 0.464102 (* 1 = 0.464102 loss)
I0523 16:04:35.547860 2712679360 sgd_solver.cpp:105] Iteration 4700, lr = 0.0001
I0523 16:05:00.517423 2712679360 solver.cpp:218] Iteration 4800 (4.00497 iter/s, 24.969s/100 iters), loss = 0.474584
I0523 16:05:00.517478 2712679360 solver.cpp:237]     Train net output #0: loss = 0.474584 (* 1 = 0.474584 loss)
I0523 16:05:00.517487 2712679360 sgd_solver.cpp:105] Iteration 4800, lr = 0.0001
I0523 16:05:24.429520 2712679360 solver.cpp:218] Iteration 4900 (4.182 iter/s, 23.912s/100 iters), loss = 0.417258
I0523 16:05:24.429554 2712679360 solver.cpp:237]     Train net output #0: loss = 0.417258 (* 1 = 0.417258 loss)
I0523 16:05:24.429563 2712679360 sgd_solver.cpp:105] Iteration 4900, lr = 0.0001
I0523 16:05:47.148733 214478848 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:48.086921 2712679360 solver.cpp:457] Snapshotting to HDF5 file examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5
I0523 16:05:48.101351 2712679360 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_quick_iter_5000.solverstate.h5
I0523 16:05:48.215885 2712679360 solver.cpp:310] Iteration 5000, loss = 0.487594
I0523 16:05:48.215921 2712679360 solver.cpp:330] Iteration 5000, Testing net (#0)
I0523 16:05:58.710295 215015424 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:05:59.149840 2712679360 solver.cpp:397]     Test net output #0: accuracy = 0.754
I0523 16:05:59.149875 2712679360 solver.cpp:397]     Test net output #1: loss = 0.742307 (* 1 = 0.742307 loss)
I0523 16:05:59.149883 2712679360 solver.cpp:315] Optimization Done.
I0523 16:05:59.149888 2712679360 caffe.cpp:259] Optimization Done.
</code></pre>

<p>训练完毕.并且在最后已经创建好了测试网络.</p>

<p>下面我们用训练好的cifar10模型来对数据进行预测:</p>

<p>运行如下命令:</p>

<pre><code>➜  caffe git:(master) ✗ ./build/tools/caffe.bin test \
-model examples/cifar10/cifar10_quick_train_test.prototxt \
-weights examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5 \
-iterations 100
</code></pre>

<p>对测试数据集进行预测:</p>

<pre><code>I0523 16:25:41.234220 2712679360 caffe.cpp:284] Use CPU.
I0523 16:25:41.238044 2712679360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0523 16:25:41.238080 2712679360 net.cpp:51] Initializing net from parameters:
name: &quot;CIFAR10_quick&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;cifar&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    mean_file: &quot;examples/cifar10/mean.binaryproto&quot;
  }
  data_param {
    source: &quot;examples/cifar10/cifar10_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.0001
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;pool1&quot;
  top: &quot;pool1&quot;
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;pool3&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv3&quot;
  top: &quot;pool3&quot;
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool3&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.1
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0523 16:25:41.238523 2712679360 layer_factory.hpp:77] Creating layer cifar
I0523 16:25:41.238731 2712679360 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0523 16:25:41.238788 2712679360 net.cpp:84] Creating Layer cifar
I0523 16:25:41.238796 2712679360 net.cpp:380] cifar -&gt; data
I0523 16:25:41.238816 2712679360 net.cpp:380] cifar -&gt; label
I0523 16:25:41.238834 2712679360 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0523 16:25:41.238957 2712679360 data_layer.cpp:45] output data size: 100,3,32,32
I0523 16:25:41.246219 2712679360 net.cpp:122] Setting up cifar
I0523 16:25:41.246245 2712679360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0523 16:25:41.246253 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246258 2712679360 net.cpp:137] Memory required for data: 1229200
I0523 16:25:41.246266 2712679360 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0523 16:25:41.246278 2712679360 net.cpp:84] Creating Layer label_cifar_1_split
I0523 16:25:41.246282 2712679360 net.cpp:406] label_cifar_1_split &lt;- label
I0523 16:25:41.246343 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_0
I0523 16:25:41.246367 2712679360 net.cpp:380] label_cifar_1_split -&gt; label_cifar_1_split_1
I0523 16:25:41.246381 2712679360 net.cpp:122] Setting up label_cifar_1_split
I0523 16:25:41.246390 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246400 2712679360 net.cpp:129] Top shape: 100 (100)
I0523 16:25:41.246409 2712679360 net.cpp:137] Memory required for data: 1230000
I0523 16:25:41.246417 2712679360 layer_factory.hpp:77] Creating layer conv1
I0523 16:25:41.246438 2712679360 net.cpp:84] Creating Layer conv1
I0523 16:25:41.246448 2712679360 net.cpp:406] conv1 &lt;- data
I0523 16:25:41.246457 2712679360 net.cpp:380] conv1 -&gt; conv1
I0523 16:25:41.246606 2712679360 net.cpp:122] Setting up conv1
I0523 16:25:41.246637 2712679360 net.cpp:129] Top shape: 100 32 32 32 (3276800)
I0523 16:25:41.246680 2712679360 net.cpp:137] Memory required for data: 14337200
I0523 16:25:41.246693 2712679360 layer_factory.hpp:77] Creating layer pool1
I0523 16:25:41.246708 2712679360 net.cpp:84] Creating Layer pool1
I0523 16:25:41.246721 2712679360 net.cpp:406] pool1 &lt;- conv1
I0523 16:25:41.246731 2712679360 net.cpp:380] pool1 -&gt; pool1
I0523 16:25:41.246752 2712679360 net.cpp:122] Setting up pool1
I0523 16:25:41.246781 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246788 2712679360 net.cpp:137] Memory required for data: 17614000
I0523 16:25:41.246793 2712679360 layer_factory.hpp:77] Creating layer relu1
I0523 16:25:41.246804 2712679360 net.cpp:84] Creating Layer relu1
I0523 16:25:41.246809 2712679360 net.cpp:406] relu1 &lt;- pool1
I0523 16:25:41.246814 2712679360 net.cpp:367] relu1 -&gt; pool1 (in-place)
I0523 16:25:41.246821 2712679360 net.cpp:122] Setting up relu1
I0523 16:25:41.246825 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.246830 2712679360 net.cpp:137] Memory required for data: 20890800
I0523 16:25:41.246834 2712679360 layer_factory.hpp:77] Creating layer conv2
I0523 16:25:41.246841 2712679360 net.cpp:84] Creating Layer conv2
I0523 16:25:41.246846 2712679360 net.cpp:406] conv2 &lt;- pool1
I0523 16:25:41.246851 2712679360 net.cpp:380] conv2 -&gt; conv2
I0523 16:25:41.247228 2712679360 net.cpp:122] Setting up conv2
I0523 16:25:41.247236 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247242 2712679360 net.cpp:137] Memory required for data: 24167600
I0523 16:25:41.247249 2712679360 layer_factory.hpp:77] Creating layer relu2
I0523 16:25:41.247259 2712679360 net.cpp:84] Creating Layer relu2
I0523 16:25:41.247264 2712679360 net.cpp:406] relu2 &lt;- conv2
I0523 16:25:41.247269 2712679360 net.cpp:367] relu2 -&gt; conv2 (in-place)
I0523 16:25:41.247274 2712679360 net.cpp:122] Setting up relu2
I0523 16:25:41.247278 2712679360 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0523 16:25:41.247283 2712679360 net.cpp:137] Memory required for data: 27444400
I0523 16:25:41.247287 2712679360 layer_factory.hpp:77] Creating layer pool2
I0523 16:25:41.247293 2712679360 net.cpp:84] Creating Layer pool2
I0523 16:25:41.247298 2712679360 net.cpp:406] pool2 &lt;- conv2
I0523 16:25:41.247301 2712679360 net.cpp:380] pool2 -&gt; pool2
I0523 16:25:41.247308 2712679360 net.cpp:122] Setting up pool2
I0523 16:25:41.247313 2712679360 net.cpp:129] Top shape: 100 32 8 8 (204800)
I0523 16:25:41.247318 2712679360 net.cpp:137] Memory required for data: 28263600
I0523 16:25:41.247321 2712679360 layer_factory.hpp:77] Creating layer conv3
I0523 16:25:41.247329 2712679360 net.cpp:84] Creating Layer conv3
I0523 16:25:41.247334 2712679360 net.cpp:406] conv3 &lt;- pool2
I0523 16:25:41.247339 2712679360 net.cpp:380] conv3 -&gt; conv3
I0523 16:25:41.248001 2712679360 net.cpp:122] Setting up conv3
I0523 16:25:41.248008 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248013 2712679360 net.cpp:137] Memory required for data: 29902000
I0523 16:25:41.248020 2712679360 layer_factory.hpp:77] Creating layer relu3
I0523 16:25:41.248025 2712679360 net.cpp:84] Creating Layer relu3
I0523 16:25:41.248051 2712679360 net.cpp:406] relu3 &lt;- conv3
I0523 16:25:41.248057 2712679360 net.cpp:367] relu3 -&gt; conv3 (in-place)
I0523 16:25:41.248067 2712679360 net.cpp:122] Setting up relu3
I0523 16:25:41.248072 2712679360 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0523 16:25:41.248077 2712679360 net.cpp:137] Memory required for data: 31540400
I0523 16:25:41.248081 2712679360 layer_factory.hpp:77] Creating layer pool3
I0523 16:25:41.248085 2712679360 net.cpp:84] Creating Layer pool3
I0523 16:25:41.248090 2712679360 net.cpp:406] pool3 &lt;- conv3
I0523 16:25:41.248095 2712679360 net.cpp:380] pool3 -&gt; pool3
I0523 16:25:41.248102 2712679360 net.cpp:122] Setting up pool3
I0523 16:25:41.248109 2712679360 net.cpp:129] Top shape: 100 64 4 4 (102400)
I0523 16:25:41.248114 2712679360 net.cpp:137] Memory required for data: 31950000
I0523 16:25:41.248117 2712679360 layer_factory.hpp:77] Creating layer ip1
I0523 16:25:41.248124 2712679360 net.cpp:84] Creating Layer ip1
I0523 16:25:41.248152 2712679360 net.cpp:406] ip1 &lt;- pool3
I0523 16:25:41.248162 2712679360 net.cpp:380] ip1 -&gt; ip1
I0523 16:25:41.248950 2712679360 net.cpp:122] Setting up ip1
I0523 16:25:41.248993 2712679360 net.cpp:129] Top shape: 100 64 (6400)
I0523 16:25:41.249008 2712679360 net.cpp:137] Memory required for data: 31975600
I0523 16:25:41.249014 2712679360 layer_factory.hpp:77] Creating layer ip2
I0523 16:25:41.249020 2712679360 net.cpp:84] Creating Layer ip2
I0523 16:25:41.249024 2712679360 net.cpp:406] ip2 &lt;- ip1
I0523 16:25:41.249038 2712679360 net.cpp:380] ip2 -&gt; ip2
I0523 16:25:41.249080 2712679360 net.cpp:122] Setting up ip2
I0523 16:25:41.249097 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249102 2712679360 net.cpp:137] Memory required for data: 31979600
I0523 16:25:41.249115 2712679360 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0523 16:25:41.249120 2712679360 net.cpp:84] Creating Layer ip2_ip2_0_split
I0523 16:25:41.249125 2712679360 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0523 16:25:41.249130 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0523 16:25:41.249143 2712679360 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0523 16:25:41.249150 2712679360 net.cpp:122] Setting up ip2_ip2_0_split
I0523 16:25:41.249155 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249164 2712679360 net.cpp:129] Top shape: 100 10 (1000)
I0523 16:25:41.249171 2712679360 net.cpp:137] Memory required for data: 31987600
I0523 16:25:41.249174 2712679360 layer_factory.hpp:77] Creating layer accuracy
I0523 16:25:41.249183 2712679360 net.cpp:84] Creating Layer accuracy
I0523 16:25:41.249187 2712679360 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0523 16:25:41.249191 2712679360 net.cpp:406] accuracy &lt;- label_cifar_1_split_0
I0523 16:25:41.249195 2712679360 net.cpp:380] accuracy -&gt; accuracy
I0523 16:25:41.249202 2712679360 net.cpp:122] Setting up accuracy
I0523 16:25:41.249205 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249209 2712679360 net.cpp:137] Memory required for data: 31987604
I0523 16:25:41.249214 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249219 2712679360 net.cpp:84] Creating Layer loss
I0523 16:25:41.249223 2712679360 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0523 16:25:41.249236 2712679360 net.cpp:406] loss &lt;- label_cifar_1_split_1
I0523 16:25:41.249241 2712679360 net.cpp:380] loss -&gt; loss
I0523 16:25:41.249249 2712679360 layer_factory.hpp:77] Creating layer loss
I0523 16:25:41.249266 2712679360 net.cpp:122] Setting up loss
I0523 16:25:41.249274 2712679360 net.cpp:129] Top shape: (1)
I0523 16:25:41.249279 2712679360 net.cpp:132]     with loss weight 1
I0523 16:25:41.249300 2712679360 net.cpp:137] Memory required for data: 31987608
I0523 16:25:41.249305 2712679360 net.cpp:198] loss needs backward computation.
I0523 16:25:41.249310 2712679360 net.cpp:200] accuracy does not need backward computation.
I0523 16:25:41.249320 2712679360 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0523 16:25:41.249325 2712679360 net.cpp:198] ip2 needs backward computation.
I0523 16:25:41.249330 2712679360 net.cpp:198] ip1 needs backward computation.
I0523 16:25:41.249366 2712679360 net.cpp:198] pool3 needs backward computation.
I0523 16:25:41.249388 2712679360 net.cpp:198] relu3 needs backward computation.
I0523 16:25:41.249392 2712679360 net.cpp:198] conv3 needs backward computation.
I0523 16:25:41.249408 2712679360 net.cpp:198] pool2 needs backward computation.
I0523 16:25:41.249413 2712679360 net.cpp:198] relu2 needs backward computation.
I0523 16:25:41.249416 2712679360 net.cpp:198] conv2 needs backward computation.
I0523 16:25:41.249420 2712679360 net.cpp:198] relu1 needs backward computation.
I0523 16:25:41.249424 2712679360 net.cpp:198] pool1 needs backward computation.
I0523 16:25:41.249428 2712679360 net.cpp:198] conv1 needs backward computation.
I0523 16:25:41.249431 2712679360 net.cpp:200] label_cifar_1_split does not need backward computation.
I0523 16:25:41.249436 2712679360 net.cpp:200] cifar does not need backward computation.
I0523 16:25:41.249439 2712679360 net.cpp:242] This network produces output accuracy
I0523 16:25:41.249444 2712679360 net.cpp:242] This network produces output loss
I0523 16:25:41.249451 2712679360 net.cpp:255] Network initialization done.
I0523 16:25:41.251152 2712679360 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 16:25:41.252013 2712679360 caffe.cpp:290] Running for 100 iterations.
I0523 16:25:41.367466 2712679360 caffe.cpp:313] Batch 0, accuracy = 0.81
I0523 16:25:41.367501 2712679360 caffe.cpp:313] Batch 0, loss = 0.650321
I0523 16:25:41.465518 2712679360 caffe.cpp:313] Batch 1, accuracy = 0.75
I0523 16:25:41.465550 2712679360 caffe.cpp:313] Batch 1, loss = 0.767328
I0523 16:25:41.560680 2712679360 caffe.cpp:313] Batch 2, accuracy = 0.71
I0523 16:25:41.560712 2712679360 caffe.cpp:313] Batch 2, loss = 0.810281
I0523 16:25:41.656878 2712679360 caffe.cpp:313] Batch 3, accuracy = 0.7
I0523 16:25:41.656913 2712679360 caffe.cpp:313] Batch 3, loss = 0.807916
I0523 16:25:41.757275 2712679360 caffe.cpp:313] Batch 4, accuracy = 0.71
I0523 16:25:41.757313 2712679360 caffe.cpp:313] Batch 4, loss = 0.797028
I0523 16:25:41.855583 2712679360 caffe.cpp:313] Batch 5, accuracy = 0.84
I0523 16:25:41.855613 2712679360 caffe.cpp:313] Batch 5, loss = 0.422262
I0523 16:25:41.953912 2712679360 caffe.cpp:313] Batch 6, accuracy = 0.73
I0523 16:25:41.953946 2712679360 caffe.cpp:313] Batch 6, loss = 0.696204
I0523 16:25:42.052671 2712679360 caffe.cpp:313] Batch 7, accuracy = 0.72
I0523 16:25:42.052705 2712679360 caffe.cpp:313] Batch 7, loss = 0.896313
I0523 16:25:42.155107 2712679360 caffe.cpp:313] Batch 8, accuracy = 0.73
I0523 16:25:42.155153 2712679360 caffe.cpp:313] Batch 8, loss = 0.862504
I0523 16:25:42.258592 2712679360 caffe.cpp:313] Batch 9, accuracy = 0.78
I0523 16:25:42.258627 2712679360 caffe.cpp:313] Batch 9, loss = 0.642714
I0523 16:25:42.362510 2712679360 caffe.cpp:313] Batch 10, accuracy = 0.75
I0523 16:25:42.362543 2712679360 caffe.cpp:313] Batch 10, loss = 0.827924
I0523 16:25:42.463922 2712679360 caffe.cpp:313] Batch 11, accuracy = 0.76
I0523 16:25:42.463953 2712679360 caffe.cpp:313] Batch 11, loss = 0.674977
I0523 16:25:42.567791 2712679360 caffe.cpp:313] Batch 12, accuracy = 0.7
I0523 16:25:42.567822 2712679360 caffe.cpp:313] Batch 12, loss = 0.717463
I0523 16:25:42.664435 2712679360 caffe.cpp:313] Batch 13, accuracy = 0.75
I0523 16:25:42.664469 2712679360 caffe.cpp:313] Batch 13, loss = 0.640668
I0523 16:25:42.759980 2712679360 caffe.cpp:313] Batch 14, accuracy = 0.78
I0523 16:25:42.760013 2712679360 caffe.cpp:313] Batch 14, loss = 0.62553
I0523 16:25:42.856386 2712679360 caffe.cpp:313] Batch 15, accuracy = 0.76
I0523 16:25:42.856417 2712679360 caffe.cpp:313] Batch 15, loss = 0.721462
I0523 16:25:42.954746 2712679360 caffe.cpp:313] Batch 16, accuracy = 0.73
I0523 16:25:42.954777 2712679360 caffe.cpp:313] Batch 16, loss = 0.858499
I0523 16:25:43.053562 2712679360 caffe.cpp:313] Batch 17, accuracy = 0.75
I0523 16:25:43.053593 2712679360 caffe.cpp:313] Batch 17, loss = 0.746772
I0523 16:25:43.155479 2712679360 caffe.cpp:313] Batch 18, accuracy = 0.74
I0523 16:25:43.155508 2712679360 caffe.cpp:313] Batch 18, loss = 0.893995
I0523 16:25:43.254688 2712679360 caffe.cpp:313] Batch 19, accuracy = 0.68
I0523 16:25:43.254716 2712679360 caffe.cpp:313] Batch 19, loss = 0.943102
I0523 16:25:43.364045 2712679360 caffe.cpp:313] Batch 20, accuracy = 0.7
I0523 16:25:43.364076 2712679360 caffe.cpp:313] Batch 20, loss = 0.786499
I0523 16:25:43.465351 2712679360 caffe.cpp:313] Batch 21, accuracy = 0.76
I0523 16:25:43.465384 2712679360 caffe.cpp:313] Batch 21, loss = 0.742349
I0523 16:25:43.560330 2712679360 caffe.cpp:313] Batch 22, accuracy = 0.8
I0523 16:25:43.560362 2712679360 caffe.cpp:313] Batch 22, loss = 0.707087
I0523 16:25:43.662050 2712679360 caffe.cpp:313] Batch 23, accuracy = 0.69
I0523 16:25:43.662077 2712679360 caffe.cpp:313] Batch 23, loss = 0.854361
I0523 16:25:43.760444 2712679360 caffe.cpp:313] Batch 24, accuracy = 0.74
I0523 16:25:43.760473 2712679360 caffe.cpp:313] Batch 24, loss = 0.844035
I0523 16:25:43.858397 2712679360 caffe.cpp:313] Batch 25, accuracy = 0.68
I0523 16:25:43.858425 2712679360 caffe.cpp:313] Batch 25, loss = 1.02302
I0523 16:25:43.959595 2712679360 caffe.cpp:313] Batch 26, accuracy = 0.82
I0523 16:25:43.959627 2712679360 caffe.cpp:313] Batch 26, loss = 0.493385
I0523 16:25:44.057914 2712679360 caffe.cpp:313] Batch 27, accuracy = 0.76
I0523 16:25:44.057942 2712679360 caffe.cpp:313] Batch 27, loss = 0.78877
I0523 16:25:44.157359 2712679360 caffe.cpp:313] Batch 28, accuracy = 0.78
I0523 16:25:44.157388 2712679360 caffe.cpp:313] Batch 28, loss = 0.709657
I0523 16:25:44.285976 2712679360 caffe.cpp:313] Batch 29, accuracy = 0.78
I0523 16:25:44.286007 2712679360 caffe.cpp:313] Batch 29, loss = 0.674438
I0523 16:25:44.390980 2712679360 caffe.cpp:313] Batch 30, accuracy = 0.79
I0523 16:25:44.391010 2712679360 caffe.cpp:313] Batch 30, loss = 0.65947
I0523 16:25:44.491211 2712679360 caffe.cpp:313] Batch 31, accuracy = 0.77
I0523 16:25:44.491241 2712679360 caffe.cpp:313] Batch 31, loss = 0.716022
I0523 16:25:44.593423 2712679360 caffe.cpp:313] Batch 32, accuracy = 0.73
I0523 16:25:44.593457 2712679360 caffe.cpp:313] Batch 32, loss = 0.805526
I0523 16:25:44.692994 2712679360 caffe.cpp:313] Batch 33, accuracy = 0.68
I0523 16:25:44.693023 2712679360 caffe.cpp:313] Batch 33, loss = 0.903316
I0523 16:25:44.795087 2712679360 caffe.cpp:313] Batch 34, accuracy = 0.72
I0523 16:25:44.795116 2712679360 caffe.cpp:313] Batch 34, loss = 0.834438
I0523 16:25:44.897828 2712679360 caffe.cpp:313] Batch 35, accuracy = 0.73
I0523 16:25:44.897874 2712679360 caffe.cpp:313] Batch 35, loss = 0.908751
I0523 16:25:44.996119 2712679360 caffe.cpp:313] Batch 36, accuracy = 0.74
I0523 16:25:44.996150 2712679360 caffe.cpp:313] Batch 36, loss = 0.981981
I0523 16:25:45.093991 2712679360 caffe.cpp:313] Batch 37, accuracy = 0.76
I0523 16:25:45.094023 2712679360 caffe.cpp:313] Batch 37, loss = 0.725703
I0523 16:25:45.195551 2712679360 caffe.cpp:313] Batch 38, accuracy = 0.78
I0523 16:25:45.195585 2712679360 caffe.cpp:313] Batch 38, loss = 0.686703
I0523 16:25:45.292881 2712679360 caffe.cpp:313] Batch 39, accuracy = 0.8
I0523 16:25:45.292912 2712679360 caffe.cpp:313] Batch 39, loss = 0.650689
I0523 16:25:45.397084 2712679360 caffe.cpp:313] Batch 40, accuracy = 0.79
I0523 16:25:45.397115 2712679360 caffe.cpp:313] Batch 40, loss = 0.755663
I0523 16:25:45.495128 2712679360 caffe.cpp:313] Batch 41, accuracy = 0.82
I0523 16:25:45.495160 2712679360 caffe.cpp:313] Batch 41, loss = 0.855221
I0523 16:25:45.597597 2712679360 caffe.cpp:313] Batch 42, accuracy = 0.81
I0523 16:25:45.597626 2712679360 caffe.cpp:313] Batch 42, loss = 0.552907
I0523 16:25:45.695441 2712679360 caffe.cpp:313] Batch 43, accuracy = 0.8
I0523 16:25:45.695472 2712679360 caffe.cpp:313] Batch 43, loss = 0.688889
I0523 16:25:45.796842 2712679360 caffe.cpp:313] Batch 44, accuracy = 0.8
I0523 16:25:45.796875 2712679360 caffe.cpp:313] Batch 44, loss = 0.713613
I0523 16:25:45.899427 2712679360 caffe.cpp:313] Batch 45, accuracy = 0.76
I0523 16:25:45.899462 2712679360 caffe.cpp:313] Batch 45, loss = 0.819739
I0523 16:25:46.003129 2712679360 caffe.cpp:313] Batch 46, accuracy = 0.77
I0523 16:25:46.003190 2712679360 caffe.cpp:313] Batch 46, loss = 0.79499
I0523 16:25:46.101080 2712679360 caffe.cpp:313] Batch 47, accuracy = 0.73
I0523 16:25:46.101112 2712679360 caffe.cpp:313] Batch 47, loss = 0.784097
I0523 16:25:46.199532 2712679360 caffe.cpp:313] Batch 48, accuracy = 0.82
I0523 16:25:46.199563 2712679360 caffe.cpp:313] Batch 48, loss = 0.509592
I0523 16:25:46.296840 2712679360 caffe.cpp:313] Batch 49, accuracy = 0.76
I0523 16:25:46.296872 2712679360 caffe.cpp:313] Batch 49, loss = 0.775396
I0523 16:25:46.399880 2712679360 caffe.cpp:313] Batch 50, accuracy = 0.77
I0523 16:25:46.399914 2712679360 caffe.cpp:313] Batch 50, loss = 0.61452
I0523 16:25:46.500458 2712679360 caffe.cpp:313] Batch 51, accuracy = 0.79
I0523 16:25:46.500488 2712679360 caffe.cpp:313] Batch 51, loss = 0.631971
I0523 16:25:46.599107 2712679360 caffe.cpp:313] Batch 52, accuracy = 0.78
I0523 16:25:46.599139 2712679360 caffe.cpp:313] Batch 52, loss = 0.613152
I0523 16:25:46.699442 2712679360 caffe.cpp:313] Batch 53, accuracy = 0.74
I0523 16:25:46.699475 2712679360 caffe.cpp:313] Batch 53, loss = 0.813763
I0523 16:25:46.802717 2712679360 caffe.cpp:313] Batch 54, accuracy = 0.69
I0523 16:25:46.802749 2712679360 caffe.cpp:313] Batch 54, loss = 0.79753
I0523 16:25:46.903400 2712679360 caffe.cpp:313] Batch 55, accuracy = 0.81
I0523 16:25:46.903430 2712679360 caffe.cpp:313] Batch 55, loss = 0.683275
I0523 16:25:47.007345 2712679360 caffe.cpp:313] Batch 56, accuracy = 0.78
I0523 16:25:47.007377 2712679360 caffe.cpp:313] Batch 56, loss = 0.785579
I0523 16:25:47.107044 2712679360 caffe.cpp:313] Batch 57, accuracy = 0.84
I0523 16:25:47.107076 2712679360 caffe.cpp:313] Batch 57, loss = 0.455638
I0523 16:25:47.204998 2712679360 caffe.cpp:313] Batch 58, accuracy = 0.7
I0523 16:25:47.205029 2712679360 caffe.cpp:313] Batch 58, loss = 0.685973
I0523 16:25:47.307816 2712679360 caffe.cpp:313] Batch 59, accuracy = 0.74
I0523 16:25:47.307848 2712679360 caffe.cpp:313] Batch 59, loss = 0.815847
I0523 16:25:47.409512 2712679360 caffe.cpp:313] Batch 60, accuracy = 0.79
I0523 16:25:47.409544 2712679360 caffe.cpp:313] Batch 60, loss = 0.694609
I0523 16:25:47.509786 2712679360 caffe.cpp:313] Batch 61, accuracy = 0.72
I0523 16:25:47.509819 2712679360 caffe.cpp:313] Batch 61, loss = 0.721049
I0523 16:25:47.608265 2712679360 caffe.cpp:313] Batch 62, accuracy = 0.76
I0523 16:25:47.608304 2712679360 caffe.cpp:313] Batch 62, loss = 0.649006
I0523 16:25:47.711271 2712679360 caffe.cpp:313] Batch 63, accuracy = 0.77
I0523 16:25:47.711302 2712679360 caffe.cpp:313] Batch 63, loss = 0.620039
I0523 16:25:47.812440 2712679360 caffe.cpp:313] Batch 64, accuracy = 0.71
I0523 16:25:47.812471 2712679360 caffe.cpp:313] Batch 64, loss = 0.706689
I0523 16:25:47.911661 2712679360 caffe.cpp:313] Batch 65, accuracy = 0.77
I0523 16:25:47.911694 2712679360 caffe.cpp:313] Batch 65, loss = 0.824431
I0523 16:25:48.011318 2712679360 caffe.cpp:313] Batch 66, accuracy = 0.73
I0523 16:25:48.011351 2712679360 caffe.cpp:313] Batch 66, loss = 0.739382
I0523 16:25:48.117573 2712679360 caffe.cpp:313] Batch 67, accuracy = 0.7
I0523 16:25:48.117606 2712679360 caffe.cpp:313] Batch 67, loss = 0.800725
I0523 16:25:48.214515 2712679360 caffe.cpp:313] Batch 68, accuracy = 0.68
I0523 16:25:48.214545 2712679360 caffe.cpp:313] Batch 68, loss = 0.807705
I0523 16:25:48.314254 2712679360 caffe.cpp:313] Batch 69, accuracy = 0.7
I0523 16:25:48.314283 2712679360 caffe.cpp:313] Batch 69, loss = 0.952385
I0523 16:25:48.412657 2712679360 caffe.cpp:313] Batch 70, accuracy = 0.74
I0523 16:25:48.412686 2712679360 caffe.cpp:313] Batch 70, loss = 0.781932
I0523 16:25:48.512931 2712679360 caffe.cpp:313] Batch 71, accuracy = 0.73
I0523 16:25:48.512964 2712679360 caffe.cpp:313] Batch 71, loss = 0.895561
I0523 16:25:48.608669 2712679360 caffe.cpp:313] Batch 72, accuracy = 0.8
I0523 16:25:48.608700 2712679360 caffe.cpp:313] Batch 72, loss = 0.615967
I0523 16:25:48.705847 2712679360 caffe.cpp:313] Batch 73, accuracy = 0.78
I0523 16:25:48.705878 2712679360 caffe.cpp:313] Batch 73, loss = 0.588951
I0523 16:25:48.803540 2712679360 caffe.cpp:313] Batch 74, accuracy = 0.72
I0523 16:25:48.803591 2712679360 caffe.cpp:313] Batch 74, loss = 0.784208
I0523 16:25:48.906528 2712679360 caffe.cpp:313] Batch 75, accuracy = 0.77
I0523 16:25:48.906565 2712679360 caffe.cpp:313] Batch 75, loss = 0.529825
I0523 16:25:49.007186 2712679360 caffe.cpp:313] Batch 76, accuracy = 0.77
I0523 16:25:49.007216 2712679360 caffe.cpp:313] Batch 76, loss = 0.794115
I0523 16:25:49.107000 2712679360 caffe.cpp:313] Batch 77, accuracy = 0.76
I0523 16:25:49.107033 2712679360 caffe.cpp:313] Batch 77, loss = 0.726804
I0523 16:25:49.205263 2712679360 caffe.cpp:313] Batch 78, accuracy = 0.77
I0523 16:25:49.205294 2712679360 caffe.cpp:313] Batch 78, loss = 0.919712
I0523 16:25:49.304277 2712679360 caffe.cpp:313] Batch 79, accuracy = 0.69
I0523 16:25:49.304309 2712679360 caffe.cpp:313] Batch 79, loss = 0.87618
I0523 16:25:49.404642 2712679360 caffe.cpp:313] Batch 80, accuracy = 0.77
I0523 16:25:49.404672 2712679360 caffe.cpp:313] Batch 80, loss = 0.704637
I0523 16:25:49.501708 2712679360 caffe.cpp:313] Batch 81, accuracy = 0.75
I0523 16:25:49.501739 2712679360 caffe.cpp:313] Batch 81, loss = 0.71787
I0523 16:25:49.599267 2712679360 caffe.cpp:313] Batch 82, accuracy = 0.76
I0523 16:25:49.599304 2712679360 caffe.cpp:313] Batch 82, loss = 0.613339
I0523 16:25:49.698971 2712679360 caffe.cpp:313] Batch 83, accuracy = 0.78
I0523 16:25:49.699002 2712679360 caffe.cpp:313] Batch 83, loss = 0.689216
I0523 16:25:49.803320 2712679360 caffe.cpp:313] Batch 84, accuracy = 0.72
I0523 16:25:49.803352 2712679360 caffe.cpp:313] Batch 84, loss = 0.817351
I0523 16:25:49.904433 2712679360 caffe.cpp:313] Batch 85, accuracy = 0.78
I0523 16:25:49.904467 2712679360 caffe.cpp:313] Batch 85, loss = 0.62069
I0523 16:25:50.005846 2712679360 caffe.cpp:313] Batch 86, accuracy = 0.75
I0523 16:25:50.005878 2712679360 caffe.cpp:313] Batch 86, loss = 0.680651
I0523 16:25:50.103121 2712679360 caffe.cpp:313] Batch 87, accuracy = 0.78
I0523 16:25:50.103153 2712679360 caffe.cpp:313] Batch 87, loss = 0.788875
I0523 16:25:50.200103 2712679360 caffe.cpp:313] Batch 88, accuracy = 0.8
I0523 16:25:50.200134 2712679360 caffe.cpp:313] Batch 88, loss = 0.620548
I0523 16:25:50.299957 2712679360 caffe.cpp:313] Batch 89, accuracy = 0.74
I0523 16:25:50.299989 2712679360 caffe.cpp:313] Batch 89, loss = 0.779962
I0523 16:25:50.399699 2712679360 caffe.cpp:313] Batch 90, accuracy = 0.75
I0523 16:25:50.399731 2712679360 caffe.cpp:313] Batch 90, loss = 0.70084
I0523 16:25:50.502117 2712679360 caffe.cpp:313] Batch 91, accuracy = 0.79
I0523 16:25:50.502148 2712679360 caffe.cpp:313] Batch 91, loss = 0.576651
I0523 16:25:50.599150 2712679360 caffe.cpp:313] Batch 92, accuracy = 0.71
I0523 16:25:50.599181 2712679360 caffe.cpp:313] Batch 92, loss = 0.9778
I0523 16:25:50.699782 2712679360 caffe.cpp:313] Batch 93, accuracy = 0.78
I0523 16:25:50.699813 2712679360 caffe.cpp:313] Batch 93, loss = 0.795732
I0523 16:25:50.802847 2712679360 caffe.cpp:313] Batch 94, accuracy = 0.77
I0523 16:25:50.802877 2712679360 caffe.cpp:313] Batch 94, loss = 0.803904
I0523 16:25:50.900668 2712679360 caffe.cpp:313] Batch 95, accuracy = 0.77
I0523 16:25:50.900702 2712679360 caffe.cpp:313] Batch 95, loss = 0.664654
I0523 16:25:50.902439 102174720 data_layer.cpp:73] Restarting data prefetching from start.
I0523 16:25:50.999625 2712679360 caffe.cpp:313] Batch 96, accuracy = 0.74
I0523 16:25:50.999656 2712679360 caffe.cpp:313] Batch 96, loss = 0.700099
I0523 16:25:51.100697 2712679360 caffe.cpp:313] Batch 97, accuracy = 0.66
I0523 16:25:51.100728 2712679360 caffe.cpp:313] Batch 97, loss = 0.937044
I0523 16:25:51.201591 2712679360 caffe.cpp:313] Batch 98, accuracy = 0.79
I0523 16:25:51.201622 2712679360 caffe.cpp:313] Batch 98, loss = 0.677679
I0523 16:25:51.299702 2712679360 caffe.cpp:313] Batch 99, accuracy = 0.76
I0523 16:25:51.299736 2712679360 caffe.cpp:313] Batch 99, loss = 0.687144
I0523 16:25:51.299741 2712679360 caffe.cpp:318] Loss: 0.742307
I0523 16:25:51.299762 2712679360 caffe.cpp:330] accuracy = 0.754
I0523 16:25:51.299773 2712679360 caffe.cpp:330] loss = 0.742307 (* 1 = 0.742307 loss)
</code></pre>

<p>得到最终的测试集准确率可以到达<code>accuracy = 0.754</code></p>

<p>到这里我们对于练习 cifar10模型<br/>
就结束了.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Struct]]></title>
    <link href="https://lockxmonk.github.io/14955029237844.html"/>
    <updated>2017-05-23T09:28:43+08:00</updated>
    <id>https://lockxmonk.github.io/14955029237844.html</id>
    <content type="html"><![CDATA[
<p>准确地讲，Python没有专门处理字节的数据类型。但由于<code>str</code>既是字符串，又可以表示字节，所以，字节数组＝str。而在C语言中，我们可以很方便地用struct、union来处理字节，以及字节和int，float的转换。</p>

<p>在Python中，比方说要把一个32位无符号整数变成字节，也就是4个长度的str，你得配合位运算符来完成.非常麻烦不利于效率开发.</p>

<p>好在Python提供了一个<code>struct</code>模块来解决str和其他二进制数据类型的转换。</p>

<p><code>struct的pack</code>函数把任意数据类型变成字符串：</p>

<pre><code class="language-py">&gt;&gt;&gt; import struct
&gt;&gt;&gt; struct.pack(&#39;&gt;I&#39;,10240099)
&#39;\x00\x9c@c&#39;
&gt;&gt;&gt;
</code></pre>

<p><code>pack</code>的第一个参数是处理指令，<code>&#39;&gt;I&#39;</code>的意思是：</p>

<p><strong><code>&gt;</code>表示字节顺序是<code>big-endian</code>，也就是网络序，<code>I</code>表示4字节无符号整数。</strong></p>

<p>后面的参数个数要和处理指令一致。</p>

<p><code>unpack</code>把<code>str</code>变成相应的数据类型：</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import struct
print struct.unpack(&#39;&gt;IH&#39;,&#39;\xf0\xf0\xf0\xf0\x80\x80&#39;)
</code></pre>

<p><img src="media/14955029237844/14955050577583.jpg" alt=""/></p>

<p>根据&gt;IH的说明，后面的str依次变为I：4字节无符号整数和H：2字节无符号整数。</p>

<p>所以，尽管Python不适合编写底层操作字节流的代码，但在对性能要求不高的地方，利用<code>struct</code>就方便多了。</p>

<p><code>struct</code>模块定义的数据类型可以参考Python官方文档：<a href="https://docs.python.org/2/library/struct.html#format-characters">https://docs.python.org/2/library/struct.html#format-characters</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Base64]]></title>
    <link href="https://lockxmonk.github.io/14955000885655.html"/>
    <updated>2017-05-23T08:41:28+08:00</updated>
    <id>https://lockxmonk.github.io/14955000885655.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">练习:</a>
</li>
<li>
<a href="#toc_1">小结</a>
</li>
</ul>


<p>Base64是一种用64个字符来表示任意二进制数据的方法。</p>

<p>用记事本打开<code>exe、jpg、pdf</code>这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。</p>

<p>Base64的原理很简单，首先，准备一个包含64个字符的数组：</p>

<pre><code>[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, ... &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, ... &#39;0&#39;, &#39;1&#39;, ... &#39;+&#39;, &#39;/&#39;]
</code></pre>

<p>然后，对二进制数据进行处理，每3个字节一组，一共是<code>3x8=24bit</code>，划为4组，每组正好6个bit：<img src="media/14955000885655/14955010424104.jpg" alt=""/></p>

<p>这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。</p>

<p>所以，Base64编码会把3字节的二进制数据编码为4字节的文本数据，长度增加33%，好处是编码后的文本数据可以在邮件正文、网页等直接显示。</p>

<p>如果要编码的二进制数据不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用<code>\x00</code>字节在末尾补足后，再在编码的末尾加上1个或2个<code>=</code>号，表示补了多少字节，解码的时候，会自动去掉。</p>

<p>Python内置的<code>base64</code>可以直接进行base64的编解码：</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import base64
print base64.b64encode(&#39;binary\x00string&#39;)
print base64.b64decode(&#39;YmluYXJ5AHN0cmluZw==&#39;)
</code></pre>

<p><img src="media/14955000885655/14955014556004.jpg" alt=""/></p>

<p>由于标准的Base64编码后可能出现字符<code>+</code>和<code>/</code>，在URL中就不能直接作为参数，所以又有一种&quot;url safe&quot;的base64编码，其实就是把字符<code>+</code>和<code>/</code>分别变成<code>-</code>和<code>_</code>：<br/>
<img src="media/14955000885655/14955018446875.jpg" alt=""/><br/>
还可以自己定义64个字符的排列顺序，这样就可以自定义Base64编码，不过，通常情况下完全没有必要。</p>

<p>Base64是一种通过查表的编码方法，不能用于加密，即使使用自定义的编码表也不行。</p>

<p>Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。</p>

<p>由于<code>=</code>字符也可能出现在Base64编码中，但<code>=</code>用在URL、Cookie里面会造成歧义，所以，很多Base64编码后会把<code>=</code>去掉：</p>

<pre><code># 标准Base64:
&#39;abcd&#39; -&gt; &#39;YWJjZA==&#39;
# 自动去掉=:
&#39;abcd&#39; -&gt; &#39;YWJjZA&#39;
</code></pre>

<p>去掉=后怎么解码呢？因为Base64是把3个字节变为4个字节，所以，Base64编码的长度永远是4的倍数，因此，需要加上=把Base64字符串的长度变为4的倍数，就可以正常解码了。</p>

<h2 id="toc_0">练习:</h2>

<p>请写一个能去掉<code>=</code>的base64解码函数:</p>

<pre><code class="language-py">#-*- coding: utf-8 -*-
import base64
def safe_b64decode(str):
    num = len(str)%4
    if num==0:
        return base64.b64decode(str)
    else :
        for x in range(num):
             str = str + &#39;=&#39;
        return str 
print base64.b64decode(&#39;YWJjZA==&#39;)
#print base64.b64decode(&#39;YWJjZA&#39;)
print safe_b64decode(&#39;YWJjZA&#39;)
</code></pre>

<p><img src="media/14955000885655/14955026953083.jpg" alt=""/></p>

<h2 id="toc_1">小结</h2>

<p>Base64是一种任意二进制到文本字符串的编码方法，常用于在URL、Cookie、网页中传输少量二进制数据。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[下学期英语课后翻译总结]]></title>
    <link href="https://lockxmonk.github.io/14948310433460.html"/>
    <updated>2017-05-15T14:50:43+08:00</updated>
    <id>https://lockxmonk.github.io/14948310433460.html</id>
    <content type="html"><![CDATA[
<p>据说这些已经被收集的纸条大约有两百万份，粗略分类后捆绑到一起，这些纸条无疑布满灰尘(并且起线头)，有的甚至已经卷曲发黄，还有的因为年久儿破碎。</p>

<p>问题之一是：志愿阅读者对于所谓“普通词语”的热情关注向来就不高；他们总是受到某种诱惑力的吸引，寄来一些令人感兴趣的词汇的引语，而不是日常词汇的引语。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在RN中利用原生Document Interaction Controller来预览和打开文档]]></title>
    <link href="https://lockxmonk.github.io/14948283904445.html"/>
    <updated>2017-05-15T14:06:30+08:00</updated>
    <id>https://lockxmonk.github.io/14948283904445.html</id>
    <content type="html"><![CDATA[
<p>在react-native开发中有一个功能需求为，打开之前已经下载到Documents中的文件。一开始本想用webview来加载文件。后来发现有<code>UIDocumentInteractionController</code>这个api来更好的完成这个功能。</p>

<p>首先在RN引用的类中加入<code>UIDocumentInteractionControllerDelegate</code>我们要用到这个来呈现预览视图：</p>

<p>NativeUtil.h<br/>
```oc</p>

<h1 id="toc_0">import <Foundation/Foundation.h></h1>

<h1 id="toc_1">import &quot;RCTBridgeModule.h&quot;</h1>

<h1 id="toc_2">import &quot;UIView+React.h&quot;       //要引入这个头文件，在rn中创建和加载原声视图</h1>

<p>@interface NativeUtil : NSObject <RCTBridgeModule,UIDocumentInteractionControllerDelegate></p>

<p>@end<br/>
```</p>

<p>在NativeUtil.m中实现该委托。定义一个暴露给RN的方法，在js中调用：</p>

<pre><code class="language-oc">RCT_EXPORT_METHOD(ShowFileChooser: (RCTResponseSenderBlock)callback){
  
    NSString *filePath = @&quot;ceshi007&quot;;           callback(@[[NSNull null],filePath]);    //测试callback，从native向rn传值。


    NSArray *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);           //获取应用本沙盒内的目录。
    NSString *documentpath = paths[0];      //获取document的路径
    NSString *fileName = [documentpath stringByAppendingPathComponent:@&quot;Screenshot_2016-12-15-21-29-07-50.png&quot;];            //指定上述图片的路径。
  NSURL *fileURL = [NSURL fileURLWithPath:fileName];
  dispatch_async(dispatch_get_main_queue(), ^{              //运行主线程来加载界面，并初始化UIDocumentInteractionController
    UIDocumentInteractionController *documentInteractionController = [UIDocumentInteractionController interactionControllerWithURL:fileURL];
    documentInteractionController.delegate =self ;
    [documentInteractionController presentPreviewAnimated:YES];             //present出来文件的预览界面。
  });
  
}
</code></pre>

<p>之后定义<code>(UIViewController *)documentInteractionControllerViewControllerForPreview:(UIDocumentInteractionController *)controller</code>方法，要加载出来预览的界面，上述方法必须实现，且返回一个当前的页面的ViewController。用于作为预览视图的父ViewController来弹出modal。</p>

<pre><code class="language-oc">- (UIViewController *)documentInteractionControllerViewControllerForPreview:(UIDocumentInteractionController *)controller{
  UIViewController *newVC =[self getPresentedViewController];  //获取当前屏幕中present出来的viewcontroller。
   return newVC;
};
</code></pre>

<p>这里我们是在一个非视图类创建并在加载一个视图，所以我们要先获取到当前界面的<code>ViewController</code>，将被展示的<code>view</code>加到当前<code>view</code>的子视图，或用当前view <code>presentViewController</code>，或<code>pushViewContrller</code>。这里我从网上找到了两个方法：</p>

<pre><code class="language-oc">//获取当前屏幕显示的viewcontroller
- (UIViewController *)getCurrentVC  
{  
    UIViewController *result = nil;  
      
    UIWindow * window = [[UIApplication sharedApplication] keyWindow];  
    if (window.windowLevel != UIWindowLevelNormal)  
    {  
        NSArray *windows = [[UIApplication sharedApplication] windows];  
        for(UIWindow * tmpWin in windows)  
        {  
            if (tmpWin.windowLevel == UIWindowLevelNormal)  
            {  
                window = tmpWin;  
                break;  
            }  
        }  
    }  
      
    UIView *frontView = [[window subviews] objectAtIndex:0];  
    id nextResponder = [frontView nextResponder];  
      
    if ([nextResponder isKindOfClass:[UIViewController class]])  
        result = nextResponder;  
    else  
        result = window.rootViewController;  
      
    return result;  
} 
</code></pre>

<pre><code class="language-oc">//获取当前屏幕中present出来的viewcontroller
- (UIViewController *)getPresentedViewController  
{  
    UIViewController *appRootVC = [UIApplication sharedApplication].keyWindow.rootViewController;  
    UIViewController *topVC = appRootVC;  
    if (topVC.presentedViewController) {  
        topVC = topVC.presentedViewController;  
    }  
      
    return topVC;  
}
</code></pre>

<blockquote>
<p>在RN中，使用modal组件之后，弹出的modal视图的viewcontroller相当于present出来的viewcontroller.</p>
</blockquote>

<p>之后使用<code>[self getPresentedViewController]</code>就可以获得<code>viewcontroller</code>来加载视图。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[caffe框架运行手写体数字识别例子（MNIST）]]></title>
    <link href="https://lockxmonk.github.io/14946429015704.html"/>
    <updated>2017-05-13T10:35:01+08:00</updated>
    <id>https://lockxmonk.github.io/14946429015704.html</id>
    <content type="html"><![CDATA[
<p>关于caffe环境的搭建暂时不做讨论，之后有时间整理一下（mac系统上cpu-only的caffe环境搭建）。</p>

<h2 id="toc_0">LeNet-5模型描述</h2>

<p>caffe框架中给的LeNet-5模型与原版有所不同，其中将Sigmoid激活函数换成了ReLu，它的描述文件为<code>examples/mnist/lenet_train_test.prototxt</code><br/>
，它的内容主要为：</p>

<pre><code>name: &quot;LeNet&quot;           //网络（Net）的名称为LeNet
layer {                 //定义一个层（Layer）
  name: &quot;mnist&quot;         //层的名称为mnist
  type: &quot;Data&quot;          //层的类型为数据层
  top: &quot;data&quot;           //层的输出blob有两个：data和label
  top: &quot;label&quot;
  include {
    phase: TRAIN        //表明该层参数只在训练阶段有效
  }
  transform_param {
    scale: 0.00390625   //数据变换使用的数据缩放因子
  }
  data_param {          //数据层参数
    source: &quot;examples/mnist/mnist_train_lmdb&quot;       //LMDB的路径
    batch_size: 64      //批量数目，一次性读取64张图
    backend: LMDB       //数据格式为LMDB
  }
}
layer {                 //一个新的数据层，名字也叫mnist，输出blob也是data和Label，但是这里定义的参数只在分类阶段有效
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST         //表明只在测试分类阶段有效
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;         //定义一个新的卷积层conv1，输入blob为data，输出blob为conv1
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1          //权值学习速率倍乘因子，1蓓表示与全局参数一致
  }
  param {
    lr_mult: 2          //bias学习速率倍乘因子，是全局参数的两倍
  }
  convolution_param {   //卷积计算参数
    num_output: 20      //输出feature map数目为20
    kernel_size: 5      //卷积核尺寸，5*5
    stride: 1           //卷积输出跳跃间隔，1表示连续输出，无跳跃
    weight_filler {     //权值使用Xavier填充器
      type: &quot;xavier&quot;
    }
    bias_filler {       //bias使用常熟填充器，默认为0
      type: &quot;constant&quot;
    }
  }
}
layer {                 //定义新的下采样层pool1，输入blob为conv1，输出blob为pool1
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {       //下采样参数
    pool: MAX           //使用最大值下采样方法
    kernel_size: 2      //下采样窗口尺寸为2*2
    stride: 2           //下采样输出跳跃间隔2*2
  }
}   
layer {                 //新的卷积层，和conv1类似
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //新的下采样层，和pool1类似
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {                 //新的全连接层，输入blob为pool2，输出blob为ip1
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {     //全连接层参数
    num_output: 500         //该层输出元素个数为500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //新的非线性层，用ReLU方法
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {             //分类准确率层，输入blob为ip2和Label，输出blob为accuracy，该层用于计算分类准确率
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {             //损失层，损失函数SoftmaxLoss，输入blob为ip2和label，输出blob为loss
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}

</code></pre>

<p>LeNet模型原图为：<br/>
<img src="media/14946429015704/14946445265310.jpg" alt=""/></p>

<p><img src="media/14946429015704/14946445430850.jpg" alt=""/></p>

<h2 id="toc_1">训练超参数：</h2>

<p>上面已近给出了LeNet模型中的网络结构图和一些参数定义，下面我们正式来训练，这是一个分类准确率可以达到99%以上的模型。</p>

<p>首先进入caffe所在目录：<br/>
执行：<code>examples/mnist/train_lenet.sh</code></p>

<p><code>train_lenet.sh</code>的代码为：</p>

<pre><code>#!/usr/bin/env sh
set -e          #暂时不知道具体作用

./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt $@

</code></pre>

<p>这里调用了之前编译好的<code>build/tools/caffe.bin</code>二进制文件，参数为：--<code>solver=examples/mnist/lenet_solver.prototxt $@</code>指定了训练超参数文件，内容如下：</p>

<pre><code class="language-py"># The train/test net protocol buffer definition
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.（训练时每迭代500次，进行一次预测）
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.（网络的基础学习速率，冲量和衰减量）
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy（学习速率的衰减策略）
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterations（每100次迭代，在屏幕上打印一次运行log）
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results（每5000次迭代打印一次快照）
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
# solver mode: CPU or GPU（求解模式为CPU模式，因为mac没有N卡）
solver_mode: CPU
</code></pre>

<h2 id="toc_2">训练日志</h2>

<p>执行上面<code>examples/mnist/train_lenet.sh</code>文件后会产生如下的日志输出：</p>

<pre><code>//使用cpu模式运行
I0513 11:18:42.330993 3659862976 caffe.cpp:211] Use CPU.
I0513 11:18:42.331964 3659862976 solver.cpp:44] Initializing solver from parameters:
//打印训练超参数文件lenet_solver.prototxt中经过解析的内容
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
solver_mode: CPU
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
train_state {
  level: 0
  stage: &quot;&quot;
}
I0513 11:18:42.332221 3659862976 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
//解析CNN网络描述文件中的网络参数，创建训练网络
I0513 11:18:42.332438 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0513 11:18:42.332453 3659862976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0513 11:18:42.332459 3659862976 net.cpp:51] Initializing net from parameters:
//打印训练网路参数描述
name: &quot;LeNet&quot;
state {
  phase: TRAIN
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_train_lmdb&quot;
    batch_size: 64
    backend: LMDB
  }
}
//........中间省略
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:18:42.332698 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.332906 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0513 11:18:42.332963 3659862976 net.cpp:84] Creating Layer mnist
//产生两个输出，data为图片数据，label为标签数据
I0513 11:18:42.332970 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.332989 3659862976 net.cpp:380] mnist -&gt; label
//之后打开了训练集LMDB，data为四维数组，又称blob，尺寸为64,1,28,28
I0513 11:18:42.333026 3659862976 data_layer.cpp:45] output data size: 64,1,28,28
I0513 11:18:42.337728 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.337738 3659862976 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0513 11:18:42.337759 3659862976 net.cpp:129] Top shape: 64 (64)
//统计占用内存，会逐层累加
I0513 11:18:42.337762 3659862976 net.cpp:137] Memory required for data: 200960
//盖一楼，conv1
I0513 11:18:42.337769 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.337776 3659862976 net.cpp:84] Creating Layer conv1
//conv1需要一个输入data（来自上一层mnist），产生一个输出conv1(送入下一层)
I0513 11:18:42.337780 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.337785 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.337836 3659862976 net.cpp:122] Setting up conv1
//conv1的输出尺寸为（64,20,24,24）
I0513 11:18:42.337842 3659862976 net.cpp:129] Top shape: 64 20 24 24 (737280)
//统计内存逐层累加
I0513 11:18:42.337847 3659862976 net.cpp:137] Memory required for data: 3150080
I0513 11:18:42.337853 3659862976 layer_factory.hpp:77] Creating layer pool1
//中间层创建类似
I0513 11:18:42.337877 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.337882 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.337887 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.337895 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.337899 3659862976 net.cpp:129] Top shape: 64 20 12 12 (184320)
I0513 11:18:42.337904 3659862976 net.cpp:137] Memory required for data: 3887360
I0513 11:18:42.337908 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.337913 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.337916 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.337921 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.338141 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.338146 3659862976 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0513 11:18:42.338162 3659862976 net.cpp:137] Memory required for data: 4706560
I0513 11:18:42.338167 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.338174 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.338178 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.338182 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.338210 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.338215 3659862976 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0513 11:18:42.338220 3659862976 net.cpp:137] Memory required for data: 4911360
I0513 11:18:42.338224 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.338232 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.338235 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.338240 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.341404 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.341413 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341418 3659862976 net.cpp:137] Memory required for data: 5039360
I0513 11:18:42.341424 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.341433 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.341435 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.341440 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.341444 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.341449 3659862976 net.cpp:129] Top shape: 64 500 (32000)
I0513 11:18:42.341451 3659862976 net.cpp:137] Memory required for data: 5167360
I0513 11:18:42.341455 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.341470 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.341473 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.341478 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.341531 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.341536 3659862976 net.cpp:129] Top shape: 64 10 (640)
I0513 11:18:42.341539 3659862976 net.cpp:137] Memory required for data: 5169920
//盖最后一层loss
I0513 11:18:42.341544 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341550 3659862976 net.cpp:84] Creating Layer loss
//该层需要两个输入ip2和label，产生一个输出loss
I0513 11:18:42.341554 3659862976 net.cpp:406] loss &lt;- ip2
I0513 11:18:42.341557 3659862976 net.cpp:406] loss &lt;- label
I0513 11:18:42.341563 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.341572 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.341583 3659862976 net.cpp:122] Setting up loss
//输出loss尺寸为1，loss weight参数为1
I0513 11:18:42.341586 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.341590 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.341598 3659862976 net.cpp:137] Memory required for data: 5169924
//从后往前统计哪些层需要做反向传播计算（BP）
I0513 11:18:42.341601 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.341606 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.341609 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.341614 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.341616 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.341620 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.341624 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.341627 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.341631 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.341655 3659862976 net.cpp:242] This network produces output loss
//盖楼完毕
I0513 11:18:42.341662 3659862976 net.cpp:255] Network initialization done.
//还需要创建测试网络，在盖一次楼
I0513 11:18:42.341949 3659862976 solver.cpp:172] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0513 11:18:42.341986 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:18:42.341996 3659862976 net.cpp:51] Initializing net 
from parameters:
//类似于第一座楼的情况，只是地基mnist改了一下lmdb源和输出尺寸，顶楼加了一个accuracy阁楼
name: &quot;LeNet&quot;
state {
  phase: TEST
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}

//....中间重复，不表
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
//具体盖楼过程与训练网络类似
I0513 11:18:42.342216 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:18:42.342300 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:18:42.342319 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:18:42.342329 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:18:42.342335 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:18:42.342345 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:18:42.343029 3659862976 net.cpp:122] Setting up mnist
I0513 11:18:42.343037 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:18:42.343057 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343061 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:18:42.343065 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:18:42.343073 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:18:42.343077 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:18:42.343082 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:18:42.343087 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:18:42.343093 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:18:42.343097 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343101 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:18:42.343106 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:18:42.343109 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:18:42.343137 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:18:42.343144 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:18:42.343152 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:18:42.343175 3659862976 net.cpp:122] Setting up conv1
I0513 11:18:42.343181 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:18:42.343186 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:18:42.343196 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:18:42.343206 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:18:42.343214 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:18:42.343219 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:18:42.343228 3659862976 net.cpp:122] Setting up pool1
I0513 11:18:42.343232 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:18:42.343236 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:18:42.343240 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:18:42.343245 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:18:42.343250 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:18:42.343253 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:18:42.343482 3659862976 net.cpp:122] Setting up conv2
I0513 11:18:42.343488 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:18:42.343503 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:18:42.343509 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:18:42.343513 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:18:42.343518 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:18:42.343521 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:18:42.343526 3659862976 net.cpp:122] Setting up pool2
I0513 11:18:42.343530 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:18:42.343534 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:18:42.343538 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:18:42.343564 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:18:42.343569 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:18:42.343575 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:18:42.346873 3659862976 net.cpp:122] Setting up ip1
I0513 11:18:42.346884 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346889 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:18:42.346895 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:18:42.346901 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:18:42.346905 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:18:42.346909 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:18:42.346915 3659862976 net.cpp:122] Setting up relu1
I0513 11:18:42.346917 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:18:42.346921 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:18:42.346925 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:18:42.346931 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:18:42.346935 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:18:42.346938 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:18:42.346987 3659862976 net.cpp:122] Setting up ip2
I0513 11:18:42.346992 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.346997 3659862976 net.cpp:137] Memory required for data: 8078800
//注意这里，ip2_ip2_0_split在网络描述中没有显示给出，是caffe解析后自动加上的
I0513 11:18:42.347002 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:18:42.347007 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
//ip2_ip2_0_split接受一个输入ip2，产生两个输出ip2_ip2_0_split_0和ip2_ip2_0_split_1，是复制关系
I0513 11:18:42.347010 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:18:42.347014 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:18:42.347019 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:18:42.347024 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:18:42.347028 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347033 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:18:42.347036 3659862976 net.cpp:137] Memory required for data: 8086800
//ip2_ip2_0_split_0给了accuracy层
I0513 11:18:42.347039 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:18:42.347069 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:18:42.347074 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:18:42.347077 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:18:42.347082 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:18:42.347088 3659862976 net.cpp:122] Setting up accuracy
//accuracy层输出尺寸为1，即分类准确率
I0513 11:18:42.347091 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347095 3659862976 net.cpp:137] Memory required for data: 8086804
//ip2_ip2_0_split_1给了loss层
I0513 11:18:42.347100 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347103 3659862976 net.cpp:84] Creating Layer loss
I0513 11:18:42.347107 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:18:42.347111 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:18:42.347115 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:18:42.347121 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:18:42.347131 3659862976 net.cpp:122] Setting up loss
I0513 11:18:42.347133 3659862976 net.cpp:129] Top shape: (1)
I0513 11:18:42.347137 3659862976 net.cpp:132]     with loss weight 1
I0513 11:18:42.347143 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:18:42.347147 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:18:42.347151 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:18:42.347156 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:18:42.347159 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:18:42.347162 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:18:42.347167 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:18:42.347169 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:18:42.347173 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:18:42.347177 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:18:42.347180 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:18:42.347184 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:18:42.347189 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:18:42.347193 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:18:42.347196 3659862976 net.cpp:242] This network produces output loss
//第二座楼盖好了
I0513 11:18:42.347203 3659862976 net.cpp:255] Network initialization done.
//装修方案确定了
I0513 11:18:42.347247 3659862976 solver.cpp:56] Solver scaffolding done.
//开始装修
I0513 11:18:42.347271 3659862976 caffe.cpp:248] Starting Optimization
I0513 11:18:42.347275 3659862976 solver.cpp:272] Solving LeNet
I0513 11:18:42.347278 3659862976 solver.cpp:273] Learning Rate Policy: inv
//先测试一次，得到出事分类准确率和损失
I0513 11:18:42.348048 3659862976 solver.cpp:330] Iteration 0, Testing net (#0)
I0513 11:18:44.611253 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:18:44.703907 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.077
I0513 11:18:44.703938 3659862976 solver.cpp:397]     Test net output #1: loss = 2.41516 (* 1 = 2.41516 loss)
//现在分类效果肯定很差，准确率只有0.077，损失值约为2.3
I0513 11:18:44.741230 3659862976 solver.cpp:218] Iteration 0 (0 iter/s, 2.393s/100 iters), loss = 2.42047
//0次迭代后，依旧很差，训练网络没有accuracy输出，只有loss输出
I0513 11:18:44.741261 3659862976 solver.cpp:237]     Train net output #0: loss = 2.42047 (* 1 = 2.42047 loss)
I0513 11:18:44.741287 3659862976 sgd_solver.cpp:105] Iteration 0, lr = 0.01
//迭代100次之后，效果就出来了，loss已经降到0.21（之前是2.42）
I0513 11:18:47.874459 3659862976 solver.cpp:218] Iteration 100 (31.9183 iter/s, 3.133s/100 iters), loss = 0.215375
I0513 11:18:47.874493 3659862976 solver.cpp:237]     Train net output #0: loss = 0.215375 (* 1 = 0.215375 loss)
I0513 11:18:47.874500 3659862976 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0513 11:18:50.998973 3659862976 solver.cpp:218] Iteration 200 (32.0102 iter/s, 3.124s/100 iters), loss = 0.144389
I0513 11:18:50.999003 3659862976 solver.cpp:237]     Train net output #0: loss = 0.144389 (* 1 = 0.144389 loss)
I0513 11:18:50.999011 3659862976 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0513 11:18:54.100409 3659862976 solver.cpp:218] Iteration 300 (32.2477 iter/s, 3.101s/100 iters), loss = 0.192488
I0513 11:18:54.100476 3659862976 solver.cpp:237]     Train net output #0: loss = 0.192488 (* 1 = 0.192488 loss)
I0513 11:18:54.100483 3659862976 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0513 11:18:57.210686 3659862976 solver.cpp:218] Iteration 400 (32.1543 iter/s, 3.11s/100 iters), loss = 0.0663644
I0513 11:18:57.210728 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0663644 (* 1 = 0.0663644 loss)
I0513 11:18:57.210737 3659862976 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
//迭代500次之后，进行一次测试。
I0513 11:19:00.279249 3659862976 solver.cpp:330] Iteration 500, Testing net (#0)
I0513 11:19:02.608597 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//发现准确度accuracy已经显著提升到0.9744了，loss为0.08
I0513 11:19:02.703658 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9744
I0513 11:19:02.703694 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0836155 (* 1 = 0.0836155 loss)
I0513 11:19:02.735476 3659862976 solver.cpp:218] Iteration 500 (18.1028 iter/s, 5.524s/100 iters), loss = 0.0916289
I0513 11:19:02.735512 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0916288 (* 1 = 0.0916288 loss)
I0513 11:19:02.735520 3659862976 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0513 11:19:05.931562 3659862976 solver.cpp:218] Iteration 600 (31.2891 iter/s, 3.196s/100 iters), loss = 0.0844364
I0513 11:19:05.931597 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0844363 (* 1 = 0.0844363 loss)
I0513 11:19:05.931604 3659862976 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0513 11:19:09.116649 3659862976 solver.cpp:218] Iteration 700 (31.3972 iter/s, 3.185s/100 iters), loss = 0.134004
I0513 11:19:09.116684 3659862976 solver.cpp:237]     Train net output #0: loss = 0.134004 (* 1 = 0.134004 loss)
I0513 11:19:09.116691 3659862976 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
//中间是训练过程。。。。。。
I0513 11:22:17.536756 3659862976 solver.cpp:218] Iteration 4800 (19.3311 iter/s, 5.173s/100 iters), loss = 0.0179583
I0513 11:22:17.536806 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0179581 (* 1 = 0.0179581 loss)
I0513 11:22:17.536818 3659862976 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0513 11:22:22.731861 3659862976 solver.cpp:218] Iteration 4900 (19.2493 iter/s, 5.195s/100 iters), loss = 0.00556874
I0513 11:22:22.731927 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00556857 (* 1 = 0.00556857 loss)
I0513 11:22:22.731940 3659862976 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
//每迭代到5000次之后，打印一次快照，保存lenet_iter_5000.caffemodel和lenet_iter_5000.solverstate
I0513 11:22:28.143353 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0513 11:22:28.167670 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0513 11:22:28.171842 3659862976 solver.cpp:330] Iteration 5000, Testing net (#0)
I0513 11:22:32.514833 57593856 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:22:32.699314 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.9888
I0513 11:22:32.699359 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0334435 (* 1 = 0.0334435 loss)
I0513 11:22:32.754936 3659862976 solver.cpp:218] Iteration 5000 (9.97705 iter/s, 10.023s/100 iters), loss = 0.0241056
I0513 11:22:32.754987 3659862976 solver.cpp:237]     Train net output #0: loss = 0.0241055 (* 1 = 0.0241055 loss)
I0513 11:22:32.754999 3659862976 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
//中间继续训练。。。。。
I0513 11:26:53.808578 3659862976 solver.cpp:218] Iteration 9900 (21.097 iter/s, 4.74s/100 iters), loss = 0.00466773
I0513 11:26:53.808624 3659862976 solver.cpp:237]     Train net output #0: loss = 0.00466757 (* 1 = 0.00466757 loss)
I0513 11:26:53.808635 3659862976 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
//最后一次打印快照
I0513 11:26:58.671659 3659862976 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0513 11:26:58.688323 3659862976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0513 11:26:58.715297 3659862976 solver.cpp:310] Iteration 10000, loss = 0.00293942
I0513 11:26:58.715337 3659862976 solver.cpp:330] Iteration 10000, Testing net (#0)
I0513 11:27:02.099313 57593856 data_layer.cpp:73] Restarting data prefetching from start.
//最终分类准确率为99%
I0513 11:27:02.230465 3659862976 solver.cpp:397]     Test net output #0: accuracy = 0.991
//最终loss值为0.03
I0513 11:27:02.230509 3659862976 solver.cpp:397]     Test net output #1: loss = 0.0304018 (* 1 = 0.0304018 loss)
I0513 11:27:02.230518 3659862976 solver.cpp:315] Optimization Done.
I0513 11:27:02.230525 3659862976 caffe.cpp:259] Optimization Done.
//装修结束
</code></pre>

<h2 id="toc_3">用训练好的模型对数据进行预测</h2>

<p>从上面的输出结果可以看到最终训练的模型权值存在lenet_iter_10000.caffemodal中，之后可以对测试数据集进行预测。运行如下命令就可以了：</p>

<pre><code>➜  caffe git:(master) ✗ ./build/tools/caffe.bin test \
-model examples/mnist/lenet_train_test.prototxt \
-weights examples/mnist/lenet_iter_10000.caffemodel \
    -iterations 100
</code></pre>

<p>上述命令解释：<br/>
./build/tools/caffe.bin test，表示只做预测（前向传播急速那），不进行参数更新（BP反向传播计算）</p>

<p>-model examples/mnist/lenet_train_test.prototxt ，指定模型描述文本文件</p>

<p>-weights examples/mnist/lenet_iter_10000.caffemodel ，指定模型预先训练好的权值文件<br/>
-iterations 100 ， 指定测试迭代次数。参与测试的样例数目为（iterations*batch_size）,batch_size在model prototxt中设定，为100时刚好覆盖全部10000个测试样本。</p>

<p>我们运行上述命令得到：</p>

<pre><code>I0513 11:37:08.827889 3659862976 caffe.cpp:284] Use CPU.
I0513 11:37:08.830747 3659862976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0513 11:37:08.830780 3659862976 net.cpp:51] Initializing net from parameters:
name: &quot;LeNet&quot;
state {
  phase: TEST
  level: 0
  stage: &quot;&quot;
}
layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;examples/mnist/mnist_test_lmdb&quot;
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;accuracy&quot;
  include {
    phase: TEST
  }
}
layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
  top: &quot;loss&quot;
}
I0513 11:37:08.831130 3659862976 layer_factory.hpp:77] Creating layer mnist
I0513 11:37:08.831360 3659862976 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0513 11:37:08.831418 3659862976 net.cpp:84] Creating Layer mnist
I0513 11:37:08.831425 3659862976 net.cpp:380] mnist -&gt; data
I0513 11:37:08.831444 3659862976 net.cpp:380] mnist -&gt; label
I0513 11:37:08.831480 3659862976 data_layer.cpp:45] output data size: 100,1,28,28
I0513 11:37:08.836457 3659862976 net.cpp:122] Setting up mnist
I0513 11:37:08.836468 3659862976 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0513 11:37:08.836488 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836491 3659862976 net.cpp:137] Memory required for data: 314000
I0513 11:37:08.836498 3659862976 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0513 11:37:08.836505 3659862976 net.cpp:84] Creating Layer label_mnist_1_split
I0513 11:37:08.836509 3659862976 net.cpp:406] label_mnist_1_split &lt;- label
I0513 11:37:08.836513 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_0
I0513 11:37:08.836519 3659862976 net.cpp:380] label_mnist_1_split -&gt; label_mnist_1_split_1
I0513 11:37:08.836525 3659862976 net.cpp:122] Setting up label_mnist_1_split
I0513 11:37:08.836529 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836534 3659862976 net.cpp:129] Top shape: 100 (100)
I0513 11:37:08.836539 3659862976 net.cpp:137] Memory required for data: 314800
I0513 11:37:08.836542 3659862976 layer_factory.hpp:77] Creating layer conv1
I0513 11:37:08.836550 3659862976 net.cpp:84] Creating Layer conv1
I0513 11:37:08.836555 3659862976 net.cpp:406] conv1 &lt;- data
I0513 11:37:08.836558 3659862976 net.cpp:380] conv1 -&gt; conv1
I0513 11:37:08.836611 3659862976 net.cpp:122] Setting up conv1
I0513 11:37:08.836616 3659862976 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0513 11:37:08.836639 3659862976 net.cpp:137] Memory required for data: 4922800
I0513 11:37:08.836648 3659862976 layer_factory.hpp:77] Creating layer pool1
I0513 11:37:08.836653 3659862976 net.cpp:84] Creating Layer pool1
I0513 11:37:08.836658 3659862976 net.cpp:406] pool1 &lt;- conv1
I0513 11:37:08.836661 3659862976 net.cpp:380] pool1 -&gt; pool1
I0513 11:37:08.836671 3659862976 net.cpp:122] Setting up pool1
I0513 11:37:08.836675 3659862976 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0513 11:37:08.836680 3659862976 net.cpp:137] Memory required for data: 6074800
I0513 11:37:08.836683 3659862976 layer_factory.hpp:77] Creating layer conv2
I0513 11:37:08.836691 3659862976 net.cpp:84] Creating Layer conv2
I0513 11:37:08.836695 3659862976 net.cpp:406] conv2 &lt;- pool1
I0513 11:37:08.836700 3659862976 net.cpp:380] conv2 -&gt; conv2
I0513 11:37:08.836917 3659862976 net.cpp:122] Setting up conv2
I0513 11:37:08.836923 3659862976 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0513 11:37:08.836971 3659862976 net.cpp:137] Memory required for data: 7354800
I0513 11:37:08.837033 3659862976 layer_factory.hpp:77] Creating layer pool2
I0513 11:37:08.837041 3659862976 net.cpp:84] Creating Layer pool2
I0513 11:37:08.837045 3659862976 net.cpp:406] pool2 &lt;- conv2
I0513 11:37:08.837049 3659862976 net.cpp:380] pool2 -&gt; pool2
I0513 11:37:08.837059 3659862976 net.cpp:122] Setting up pool2
I0513 11:37:08.837062 3659862976 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0513 11:37:08.837067 3659862976 net.cpp:137] Memory required for data: 7674800
I0513 11:37:08.837070 3659862976 layer_factory.hpp:77] Creating layer ip1
I0513 11:37:08.837076 3659862976 net.cpp:84] Creating Layer ip1
I0513 11:37:08.837080 3659862976 net.cpp:406] ip1 &lt;- pool2
I0513 11:37:08.837085 3659862976 net.cpp:380] ip1 -&gt; ip1
I0513 11:37:08.840445 3659862976 net.cpp:122] Setting up ip1
I0513 11:37:08.840461 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840467 3659862976 net.cpp:137] Memory required for data: 7874800
I0513 11:37:08.840476 3659862976 layer_factory.hpp:77] Creating layer relu1
I0513 11:37:08.840487 3659862976 net.cpp:84] Creating Layer relu1
I0513 11:37:08.840492 3659862976 net.cpp:406] relu1 &lt;- ip1
I0513 11:37:08.840497 3659862976 net.cpp:367] relu1 -&gt; ip1 (in-place)
I0513 11:37:08.840504 3659862976 net.cpp:122] Setting up relu1
I0513 11:37:08.840507 3659862976 net.cpp:129] Top shape: 100 500 (50000)
I0513 11:37:08.840512 3659862976 net.cpp:137] Memory required for data: 8074800
I0513 11:37:08.840517 3659862976 layer_factory.hpp:77] Creating layer ip2
I0513 11:37:08.840523 3659862976 net.cpp:84] Creating Layer ip2
I0513 11:37:08.840528 3659862976 net.cpp:406] ip2 &lt;- ip1
I0513 11:37:08.840533 3659862976 net.cpp:380] ip2 -&gt; ip2
I0513 11:37:08.840591 3659862976 net.cpp:122] Setting up ip2
I0513 11:37:08.840597 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840601 3659862976 net.cpp:137] Memory required for data: 8078800
I0513 11:37:08.840606 3659862976 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0513 11:37:08.840612 3659862976 net.cpp:84] Creating Layer ip2_ip2_0_split
I0513 11:37:08.840616 3659862976 net.cpp:406] ip2_ip2_0_split &lt;- ip2
I0513 11:37:08.840623 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_0
I0513 11:37:08.840631 3659862976 net.cpp:380] ip2_ip2_0_split -&gt; ip2_ip2_0_split_1
I0513 11:37:08.840637 3659862976 net.cpp:122] Setting up ip2_ip2_0_split
I0513 11:37:08.840641 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840646 3659862976 net.cpp:129] Top shape: 100 10 (1000)
I0513 11:37:08.840649 3659862976 net.cpp:137] Memory required for data: 8086800
I0513 11:37:08.840653 3659862976 layer_factory.hpp:77] Creating layer accuracy
I0513 11:37:08.840659 3659862976 net.cpp:84] Creating Layer accuracy
I0513 11:37:08.840663 3659862976 net.cpp:406] accuracy &lt;- ip2_ip2_0_split_0
I0513 11:37:08.840668 3659862976 net.cpp:406] accuracy &lt;- label_mnist_1_split_0
I0513 11:37:08.840672 3659862976 net.cpp:380] accuracy -&gt; accuracy
I0513 11:37:08.840678 3659862976 net.cpp:122] Setting up accuracy
I0513 11:37:08.840708 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840714 3659862976 net.cpp:137] Memory required for data: 8086804
I0513 11:37:08.840718 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840724 3659862976 net.cpp:84] Creating Layer loss
I0513 11:37:08.840728 3659862976 net.cpp:406] loss &lt;- ip2_ip2_0_split_1
I0513 11:37:08.840733 3659862976 net.cpp:406] loss &lt;- label_mnist_1_split_1
I0513 11:37:08.840737 3659862976 net.cpp:380] loss -&gt; loss
I0513 11:37:08.840746 3659862976 layer_factory.hpp:77] Creating layer loss
I0513 11:37:08.840759 3659862976 net.cpp:122] Setting up loss
I0513 11:37:08.840762 3659862976 net.cpp:129] Top shape: (1)
I0513 11:37:08.840767 3659862976 net.cpp:132]     with loss weight 1
I0513 11:37:08.840776 3659862976 net.cpp:137] Memory required for data: 8086808
I0513 11:37:08.840780 3659862976 net.cpp:198] loss needs backward computation.
I0513 11:37:08.840785 3659862976 net.cpp:200] accuracy does not need backward computation.
I0513 11:37:08.840790 3659862976 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0513 11:37:08.840793 3659862976 net.cpp:198] ip2 needs backward computation.
I0513 11:37:08.840798 3659862976 net.cpp:198] relu1 needs backward computation.
I0513 11:37:08.840802 3659862976 net.cpp:198] ip1 needs backward computation.
I0513 11:37:08.840806 3659862976 net.cpp:198] pool2 needs backward computation.
I0513 11:37:08.840811 3659862976 net.cpp:198] conv2 needs backward computation.
I0513 11:37:08.840814 3659862976 net.cpp:198] pool1 needs backward computation.
I0513 11:37:08.840818 3659862976 net.cpp:198] conv1 needs backward computation.
I0513 11:37:08.840822 3659862976 net.cpp:200] label_mnist_1_split does not need backward computation.
I0513 11:37:08.840827 3659862976 net.cpp:200] mnist does not need backward computation.
I0513 11:37:08.840831 3659862976 net.cpp:242] This network produces output accuracy
I0513 11:37:08.840836 3659862976 net.cpp:242] This network produces output loss
I0513 11:37:08.840843 3659862976 net.cpp:255] Network initialization done.
I0513 11:37:08.843325 3659862976 caffe.cpp:290] Running for 100 iterations.
I0513 11:37:08.871536 3659862976 caffe.cpp:313] Batch 0, accuracy = 1
I0513 11:37:08.871567 3659862976 caffe.cpp:313] Batch 0, loss = 0.0085843
I0513 11:37:08.894382 3659862976 caffe.cpp:313] Batch 1, accuracy = 1
I0513 11:37:08.894414 3659862976 caffe.cpp:313] Batch 1, loss = 0.00573037
I0513 11:37:08.918002 3659862976 caffe.cpp:313] Batch 2, accuracy = 0.99
I0513 11:37:08.918031 3659862976 caffe.cpp:313] Batch 2, loss = 0.0333053
I0513 11:37:08.943091 3659862976 caffe.cpp:313] Batch 3, accuracy = 0.99
I0513 11:37:08.943127 3659862976 caffe.cpp:313] Batch 3, loss = 0.0271862
I0513 11:37:08.967147 3659862976 caffe.cpp:313] Batch 4, accuracy = 0.99
I0513 11:37:08.967177 3659862976 caffe.cpp:313] Batch 4, loss = 0.0571239
I0513 11:37:08.989929 3659862976 caffe.cpp:313] Batch 5, accuracy = 0.99
I0513 11:37:08.989961 3659862976 caffe.cpp:313] Batch 5, loss = 0.0569953
I0513 11:37:09.015426 3659862976 caffe.cpp:313] Batch 6, accuracy = 0.98
I0513 11:37:09.015463 3659862976 caffe.cpp:313] Batch 6, loss = 0.0698283
I0513 11:37:09.039398 3659862976 caffe.cpp:313] Batch 7, accuracy = 0.99
I0513 11:37:09.039432 3659862976 caffe.cpp:313] Batch 7, loss = 0.0349087
I0513 11:37:09.063937 3659862976 caffe.cpp:313] Batch 8, accuracy = 1
I0513 11:37:09.063967 3659862976 caffe.cpp:313] Batch 8, loss = 0.0115442
I0513 11:37:09.086630 3659862976 caffe.cpp:313] Batch 9, accuracy = 0.99
I0513 11:37:09.086663 3659862976 caffe.cpp:313] Batch 9, loss = 0.0361095
I0513 11:37:09.111706 3659862976 caffe.cpp:313] Batch 10, accuracy = 0.98
I0513 11:37:09.111735 3659862976 caffe.cpp:313] Batch 10, loss = 0.0702643
I0513 11:37:09.135445 3659862976 caffe.cpp:313] Batch 11, accuracy = 0.97
I0513 11:37:09.135478 3659862976 caffe.cpp:313] Batch 11, loss = 0.0508112
I0513 11:37:09.159065 3659862976 caffe.cpp:313] Batch 12, accuracy = 0.95
I0513 11:37:09.159097 3659862976 caffe.cpp:313] Batch 12, loss = 0.148118
I0513 11:37:09.181542 3659862976 caffe.cpp:313] Batch 13, accuracy = 0.98
I0513 11:37:09.181607 3659862976 caffe.cpp:313] Batch 13, loss = 0.036772
I0513 11:37:09.205440 3659862976 caffe.cpp:313] Batch 14, accuracy = 1
I0513 11:37:09.205476 3659862976 caffe.cpp:313] Batch 14, loss = 0.00694412
I0513 11:37:09.228198 3659862976 caffe.cpp:313] Batch 15, accuracy = 0.99
I0513 11:37:09.228229 3659862976 caffe.cpp:313] Batch 15, loss = 0.0389514
I0513 11:37:09.251550 3659862976 caffe.cpp:313] Batch 16, accuracy = 0.98
I0513 11:37:09.251581 3659862976 caffe.cpp:313] Batch 16, loss = 0.0298825
I0513 11:37:09.275153 3659862976 caffe.cpp:313] Batch 17, accuracy = 1
I0513 11:37:09.275182 3659862976 caffe.cpp:313] Batch 17, loss = 0.0170967
I0513 11:37:09.298004 3659862976 caffe.cpp:313] Batch 18, accuracy = 0.99
I0513 11:37:09.298035 3659862976 caffe.cpp:313] Batch 18, loss = 0.0189575
I0513 11:37:09.321348 3659862976 caffe.cpp:313] Batch 19, accuracy = 0.99
I0513 11:37:09.321379 3659862976 caffe.cpp:313] Batch 19, loss = 0.0455956
I0513 11:37:09.344025 3659862976 caffe.cpp:313] Batch 20, accuracy = 0.98
I0513 11:37:09.344058 3659862976 caffe.cpp:313] Batch 20, loss = 0.108723
I0513 11:37:09.368069 3659862976 caffe.cpp:313] Batch 21, accuracy = 0.98
I0513 11:37:09.368101 3659862976 caffe.cpp:313] Batch 21, loss = 0.0780955
I0513 11:37:09.390791 3659862976 caffe.cpp:313] Batch 22, accuracy = 0.99
I0513 11:37:09.390823 3659862976 caffe.cpp:313] Batch 22, loss = 0.0368689
I0513 11:37:09.414577 3659862976 caffe.cpp:313] Batch 23, accuracy = 0.97
I0513 11:37:09.414621 3659862976 caffe.cpp:313] Batch 23, loss = 0.0296016
I0513 11:37:09.437597 3659862976 caffe.cpp:313] Batch 24, accuracy = 0.97
I0513 11:37:09.437628 3659862976 caffe.cpp:313] Batch 24, loss = 0.0589915
I0513 11:37:09.460636 3659862976 caffe.cpp:313] Batch 25, accuracy = 0.99
I0513 11:37:09.460669 3659862976 caffe.cpp:313] Batch 25, loss = 0.0754509
I0513 11:37:09.483229 3659862976 caffe.cpp:313] Batch 26, accuracy = 0.99
I0513 11:37:09.483261 3659862976 caffe.cpp:313] Batch 26, loss = 0.118656
I0513 11:37:09.508059 3659862976 caffe.cpp:313] Batch 27, accuracy = 0.98
I0513 11:37:09.508092 3659862976 caffe.cpp:313] Batch 27, loss = 0.0222734
I0513 11:37:09.530911 3659862976 caffe.cpp:313] Batch 28, accuracy = 0.99
I0513 11:37:09.530943 3659862976 caffe.cpp:313] Batch 28, loss = 0.0315118
I0513 11:37:09.555687 3659862976 caffe.cpp:313] Batch 29, accuracy = 0.97
I0513 11:37:09.555721 3659862976 caffe.cpp:313] Batch 29, loss = 0.129427
I0513 11:37:09.579476 3659862976 caffe.cpp:313] Batch 30, accuracy = 1
I0513 11:37:09.579507 3659862976 caffe.cpp:313] Batch 30, loss = 0.0196561
I0513 11:37:09.602957 3659862976 caffe.cpp:313] Batch 31, accuracy = 1
I0513 11:37:09.602993 3659862976 caffe.cpp:313] Batch 31, loss = 0.00242798
I0513 11:37:09.626893 3659862976 caffe.cpp:313] Batch 32, accuracy = 0.99
I0513 11:37:09.626924 3659862976 caffe.cpp:313] Batch 32, loss = 0.0169622
I0513 11:37:09.650236 3659862976 caffe.cpp:313] Batch 33, accuracy = 1
I0513 11:37:09.650270 3659862976 caffe.cpp:313] Batch 33, loss = 0.00425847
I0513 11:37:09.673212 3659862976 caffe.cpp:313] Batch 34, accuracy = 0.99
I0513 11:37:09.673243 3659862976 caffe.cpp:313] Batch 34, loss = 0.0726783
I0513 11:37:09.696039 3659862976 caffe.cpp:313] Batch 35, accuracy = 0.95
I0513 11:37:09.696071 3659862976 caffe.cpp:313] Batch 35, loss = 0.173234
I0513 11:37:09.719209 3659862976 caffe.cpp:313] Batch 36, accuracy = 1
I0513 11:37:09.719241 3659862976 caffe.cpp:313] Batch 36, loss = 0.0126433
I0513 11:37:09.741852 3659862976 caffe.cpp:313] Batch 37, accuracy = 0.99
I0513 11:37:09.741884 3659862976 caffe.cpp:313] Batch 37, loss = 0.0380185
I0513 11:37:09.766039 3659862976 caffe.cpp:313] Batch 38, accuracy = 1
I0513 11:37:09.766072 3659862976 caffe.cpp:313] Batch 38, loss = 0.0161337
I0513 11:37:09.788811 3659862976 caffe.cpp:313] Batch 39, accuracy = 0.98
I0513 11:37:09.788844 3659862976 caffe.cpp:313] Batch 39, loss = 0.0317039
I0513 11:37:09.812556 3659862976 caffe.cpp:313] Batch 40, accuracy = 1
I0513 11:37:09.812587 3659862976 caffe.cpp:313] Batch 40, loss = 0.0283054
I0513 11:37:09.835418 3659862976 caffe.cpp:313] Batch 41, accuracy = 0.98
I0513 11:37:09.835450 3659862976 caffe.cpp:313] Batch 41, loss = 0.0595546
I0513 11:37:09.858765 3659862976 caffe.cpp:313] Batch 42, accuracy = 0.98
I0513 11:37:09.858793 3659862976 caffe.cpp:313] Batch 42, loss = 0.033258
I0513 11:37:09.881479 3659862976 caffe.cpp:313] Batch 43, accuracy = 1
I0513 11:37:09.881510 3659862976 caffe.cpp:313] Batch 43, loss = 0.00560485
I0513 11:37:09.906558 3659862976 caffe.cpp:313] Batch 44, accuracy = 1
I0513 11:37:09.906590 3659862976 caffe.cpp:313] Batch 44, loss = 0.0164246
I0513 11:37:09.932261 3659862976 caffe.cpp:313] Batch 45, accuracy = 0.99
I0513 11:37:09.932294 3659862976 caffe.cpp:313] Batch 45, loss = 0.047733
I0513 11:37:09.957159 3659862976 caffe.cpp:313] Batch 46, accuracy = 1
I0513 11:37:09.957190 3659862976 caffe.cpp:313] Batch 46, loss = 0.00406718
I0513 11:37:09.979852 3659862976 caffe.cpp:313] Batch 47, accuracy = 0.99
I0513 11:37:09.979883 3659862976 caffe.cpp:313] Batch 47, loss = 0.0176224
I0513 11:37:10.003631 3659862976 caffe.cpp:313] Batch 48, accuracy = 0.95
I0513 11:37:10.003666 3659862976 caffe.cpp:313] Batch 48, loss = 0.0918992
I0513 11:37:10.027333 3659862976 caffe.cpp:313] Batch 49, accuracy = 1
I0513 11:37:10.027365 3659862976 caffe.cpp:313] Batch 49, loss = 0.00535747
I0513 11:37:10.050904 3659862976 caffe.cpp:313] Batch 50, accuracy = 1
I0513 11:37:10.050935 3659862976 caffe.cpp:313] Batch 50, loss = 0.000293352
I0513 11:37:10.076280 3659862976 caffe.cpp:313] Batch 51, accuracy = 1
I0513 11:37:10.076314 3659862976 caffe.cpp:313] Batch 51, loss = 0.00675426
I0513 11:37:10.099964 3659862976 caffe.cpp:313] Batch 52, accuracy = 1
I0513 11:37:10.099993 3659862976 caffe.cpp:313] Batch 52, loss = 0.0113504
I0513 11:37:10.123363 3659862976 caffe.cpp:313] Batch 53, accuracy = 1
I0513 11:37:10.123394 3659862976 caffe.cpp:313] Batch 53, loss = 0.00080642
I0513 11:37:10.146338 3659862976 caffe.cpp:313] Batch 54, accuracy = 1
I0513 11:37:10.146368 3659862976 caffe.cpp:313] Batch 54, loss = 0.0119724
I0513 11:37:10.170075 3659862976 caffe.cpp:313] Batch 55, accuracy = 1
I0513 11:37:10.170106 3659862976 caffe.cpp:313] Batch 55, loss = 9.95353e-05
I0513 11:37:10.192754 3659862976 caffe.cpp:313] Batch 56, accuracy = 1
I0513 11:37:10.192785 3659862976 caffe.cpp:313] Batch 56, loss = 0.00792123
I0513 11:37:10.215930 3659862976 caffe.cpp:313] Batch 57, accuracy = 1
I0513 11:37:10.215963 3659862976 caffe.cpp:313] Batch 57, loss = 0.0106224
I0513 11:37:10.238731 3659862976 caffe.cpp:313] Batch 58, accuracy = 1
I0513 11:37:10.238765 3659862976 caffe.cpp:313] Batch 58, loss = 0.00865888
I0513 11:37:10.261700 3659862976 caffe.cpp:313] Batch 59, accuracy = 0.98
I0513 11:37:10.261731 3659862976 caffe.cpp:313] Batch 59, loss = 0.0758659
I0513 11:37:10.284554 3659862976 caffe.cpp:313] Batch 60, accuracy = 1
I0513 11:37:10.284585 3659862976 caffe.cpp:313] Batch 60, loss = 0.00406362
I0513 11:37:10.310072 3659862976 caffe.cpp:313] Batch 61, accuracy = 1
I0513 11:37:10.310102 3659862976 caffe.cpp:313] Batch 61, loss = 0.00472714
I0513 11:37:10.332813 3659862976 caffe.cpp:313] Batch 62, accuracy = 1
I0513 11:37:10.332845 3659862976 caffe.cpp:313] Batch 62, loss = 0.00013836
I0513 11:37:10.356101 3659862976 caffe.cpp:313] Batch 63, accuracy = 1
I0513 11:37:10.356132 3659862976 caffe.cpp:313] Batch 63, loss = 0.000318341
I0513 11:37:10.378556 3659862976 caffe.cpp:313] Batch 64, accuracy = 1
I0513 11:37:10.378587 3659862976 caffe.cpp:313] Batch 64, loss = 0.000235923
I0513 11:37:10.402688 3659862976 caffe.cpp:313] Batch 65, accuracy = 0.94
I0513 11:37:10.402724 3659862976 caffe.cpp:313] Batch 65, loss = 0.174556
I0513 11:37:10.426704 3659862976 caffe.cpp:313] Batch 66, accuracy = 0.98
I0513 11:37:10.426736 3659862976 caffe.cpp:313] Batch 66, loss = 0.0710799
I0513 11:37:10.450608 3659862976 caffe.cpp:313] Batch 67, accuracy = 0.99
I0513 11:37:10.450641 3659862976 caffe.cpp:313] Batch 67, loss = 0.0471492
I0513 11:37:10.474786 3659862976 caffe.cpp:313] Batch 68, accuracy = 1
I0513 11:37:10.474853 3659862976 caffe.cpp:313] Batch 68, loss = 0.00714237
I0513 11:37:10.497565 3659862976 caffe.cpp:313] Batch 69, accuracy = 1
I0513 11:37:10.497596 3659862976 caffe.cpp:313] Batch 69, loss = 0.00141993
I0513 11:37:10.520592 3659862976 caffe.cpp:313] Batch 70, accuracy = 1
I0513 11:37:10.520623 3659862976 caffe.cpp:313] Batch 70, loss = 0.00206052
I0513 11:37:10.543385 3659862976 caffe.cpp:313] Batch 71, accuracy = 1
I0513 11:37:10.543418 3659862976 caffe.cpp:313] Batch 71, loss = 0.000801532
I0513 11:37:10.567934 3659862976 caffe.cpp:313] Batch 72, accuracy = 0.99
I0513 11:37:10.567965 3659862976 caffe.cpp:313] Batch 72, loss = 0.0175235
I0513 11:37:10.591750 3659862976 caffe.cpp:313] Batch 73, accuracy = 1
I0513 11:37:10.591784 3659862976 caffe.cpp:313] Batch 73, loss = 0.000181734
I0513 11:37:10.617092 3659862976 caffe.cpp:313] Batch 74, accuracy = 1
I0513 11:37:10.617122 3659862976 caffe.cpp:313] Batch 74, loss = 0.00376508
I0513 11:37:10.639822 3659862976 caffe.cpp:313] Batch 75, accuracy = 1
I0513 11:37:10.639853 3659862976 caffe.cpp:313] Batch 75, loss = 0.00211647
I0513 11:37:10.664058 3659862976 caffe.cpp:313] Batch 76, accuracy = 1
I0513 11:37:10.664090 3659862976 caffe.cpp:313] Batch 76, loss = 0.000218412
I0513 11:37:10.686815 3659862976 caffe.cpp:313] Batch 77, accuracy = 1
I0513 11:37:10.686847 3659862976 caffe.cpp:313] Batch 77, loss = 0.000203503
I0513 11:37:10.710923 3659862976 caffe.cpp:313] Batch 78, accuracy = 1
I0513 11:37:10.710953 3659862976 caffe.cpp:313] Batch 78, loss = 0.0013391
I0513 11:37:10.733860 3659862976 caffe.cpp:313] Batch 79, accuracy = 1
I0513 11:37:10.733891 3659862976 caffe.cpp:313] Batch 79, loss = 0.00335708
I0513 11:37:10.758643 3659862976 caffe.cpp:313] Batch 80, accuracy = 0.99
I0513 11:37:10.758677 3659862976 caffe.cpp:313] Batch 80, loss = 0.0256179
I0513 11:37:10.781409 3659862976 caffe.cpp:313] Batch 81, accuracy = 1
I0513 11:37:10.781440 3659862976 caffe.cpp:313] Batch 81, loss = 0.0023732
I0513 11:37:10.805886 3659862976 caffe.cpp:313] Batch 82, accuracy = 0.99
I0513 11:37:10.805920 3659862976 caffe.cpp:313] Batch 82, loss = 0.0162458
I0513 11:37:10.828743 3659862976 caffe.cpp:313] Batch 83, accuracy = 1
I0513 11:37:10.828775 3659862976 caffe.cpp:313] Batch 83, loss = 0.00678432
I0513 11:37:10.852507 3659862976 caffe.cpp:313] Batch 84, accuracy = 0.99
I0513 11:37:10.852538 3659862976 caffe.cpp:313] Batch 84, loss = 0.0189542
I0513 11:37:10.875788 3659862976 caffe.cpp:313] Batch 85, accuracy = 0.99
I0513 11:37:10.875819 3659862976 caffe.cpp:313] Batch 85, loss = 0.0198986
I0513 11:37:10.899011 3659862976 caffe.cpp:313] Batch 86, accuracy = 1
I0513 11:37:10.899040 3659862976 caffe.cpp:313] Batch 86, loss = 0.000146087
I0513 11:37:10.921692 3659862976 caffe.cpp:313] Batch 87, accuracy = 1
I0513 11:37:10.921723 3659862976 caffe.cpp:313] Batch 87, loss = 0.000129989
I0513 11:37:10.944453 3659862976 caffe.cpp:313] Batch 88, accuracy = 1
I0513 11:37:10.944484 3659862976 caffe.cpp:313] Batch 88, loss = 4.1275e-05
I0513 11:37:10.968449 3659862976 caffe.cpp:313] Batch 89, accuracy = 1
I0513 11:37:10.968482 3659862976 caffe.cpp:313] Batch 89, loss = 4.4345e-05
I0513 11:37:10.994932 3659862976 caffe.cpp:313] Batch 90, accuracy = 0.97
I0513 11:37:10.994962 3659862976 caffe.cpp:313] Batch 90, loss = 0.0680957
I0513 11:37:11.018280 3659862976 caffe.cpp:313] Batch 91, accuracy = 1
I0513 11:37:11.018312 3659862976 caffe.cpp:313] Batch 91, loss = 2.29651e-05
I0513 11:37:11.044423 3659862976 caffe.cpp:313] Batch 92, accuracy = 1
I0513 11:37:11.044457 3659862976 caffe.cpp:313] Batch 92, loss = 0.000162702
I0513 11:37:11.068132 3659862976 caffe.cpp:313] Batch 93, accuracy = 1
I0513 11:37:11.068163 3659862976 caffe.cpp:313] Batch 93, loss = 0.000582345
I0513 11:37:11.090775 3659862976 caffe.cpp:313] Batch 94, accuracy = 1
I0513 11:37:11.090806 3659862976 caffe.cpp:313] Batch 94, loss = 0.000352066
I0513 11:37:11.115216 3659862976 caffe.cpp:313] Batch 95, accuracy = 1
I0513 11:37:11.115247 3659862976 caffe.cpp:313] Batch 95, loss = 0.00453322
I0513 11:37:11.115762 84811776 data_layer.cpp:73] Restarting data prefetching from start.
I0513 11:37:11.137984 3659862976 caffe.cpp:313] Batch 96, accuracy = 0.97
I0513 11:37:11.138017 3659862976 caffe.cpp:313] Batch 96, loss = 0.0792528
I0513 11:37:11.162164 3659862976 caffe.cpp:313] Batch 97, accuracy = 0.98
I0513 11:37:11.162194 3659862976 caffe.cpp:313] Batch 97, loss = 0.106678
I0513 11:37:11.184717 3659862976 caffe.cpp:313] Batch 98, accuracy = 1
I0513 11:37:11.184751 3659862976 caffe.cpp:313] Batch 98, loss = 0.0035934
I0513 11:37:11.208353 3659862976 caffe.cpp:313] Batch 99, accuracy = 0.99
I0513 11:37:11.208385 3659862976 caffe.cpp:313] Batch 99, loss = 0.0180797
I0513 11:37:11.208390 3659862976 caffe.cpp:318] Loss: 0.0304018
I0513 11:37:11.208411 3659862976 caffe.cpp:330] accuracy = 0.991
I0513 11:37:11.208425 3659862976 caffe.cpp:330] loss = 0.0304018 (* 1 = 0.0304018 loss)
</code></pre>

<p>最后accuracy为0.991，loss为0.03</p>

<h2 id="toc_4">总结</h2>

<p>通过上述内容,我们可以初步了解一个完整的深度学习系统最核心的两个方面:<strong>数据和模型</strong>.数据是带标签的图片集,分训练集和测试集;模型是描述CNN结构的有向无环图(DAG),表示对原始数据的处理方式.</p>

<p><font color=red>Caffe并不直接处理原始数据,由预处理程序将原始数据存储为<code>LMDB</code>格式,来保持较高的IO效率.模型通常用ProtoBuffer文本格式表述,训练结果保存为ProtoBuffer二进制文件或HDF5格式文件.</font><strong>深度学习的过程就是利用训练数据对模型进行训练,将数据中蕴藏的大量信息通过机器学习算法不断收集到模型中,利用训练好的模型对现实世界中相似数据进行特定处理(如分类,识别,检测,定位).</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[常用内建模块]]></title>
    <link href="https://lockxmonk.github.io/14945736764798.html"/>
    <updated>2017-05-12T15:21:16+08:00</updated>
    <id>https://lockxmonk.github.io/14945736764798.html</id>
    <content type="html"><![CDATA[
<p>Python中有很多內建模块可以直接使用，无需额外安装。</p>

<h2 id="toc_0">collection</h2>

<p>collections是Python内建的一个集合模块，提供了许多有用的集合类。</p>

<h3 id="toc_1">namedtuple</h3>

<p>我们知道<code>tuple</code>可以表示不变集合，例如，一个点的二维坐标就可以表示成：</p>

<pre><code>p = (1,2)
</code></pre>

<p>但是，看到<code>(1, 2)</code>，很难看出这个<code>tuple</code>是用来表示一个坐标的。</p>

<p>定义一个<code>class</code>又小题大做了，这时，<code>namedtuple</code>就派上了用场：</p>

<pre><code class="language-py">from collections import namedtuple
Point = namedtuple(&#39;Point&#39;,[&#39;x&#39;,&#39;y&#39;])
p = Point(1,2)
print p.x , p.y
</code></pre>

<p><img src="media/14945736764798/14945744048083.jpg" alt=""/><br/>
<code>namedtuple</code>是一个函数，它用来创建一个自定义的<code>tuple</code>对象，并且规定了<code>tuple</code>元素的个数，并可以用属性而不是索引来引用<code>tuple</code>的某个元素。</p>

<p>这样一来，我们用<code>namedtuple</code>可以很方便地定义一种数据类型，它具备<code>tuple</code>的不变性，又可以根据属性来引用，使用十分方便。</p>

<p>可以验证创建的<code>Point</code>对象是<code>tuple</code>的一种子类：</p>

<pre><code class="language-py">print p.x , p.y ,isinstance(p, tuple),isinstance(p, Point)
</code></pre>

<p><img src="media/14945736764798/14945746594923.jpg" alt=""/></p>

<p>类似的，如果要用坐标和半径表示一个圆，也可以用<code>namedtuple</code>定义：</p>

<pre><code class="language-py"># namedtuple(&#39;名称&#39;, [属性list]):
Circle = namedtuple(&#39;Circle&#39;, [&#39;x&#39;, &#39;y&#39;, &#39;r&#39;])
</code></pre>

<h3 id="toc_2">deque</h3>

<p>使用<code>list</code>存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为<code>list</code>是线性存储，数据量大的时候，插入和删除效率很低。</p>

<p><code>deque</code>是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：</p>

<pre><code class="language-py">from collections import deque
q = deque([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])
q.append(&#39;x&#39;)
q.appendleft(&#39;y&#39;)
print q
</code></pre>

<p><img src="media/14945736764798/14945755049453.jpg" alt=""/></p>

<p><code>deque</code>除了实现list的<code>append()</code>和<code>pop()</code>外，还支持<code>appendleft()</code>和<code>popleft()</code>，这样就可以非常高效地往头部添加或删除元素。</p>

<h3 id="toc_3">defaultdict</h3>

<p>使用<code>dict</code>时，如果引用的Key不存在，就会抛出<code>KeyError</code>。如果希望<code>key</code>不存在时，返回一个默认值，就可以用<code>defaultdict</code>：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

from collections import defaultdict
dd = defaultdict(lambda: &#39;N/A&#39;)
dd[&#39;key1&#39;] = &#39;abc&#39;
print dd[&#39;key1&#39;], dd[&#39;key2&#39;]    #key1存在，key2不存在

</code></pre>

<p><img src="media/14945736764798/14945759067274.jpg" alt=""/><br/>
注意默认值是调用函数返回的，而函数在创建<code>defaultdict</code>对象时传入。</p>

<p>除了在<code>Key</code>不存在时返回默认值，<code>defaultdict</code>的其他行为跟<code>dict</code>是完全一样的。</p>

<h3 id="toc_4">OrderedDict</h3>

<p>使用<code>dict</code>时，Key是无序的。在对<code>dict</code>做迭代时，我们无法确定<code>Key</code>的顺序。</p>

<p>如果要保持Key的顺序，可以用<code>OrderedDict</code>：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

from collections import OrderedDict
d = dict([(&#39;a&#39;,1),(&#39;b&#39;,2),(&#39;c&#39;,3)])
print d    # dict的Key是无序的
od = OrderedDict([(&#39;a&#39;,1),(&#39;b&#39;,2),(&#39;c&#39;,3)])
print od   #这个是有序的

</code></pre>

<p><img src="media/14945736764798/14945761816386.jpg" alt=""/></p>

<p><font color=red>注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：</font></p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

from collections import OrderedDict
od = OrderedDict()
od[&#39;z&#39;] = 1
od[&#39;y&#39;] = 2
od[&#39;x&#39;] = 3

print od
</code></pre>

<p><img src="media/14945736764798/14945763021698.jpg" alt=""/></p>

<p><code>OrderedDict</code>可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-

from collections import OrderedDict

class LastUpdatedOrderedDict(OrderedDict):

    def __init__(self, capacity):
        super(LastUpdatedOrderedDict, self).__init__()
        self._capacity = capacity

    def __setitem__(self, key, value):
        containsKey = 1 if key in self else 0
        if len(self) - containsKey &gt;= self._capacity:
            last = self.popitem(last=False)
            print &#39;remove:&#39;, last
        if containsKey:
            del self[key]
            print &#39;set:&#39;, (key, value)
        else:
            print &#39;add:&#39;, (key, value)
        OrderedDict.__setitem__(self, key, value)
od = LastUpdatedOrderedDict(3)
od[&#39;z&#39;] = 1
od[&#39;w&#39;] = 2
od[&#39;a&#39;] = 3
od[&#39;q&#39;] = 4
print od  
</code></pre>

<p><img src="media/14945736764798/14945770697335.jpg" alt=""/></p>

<h3 id="toc_5">Counter</h3>

<p><code>Counter</code>是一个简单的计数器，例如，统计字符出现的个数：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*-
from collections import Counter

c = Counter()
for ch in &#39;Programming&#39;:
    c[ch] = c[ch] + 1

print c

</code></pre>

<p><img src="media/14945736764798/14945772600780.jpg" alt=""/></p>

<h1 id="toc_6">小结</h1>

<p><code>collections</code>模块提供了一些有用的集合类，可以根据需要选用。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CNN卷积神经网络]]></title>
    <link href="https://lockxmonk.github.io/14940530686573.html"/>
    <updated>2017-05-06T14:44:28+08:00</updated>
    <id>https://lockxmonk.github.io/14940530686573.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">简介</h2>

<p>卷积神经网络是近年发展起来，并引起广泛重视的一种高效识别方法。20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络（Convolutional Neural Networks-简称CNN）。现在，CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，<mark>由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用</mark>。</p>

<p>一般地，CNN的基本结构包括两层：</p>

<ol>
<li><p>其一为<strong>特征提取层</strong>：每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；</p></li>
<li><p>其二是<strong>特征映射层</strong>：网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。</p></li>
</ol>

<p>CNN主要<strong>用来识别位移、缩放及其他形式扭曲不变性的二维图形</strong>。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，避免了显示的特征抽取，而隐式地从训练数据中进行学习；</p>

<p>再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。<font color=red>卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，</font><strong>其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度</strong>。</p>

<h2 id="toc_1">卷积神经网络</h2>

<p>在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。在上一节中提到的神经网络中，如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为：\(1000000×1000000=10^{12}\)，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。就跟辟邪剑谱似的，普通人练得很挫，一旦自宫后内力变强剑法变快，就变的很牛了。</p>

<h3 id="toc_2">局部感知</h3>

<p>卷积神经网络有<mark>两种神器</mark>可以降低参数数目，<mark>第一种神器叫做局部感知野</mark>。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。<strong>因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知</strong>，然后在更高层将局部的信息综合起来就得到了全局的信息。<strong>网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。</strong>如下图所示：左图为全连接，右图为局部连接。</p>

<p><img src="media/14940530686573/14940535875953.jpg" alt=""/></p>

<p>在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的千分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。</p>

<h3 id="toc_3">参数共享</h3>

<p>但其实这样的话参数仍然过多，那么就启动第二级神器，<mark>即权值共享</mark>。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100了。</p>

<p>怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。<font color=red>这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征</font>。</p>

<p>更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8×8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8×8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8×8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>

<p>如下图所示，展示了一个33的卷积核在55的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来</p>

<p><img src="media/14940530686573/14940539862222.gif" alt=""/></p>

<h3 id="toc_4">多卷积核</h3>

<p>上面所述只有100个参数时，表明只有1个100*100的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示：<br/>
<img src="media/14940530686573/14940546422731.jpg" alt=""/></p>

<p>上图右，不同颜色表明不同的卷积核。每个卷积核都会将图像生成为另一幅图像。比如两个卷积核就可以将生成两幅图像，这两幅图像可以看做是一张图像的不同的通道。如下图所示，下图有个小错误，即将w1改为w0，w2改为w1即可。下文中仍以w1和w2称呼它们.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[正则表达式]]></title>
    <link href="https://lockxmonk.github.io/14939712159345.html"/>
    <updated>2017-05-05T16:00:15+08:00</updated>
    <id>https://lockxmonk.github.io/14939712159345.html</id>
    <content type="html"><![CDATA[
<p>字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取<code>@</code>前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。</p>

<p>正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。</p>

<p>所以我们判断一个字符串是否是合法的Email的方法是：</p>

<ol>
<li><p>创建一个匹配Email的正则表达式；</p></li>
<li><p>用该正则表达式去匹配用户的输入来判断是否合法。</p></li>
</ol>

<p>因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。</p>

<p>在正则表达式中，如果直接给出字符，就是精确匹配。用<code>\d</code>可以匹配一个数字，<code>\w</code>可以匹配一个字母或数字，所以：</p>

<ul>
<li><p><code>&#39;00\d&#39;</code>可以匹配<code>&#39;007&#39;</code>，但无法匹配<code>&#39;00A&#39;</code>；</p></li>
<li><p><code>&#39;\d\d\d&#39;</code>可以匹配<code>&#39;010&#39;</code>；</p></li>
<li><p><code>&#39;\w\w\d&#39;</code>可以匹配<code>&#39;py3&#39;</code>；</p></li>
</ul>

<p><code>.</code><strong>可以匹配任意字符</strong>，所以：</p>

<ul>
<li><code>&#39;py.&#39;</code>可以匹配<code>&#39;pyc&#39;</code>、<code>&#39;pyo&#39;</code>、<code>&#39;py!&#39;</code>等等。</li>
</ul>

<p>要匹配变长的字符，在正则表达式中，用<code>*</code>表示任意个字符（包括0个），用<code>+</code>表示至少一个字符，用<code>?</code>表示0个或1个字符，用<code>{n}</code>表示n个字符，用<code>{n,m}</code>表示n-m个字符：</p>

<p>来看一个复杂的例子：<code>\d{3}\s+\d{3,8}</code>。</p>

<p>我们来从左到右解读一下：</p>

<ol>
<li><p><code>\d{3}</code>表示匹配3个数字，例如<code>&#39;010&#39;</code>；</p></li>
<li><p><code>\s</code>可以匹配一个空格（也包括Tab等空白符），所以<code>\s+</code>表示至少有一个空格，例如匹配<code>&#39; &#39;，&#39; &#39;</code>等；</p></li>
<li><p><code>\d{3,8}</code>表示3-8个数字，例如<code>&#39;1234567&#39;</code>。</p></li>
</ol>

<p>综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。</p>

<p>如果要匹配<code>&#39;010-12345&#39;</code>这样的号码呢？由于<code>&#39;-&#39;</code>是特殊字符，在正则表达式中，要用<code>&#39;\&#39;</code>转义，所以，上面的正则是<code>\d{3}\-\d{3,8}</code>。</p>

<p>但是，仍然无法匹配<code>&#39;010 - 12345&#39;</code>，因为带有空格。所以我们需要更复杂的匹配方式。</p>

<h2 id="toc_0">进阶</h2>

<p>要做更精确地匹配，可以用<code>[]</code>表示范围，比如：</p>

<ul>
<li><p><code>[0-9a-zA-Z\_]</code>可以匹配一个数字、字母或者下划线；</p></li>
<li><p><code>[0-9a-zA-Z\_]+</code>可以匹配至少由一个数字、字母或者下划线组成的字符串，比如<code>&#39;a100&#39;，&#39;0_Z&#39;，&#39;Py3000&#39;</code>等等；</p></li>
<li><p><code>[a-zA-Z\_][0-9a-zA-Z\_]*</code>可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；</p></li>
<li><p><code>[a-zA-Z\_][0-9a-zA-Z\_]{0, 19}</code>更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。</p></li>
</ul>

<p><code>A|B</code>可以匹配A或B，所以<code>(P|p)ython</code>可以匹配<code>&#39;Python&#39;</code>或者<code>&#39;python&#39;</code>。</p>

<p><code>^</code>表示行的开头，<code>^\d</code>表示必须以数字开头。</p>

<p><code>$</code>表示行的结束，<code>\d$</code>表示必须以数字结束。</p>

<p>你可能注意到了，<code>py</code>也可以匹配&#39;python&#39;，但是加上<code>^py$</code>就变成了整行匹配，就只能匹配<code>&#39;py&#39;</code>了。</p>

<h2 id="toc_1">re模块</h2>

<p>有了准备知识，我们就可以在Python中使用正则表达式了。Python提供<code>re</code>模块，包含所有正则表达式的功能。由于Python的字符串本身也用<code>\</code>转义，所以要特别注意：</p>

<pre><code class="language-py">s = &#39;ABC\\-001&#39; # Python的字符串
# 对应的正则表达式字符串变成：
# &#39;ABC\-001&#39;
</code></pre>

<p>因此我们强烈建议使用Python的<code>r</code>前缀，就不用考虑转义的问题了：</p>

<pre><code class="language-py">s = r&#39;ABC\-001&#39; # Python的字符串
# 对应的正则表达式字符串不变：
# &#39;ABC\-001&#39;
</code></pre>

<p>先看看如何判断正则表达式是否匹配：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*
import re

print re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010-12345&#39;)
print re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010 12345&#39;)

</code></pre>

<p><code>match()</code>方法判断是否匹配，如果匹配成功，返回一个<code>Match</code>对象，否则返回<code>None</code>。<br/>
<img src="media/14939712159345/14939732218646.jpg" alt=""/></p>

<p>常见的判断方法就是：</p>

<pre><code class="language-py"># -*- coding: utf-8 -*
import re

Spattern = r&#39;^\d{3}\-\d{3,8}$&#39;  #用户定义的正则表达式
test = &#39;010-12345&#39;  #用户输入的字符串
test2 = &#39;012 12345&#39;
if re.match(Spattern , test):
    print &#39;OK&#39;
else:
    print &#39;failed&#39;

if re.match(Spattern , test2):
    print &#39;OK&#39;
else:
    print &#39;failed&#39;
</code></pre>

<p><img src="media/14939712159345/14939734520573.jpg" alt=""/></p>

]]></content>
  </entry>
  
</feed>
