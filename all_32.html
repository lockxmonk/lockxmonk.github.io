<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="MAC%20OS.html">MAC OS</a></li>
        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">图像去雾技术</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html">算法学习</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14932573311953.html">
                
                  <h1>图像去雾相关论文总结</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1.特征学习的单幅图像去雾算法 2016年</a>
</li>
</ul>
</li>
<li>
<a href="#toc_1">2.尺度自适应暗通道先验去雾方法</a>


<h2 id="toc_0">1.特征学习的单幅图像去雾算法 2016年</h2>

<p><strong>目的：</strong>为提高图像去雾的普适性，提出一种特征学习的单幅图像去雾方法。</p>

<p><strong>方法：</strong>通过稀疏自动编码机对有雾图像进行<strong>多尺度的纹理结构特征提取</strong>，同时抽取各种与雾相关的颜色特征。然后采用多层神经网络进行样本训练，得到雾天条件下纹理结构特征及颜色特征与场景深度间的映射关系，并估算出有雾图像的场景深度图。最后结合大气散射模型，根据场景深度图复原无雾图像。</p>

<p><strong>结论：对实验结果的定性及定量分 析表明，本文算法能有效获取有雾图像的场景深度，复原出视觉效果理想的无雾图像，且具有很好的场景普适性。</strong></p>

<p><strong>理解</strong>：<font color=red>本文认为图像去雾的问题可进一步转化为场景深度d的求解问题。</font>本文通过自动编码机获取到图片中物体的纹理特征，然后用多尺度的方法分别提取<strong>颜色特征、暗原色特征、颜色衰减特征、纹理结构</strong>特征。<strong>四者结合对场景深度进行估计</strong>。</p>

<p><strong>可以改进的</strong>：</p>

<ol>
<li><p>算法实时性不够强，效率比较低，可以考虑缩小图片尺寸，降低数据量，从而运用到视频去雾。</p></li>
<li><p>对雾气分布不均匀的图像，效果不理想，需要结合不同的气象模型来建立更为鲁棒的物理模型。</p></li>
</ol>

<h1 id="toc_1">2.尺度自适应暗通道先验去雾方法</h1>

<p><strong>目的：</strong><br/>
针对暗通道先验去雾方法的尺度选择问题 提出了一种尺度自适应方法， 根据图像的颜色和边 缘特征将暗通道求解的尺度自适应地调整到一个合适的范围。</p>

<p><strong>方法：</strong></p>

<ol>
<li><p>由颜色特征求解初始尺度：<font color=red>对于图像的不同区域采用不同的尺度求解暗通道: 在亮度较低或饱和度较高的区域，采用较小尺度; 在亮度较高且饱和度较低的区域，采用较大尺度;在景深突变处，采用较小尺度;在平滑区域，采用较大尺度。</font></p></li>
<li><p>由边缘特征对尺度进行修正:由于“光晕”现象发生在景深突变处，如果在边缘附近采用较小的尺度，可使透射率的求解窗口\(Ω_r(x)\)尽量不跨越景深边界，从而减小“光晕”现象; 在非边缘处采用较大的尺度，可以增大 \(J_{dark}(x)→0\)的概率，使复原图像的背景更平滑，噪声和失真更小。 由边缘特征对初始尺度\(r_0(x)\)进行修正.</p></li>
</ol>

<p><strong>结论：</strong><br/>
整个去雾过程参数自动配置，无需人工干预，兼顾不同尺度复原图像的优点，复原图像色彩自然，对比度提升显著，并有效抑制了“光晕”现象。对多种雾化场景图像的处理结果表明: <strong>文中方法对场景的适应性强， 在任何情况下的处理结果均能达到 DG 方法的最优结果，甚至更好。</strong></p>

<p><strong>理解：</strong><br/>
该论文主要是针对HE的去雾方法，对暗通道尺寸进行自适应确定，减少了人工干预的复杂度。</p>

<p><strong>可以改进：</strong><br/>
<font color=red>该算法在运行时间上可以改进，也可以利用机器学习的方法来提取图片特征，避免Canny算子的局限性造成的边缘提取误差。<br/>
可以与上篇论文进行结合来提高上文尺度特征提取的效率。</font></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html'>图像去雾技术</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14928484245894.html">
                
                  <h1>文档测试</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>如果你经常阅读Python的官方文档，可以看到很多文档都有示例代码。比如<a href="https://docs.python.org/2/library/re.html">re</a>模块就带了很多示例代码：</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; m = re.search(&#39;(?&lt;=abc)def&#39;, &#39;abcdef&#39;)
&gt;&gt;&gt; m.group(0)
&#39;def&#39;
</code></pre>

<p>可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。</p>

<p>这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以粘贴出来直接运行，那么，可不可以自动执行写在注释中的这些代码呢？</p>

<p>答案是肯定的。</p>

<p>当我们编写注释时，如果写上这样的注释：</p>

<pre><code class="language-py">
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/22</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14920445399813.html">
                
                  <h1>模型学习的最优化算法（接上一文）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">改进的迭代尺度法</a>
</li>
</ul>


<p>逻辑斯谛回归模型、最大嫡模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解.从最优化的观点看，这时的目标函数具有很好的性质.<font color=red>它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解.</font>常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法.牛顿法或拟牛顿法一般收敛速度更快.</p>

<p>这次主要学习基于改进的迭代尺度法与拟牛顿法的最大熵模型学习算法。（还有梯度下降法，这次不过多学习）</p>

<h2 id="toc_0">改进的迭代尺度法</h2>

<p>改进的迭代尺度法（improved iterative scaling，IIS)是一种最大熵模型学习的最优化算法.</p>

<p>已知最大熵模型为：<br/>
<img src="media/14920445399813/14920447866369.jpg" alt=""/><br/>
对数似然函数为：<br/>
<img src="media/14920445399813/14920449425684.jpg" alt=""/><br/>
目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值\(\hat{w}\)</p>

<p>改进的迭代尺度算法（iis）的想法是：假设最大熵模型当前的参数向量是\(w=(w_1,w_2,...,w_n)^T\),我们 希望找到一个新的参数向量\(w+\delta = (w_1+\delta_1,w_2+\delta_2,....,w_n+\delta_n)^T\)，使得模型的对数似然函数值增大.如果能有这样一种参数向量更新的方法\(\tau:w \rightarrow w+\delta\),那么就 可以重复使用这一方法，直至找到对数似然函数的最大值.</p>

<p>对于给定的经验分布\(\tilde P(x,y)\),模型参数从\(w到w+\delta\)，对数似然函数的改变量是：<br/>
<img src="media/14920445399813/14920457816508.jpg" alt=""/></p>

<p>如果能找到适当的\(\delta使下届A(\delta | w)\)提高，那么对数似然函数也会提高。然而，函数\(A(\delta | w)\)中的\(\delta\)是一个向量，含有多个变量，不容易同时优化，IIS试图以此只优化其中一个变量\(\delta_i\)，而固定其他变量\(\delta_j,i  \neq j\).</p>

<p>为达到这一目的，IIS进一步降低下界\(A(\delta | w)\),具体的，IIS引进一个量\(f^{\#}(x,y)\):<br/>
<img src="media/14920445399813/14920481227290.jpg" alt=""/><br/>
因为\(f_i\)是一个二值函数，故\(f^{\#}(x,y)\)表示所有特征在（x，y）出现的次数，这样，\(A(\delta | w)\)可以改写为：</p>

<p><img src="media/14920445399813/14920483320007.jpg" alt=""/></p>

<p><strong>下面给出IIS算法：</strong><br/>
<img src="media/14920445399813/14920487060621.jpg" alt=""/><br/>
<img src="media/14920445399813/14920491294532.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/13</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14917858710123.html">
                
                  <h1>logistic regression(逻辑斯蒂回归)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">逻辑斯蒂回归模型</a>
<ul>
<li>
<a href="#toc_1">逻辑斯蒂分布</a>
</li>
<li>
<a href="#toc_2">二项逻辑斯蒂回归模型</a>
</li>
<li>
<a href="#toc_3">模型参数估计</a>
</li>
<li>
<a href="#toc_4">多项了逻辑斯蒂回归</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_5">最大熵模型</a>
<ul>
<li>
<a href="#toc_6">最大熵原理</a>
<ul>
<li>
<a href="#toc_7">最大熵模型的定义</a>
</li>
<li>
<a href="#toc_8">最大熵模型的学习</a>
</li>
<li>
<a href="#toc_9">极大似然估计</a>
</li>
</ul>
</li>
</ul>


<p>简介：逻辑斯谛回归（logistic regression)是统计学习中的经典分类方法.最大熵是 概率模型学习的一个准则，将其推广到分类问题得到最大熵模型(maximum entropy model).逻辑斯谛回归模型与最大熵模型都属于对数线性模型.</p>

<h2 id="toc_0">逻辑斯蒂回归模型</h2>

<h3 id="toc_1">逻辑斯蒂分布</h3>

<p><img src="media/14917858710123/14918919297549.jpg" alt=""/></p>

<h3 id="toc_2">二项逻辑斯蒂回归模型</h3>

<p>二项逻辑斯诗回归模型（binomial logistic regression model)是一种分类模型，由条件概率分布\(P(Y|X)\)表示，形式为参数化的逻辑斯谛分布.这里，随机变量X取值为实数，随机变量Y取值为1或0.我们通过监督学习的方法来估计模型参数.</p>

<p><img src="media/14917858710123/14918930389977.jpg" alt=""/><br/>
现在考査逻辑斯谛回归模型的特点.一个事件的几率（odds)是指该事件发生的概率与该事件不发生的概率的比值.如果事件发生的概率是p,那么该亊件的几率是\(\frac{1}{1-p}\) ,该事件的对数几率(log odds)或logit函数是<br/>
<img src="media/14917858710123/14918936472837.jpg" alt=""/><br/>
<font color = red>这就是说，在逻辑斯谛回归模型中，输出y = l的对数几率是输入x的线性函数.或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型.</font></p>

<p>换一个角度看，考虑对输入x进行分类的线性函数\(w*x\),其值域为实数域.注意，这里\(x\in R^{n+1},w \in R^{n+1}\).通过逻辑斯谛回归模型定义式(6.5)可以将线性函数\(w*x\)转换为概率：<br/>
<img src="media/14917858710123/14918951209571.jpg" alt=""/></p>

<p><strong>这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0(如图6.1所示).这样的模型就是逻辑斯谛回归模型.</strong></p>

<h3 id="toc_3">模型参数估计</h3>

<p>逻辑斯谛回归模型学习时，对于给定的训练数据集\(T=\{{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\}}\),其中\(x_i\in R^n,y_i \in \{{0,1\}}\)，<strong>可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型.</strong><br/>
<img src="media/14917858710123/14918969386189.jpg" alt=""/></p>

<h3 id="toc_4">多项了逻辑斯蒂回归</h3>

<p>上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类.可以将其推广为多项逻辑斯缔回归模型(multi-nominal logistic regression model)，用于多类分类.假设离散型随机变量Y的取值集合是{1,2...K}，那么多项逻辑斯谛回归模型是:<br/>
<img src="media/14917858710123/14918973306162.jpg" alt=""/><br/>
二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯讳回归.</p>

<h1 id="toc_5">最大熵模型</h1>

<p>最大熵模型（maximum entropy model)由最大熵原理推导实现.这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。</p>

<h2 id="toc_6">最大熵原理</h2>

<p>最大熵原理是概率模型学习的一个准则.<font color=red>最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型.通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型.</font></p>

<p>假设离散随机变量X的概率分布是p(X),则其熵是：<br/>
<img src="media/14917858710123/14919006601164.jpg" alt=""/><br/>
直观地，<font color=red>最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件.在没有更多信息的情况下，那些不确定的部分都是“等可能的”.最大熵原理通过熵的最大化来表示等可能性.</font>“等可能”不容易操作，而熵则是一个可优化的数值指标.</p>

<p>首先，可以通过一个简单的例子来了解一下最大熵原理.<br/>
<img src="media/14917858710123/14919019478267.jpg" alt=""/></p>

<p>图6.2提供了用最大熵原理进行概率模型选择的几何解释.概率模型集合P可由欧氏空间中的单纯形(simplex)表示，如左图的三角形(2-单纯形).一个点代表一个模型，整个单纯形代表模型集合.右图上的一条直线对应于一个约束 条件，直线的交集对应于满足所有约束条件的模型集合.一般地，这样的模型仍有无穷多个.学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则给出最优模型选择的一个准则.<br/>
<img src="media/14917858710123/14919593499530.jpg" alt=""/></p>

<h3 id="toc_7">最大熵模型的定义</h3>

<p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型. </p>

<p>假设分类模型是一个条件概率分布\(P(Y|X),X \in \mathcal{X} \subseteq R^n\)表示输入，\(Y \in \mathcal{Y}\)表示输出，\(\mathcal{X}\)和\(\mathcal{Y}\)分别是输入和输出的集合.这个模型表示的是对于给定的输入\(\mathcal{X}\)，以条作概率\(P(Y|X)\)输出Y.<br/>
给定一个训练数据集<br/>
\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)<br/>
学习的目标是用最大熵原理选择最好的分类模型。</p>

<p>首先考虑模型应该满足的条件，给定训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布，分别以\(\tilde{P}(X,Y)和\tilde{P}(X)\)表示。这里：<br/>
<img src="media/14917858710123/14919609290680.jpg" alt=""/><br/>
 用特征函数\(f(x,y)描述输入x和输出y之间的某一个事实\)。其定义是：<br/>
 <img src="media/14917858710123/14919611561526.jpg" alt=""/><br/>
特征函数f(x,y)关于经验分布\(\tilde{P}(X,Y)\)的期望值，用\(E_\tilde{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919613926643.jpg" alt=""/><br/>
特征函数f(x,y)关于模型P(Y|X)与经验分布\(\tilde{P}(X)\)的期望值，用\(E_{P}(f)\)表示：<br/>
<img src="media/14917858710123/14919617357953.jpg" alt=""/></p>

<p>最大熵模型的定义为：<br/>
<img src="media/14917858710123/14919625972087.jpg" alt=""/></p>

<h3 id="toc_8">最大熵模型的学习</h3>

<p>最大熵模型的学习过程就是求解最大熵模型的过程.最大熵模型的学习可以形式化为约束最优化问题.<br/>
对于给定的训练数据集\(T= {\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\}\)以及特征函数\(f_i(x,y),i=1,2,...n\),最大熵模型的学习等价于约束最优化问题：<br/>
<img src="media/14917858710123/14919630413332.jpg" alt=""/><br/>
求解约束最优化问题(6.14)〜(6.16)，所得出的解，就是最大熵模型学习的解.下面给出具体推导.<br/>
这里，将约束最优化的原始问题转换为无约束最优化的对偶问题.通过求解对偶问题求解原始问题.<br/>
<img src="media/14917858710123/14919633775267.jpg" alt=""/><br/>
<img src="media/14917858710123/14919633957280.jpg" alt=""/></p>

<p>下面举例来说明最大熵模型的计算：</p>

<p><strong>例：假设随机变量X有5个取值{A,B,C,D,E}，要估计各个值的概率P(A),P(B),P(C),P(D),P(E).</strong></p>

<p><img src="media/14917858710123/14919642672585.jpg" alt=""/><br/>
<img src="media/14917858710123/14919642856866.jpg" alt=""/><br/>
<img src="media/14917858710123/14919643007948.jpg" alt=""/></p>

<h3 id="toc_9">极大似然估计</h3>

<p>从以上最大熵模型学习中可以看出，最大熵模型是由式(6.22)、式(6.23)表示的 条件概率分布.下面证明对偶函数的极大化等价于最大嫡模型的极大似然估计.</p>

<p>己知训练数据的经验概率分布\(\tilde{P}(X,Y)\),条件概率分布\(P(Y|X)\)的对数似然 函数表示为:<br/>
<img src="media/14917858710123/14919659054476.jpg" alt=""/><br/>
当提哦啊煎概率分布P(y|x)是最大熵模型(6.22)和(6.23)时,对数似然函数\(L_ \tilde{p}(P_w)\)为：<br/>
<img src="media/14917858710123/14919660761710.jpg" alt=""/><br/>
比较算式(6.26)和式(6.27),可得：<br/>
<img src="media/14917858710123/14920442862315.jpg" alt=""/></p>

<p>既然对偶函数\(\psi(w)\)等价于对数似然函数\(L_{\tilde p}(P_w)\) ,于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实.<br/>
这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题.</p>

<p>可以将最大熵模型写成更一般的形式.<br/>
<img src="media/14917858710123/14920444597468.jpg" alt=""/><br/>
最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型(log linear model).模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/4/10</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14908575557810.html">
                
                  <h1>决策树生成--CART算法</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">CART算法</a>
<ul>
<li>
<a href="#toc_1">CART生成</a>
<ul>
<li>
<a href="#toc_2">1.回归树的生成</a>
</li>
<li>
<a href="#toc_3">2.分类树的生成</a>
</li>
<li>
<a href="#toc_4">CART剪枝</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">CART算法</h2>

<p>分类与回归树(classification and regression tree, CART)模型由 Breiman等人在1984年提出，是应用广泛的决策树学习方法.CART同样由特征选择、树 的生成及剪枝组成，<mark>既可以用于分类也可以用于回归.</mark>以下将用于分类与回归的树统称为决策树.</p>

<p>CART是在给定输入随机变量X的条件下输出随机变量Y的条件概率分布的学习方法.CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支.这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布.</p>

<p><strong>CART算法由以下两步组成：</strong></p>

<p>1.决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大：<br/>
2.决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准.</p>

<h3 id="toc_1">CART生成</h3>

<p>决策树的生成就是递归地构建二叉决策树的过程.对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树.</p>

<h4 id="toc_2">1.回归树的生成</h4>

<p>假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集:<br/>
    \(D={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}\)<br/>
考虑如何生成回归树。</p>

<p>一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上 的输出值.假设己将输入空间划分为M个单元\(R_1,R_2,R_3...R_M\),并且在每个单元\(R_M\)上 有一个固定的输出值\(c_m\)，于是回归树模型可表示为:<br/>
<img src="media/14908575557810/14909500378632.jpg" alt=""/></p>

<p>当输入空间的划分确定时，可以用平方误差\(\sum_{x_i\in R}(y_i-f(x_i))^2\)来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值.易知，单元\(R_m\)上的\(c_m\)的最优值\(\hat c_m\)是\(R_m\)上的所有输入实例\(x_i\)对应的输出\(y_i\)的均值，即:<br/>
<img src="media/14908575557810/14909504677321.jpg" alt=""/></p>

<p>问题是怎样对输入空间进行划分,这里采用启发式的方法，选择第j个变量\(x^{(j)}\)和它取的值s，作为切分变量（splitting variable)和切分点（splitting point),并定义两个区域：<br/>
<img src="media/14908575557810/14909505605933.jpg" alt=""/><br/>
然后寻找最优切分变量j和最优切分点s，最优地，求解：<br/>
<img src="media/14908575557810/14909506372664.jpg" alt=""/><br/>
对固定输入变量j可以找到最优切分点s:<br/>
<img src="media/14908575557810/14909506657025.jpg" alt=""/><br/>
便利所有输入变量，找到最优的切分变量j，构成一个对\((j,s)\).依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止.这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(least squares regression tree)，现将算法叙述如下：</p>

<p>最小二乘回归树生成算法：<br/>
<img src="media/14908575557810/14909510077124.jpg" alt=""/></p>

<h4 id="toc_3">2.分类树的生成</h4>

<p>分类树用基尼指数选择最有特征，同时决定该特征的最优二值切分点。</p>

<p><strong>基尼指数：</strong><br/>
<img src="media/14908575557810/14913540871077.jpg" alt=""/><br/>
如果样本集合D根据特征A是否取某一可能值a被分割成\(D_1和D_2\)两部分，即：<br/>
<img src="media/14908575557810/14913550101283.jpg" alt=""/></p>

<p>基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p>

<p>图5.7显示二类分类问题中基尼指数Gini(p)、熵（单位比特）之半和分类误差率的关系.横坐标表示概率P，纵坐标表示损失.可以看出基尼指数和 熵之半的曲线很接近，都可以近似地代表分类误差率.<br/>
<img src="media/14908575557810/14913559817897.jpg" alt=""/></p>

<p><strong>CART生成算法：</strong><br/>
<img src="media/14908575557810/14913560936133.jpg" alt=""/><br/>
<img src="media/14908575557810/14913561059742.jpg" alt=""/></p>

<p><strong>下面继续用例题来说明该算法：</strong><br/>
<img src="media/14898241272608/14908402191601.jpg" alt=""/><br/>
根据上表所给的训练数据集，应用CART算法生成决策树。</p>

<p><img src="media/14908575557810/14913582940784.jpg" alt=""/></p>

<h4 id="toc_4">CART剪枝</h4>

<p>CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小(模型变简单)，从而能够对未知数据有更准确的预测.CART剪枝算法由两步组成：首先从生成算法产生的决策树\(T_0\)底端开始不断剪枝，直到\(T_0\)的根结点，形成一个子树序列\( {T_0,T_1,...,T_n}\);然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树.</p>

<p>1.<strong>剪枝</strong>，形成一个子树序列<br/>
在剪枝过程中，计算子树的损失函数：<br/>
<img src="media/14908575557810/14913604606599.jpg" alt=""/><br/>
其中，T为任意子树，C(T)为对训练数据的预测误差(如基尼指数)，丨T丨为子树的叶结点个数，\(α \geq0\)为参数，\(C_{\alpha}(T)\)为参数是\(\alpha\)时的子树T的整体损失.参数\(\alpha\)权衡训练数据的拟合程度与模型的复杂度.</p>

<p>对固定的\(\alpha\), —定存在使损失函数\(C_{\alpha}(T)\)最小的子树，将其表示为\(T_{\alpha}\)。\(T_{\alpha}\)在损失函数\(C_{\alpha}(T)\)最小的意义下是最优的.容易验证这样的最优子树是唯一的.当\(\alpha\)大的时候，最优子树\(T_{\alpha}\)偏小；当\(\alpha\)小的时候，最优子树\(T_{\alpha}\)偏大.极端情况，当\(\alpha\) = 0时，整体树是最优的.当\(\alpha \rightarrow \infty \)时，根结点组成的单结点树是最优的.</p>

<p>Breiman等人证明：可以用递归的方法对树进行剪枝.将\(\alpha\)从小增大，\(0=\alpha_0&lt;\alpha_1&lt;...&lt;\alpha_n&lt;\infty,\)，产生一系列的区间\([\alpha_i,\alpha_{i+1}),i=0,1,...n;\)，剪枝得到的子树 序列对应着区间\(\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,...n;\)的最优子树序列\( {T_0,T_1,...,T_n}\),序列中的子树是嵌套的.</p>

<p><img src="media/14908575557810/14913621104324.jpg" alt=""/><br/>
<img src="media/14908575557810/14913625337737.jpg" alt=""/></p>

<p><strong>CART剪枝算法：</strong><br/>
<img src="media/14908575557810/14913629643758.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/3/30</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14878150947508.html">
                
                  <h1>MNIST机器学习入门</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>MNIST是一个入门级的计算机视觉数据集，它包含各种手写数字图片：<br/>
<img src="media/14878150947508/14878151559670.png" alt=""/></p>

<p>它也包含每一张图片对应的标签，告诉我们这个是数字几。比如，上面这四张图片的标签分别是5，0，4，1。</p>

<p>这里我们将试着训练一个机器学习模型用于预测图片里面的数字。这次我们的主要目的是研究如何使用TensorFlow。所以，我们这里会从一个很简单的数学模型开始，它叫做Softmax Regression。</p>

<p>对应这个教程的实现代码很短，而且真正有意思的内容只包含在三行代码里面。但是，去理解包含在这些代码里面的设计思想是非常重要的：TensorFlow工作流程和机器学习的基本概念。因此，此次会很详细地介绍这些代码的实现原理。</p>

<h1 id="toc_0">MNIST数据集</h1>

<p>MNIST数据集的官网是<a href="http://yann.lecun.com/exdb/mnist/">Yann LeCun&#39;s website</a>。在这里，我们提供了一份python源代码（见附录）用于自动下载和安装这个数据集。你可以下载这份代码，然后用下面的代码导入到你的项目里面，也可以直接复制粘贴到你的代码文件里面。</p>

<pre><code>import input_data
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)
</code></pre>

<p>下载下来的数据集被分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。这样的切分很重要，在机器学习模型设计时必须有一个单独的测试数据集不用于训练而是用来评估这个模型的性能，从而更加容易把设计的模型推广到其他数据集上（泛化）。</p>

<p>正如前面提到的一样，每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为“xs”，把这些标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是<code>mnist.train.images</code> ，训练数据集的标签是 <code>mnist.train.labels</code>。</p>

<p>每一张图片包含28像素X28像素。我们可以用一个数字数组来表示这张图片：</p>

<p><img src="media/14878150947508/14878156081245.png" alt=""/><br/>
我们把这个数组展开成一个向量，长度是 28x28 = 784。如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开。从这个角度来看，MNIST数据集的图片就是在784维向量空间里面的点, 并且拥有比较<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/">复杂的结构</a> (提醒: 此类数据的可视化是计算密集型的)。</p>

<p>展平图片的数字数组会丢失图片的二维结构信息。这显然是不理想的，最优秀的计算机视觉方法会挖掘并利用这些结构信息，我们会在后续教程中介绍。但是在这个教程中我们忽略这些结构，所介绍的简单数学模型，softmax回归(softmax regression)，不会利用这些结构信息。</p>

<p>因此，在MNIST训练数据集中，mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。</p>

<p><img src="media/14878150947508/14878161521414.png" alt=""/></p>

<p>相对应的MNIST数据集的标签是介于0到9的数字，用来描述给定图片里表示的数字。为了用于这个教程，我们使标签数据是&quot;one-hot vectors&quot;。 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0。所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此，<code>mnist.train.labels</code> 是一个 [60000, 10] 的数字矩阵。</p>

<p><img src="media/14878150947508/14878162172884.png" alt=""/></p>

<p>现在，我们准备好可以开始构建我们的模型啦！</p>

<h2 id="toc_1">Softmax回归介绍</h2>

<p>我们知道MNIST的每一张图片都表示一个数字，从0到9。我们希望得到给定图片代表每个数字的概率。比如说，我们的模型可能推测一张包含9的图片代表数字9的概率是80%但是判断它是8的概率是5%（因为8和9都有上半部分的小圆），然后给予它代表其他数字的概率更小的值。</p>

<p>这是一个使用softmax回归（softmax regression）模型的经典案例。softmax模型可以用来给不同的对象分配概率。即使在之后，我们训练更加精细的模型时，最后一步也需要用softmax来分配概率。</p>

<p>softmax回归（softmax regression）分两步：第一步</p>

<p>为了得到一张给定图片属于某个特定数字类的证据（evidence），我们对图片像素值进行加权求和。如果这个像素具有很强的证据说明这张图片不属于该类，那么相应的权值为负数，相反如果这个像素拥有有利的证据支持这张图片属于这个类，那么权值是正数。</p>

<p>下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值。红色代表负数权值，蓝色代表正数权值。</p>

<p><img src="media/14878150947508/14878173006094.png" alt=""/></p>

<p>我们也需要加入一个额外的偏置量（bias），因为输入往往会带有一些无关的干扰量。因此对于给定的输入图片 x 它代表的是数字 i 的证据可以表示为</p>

<p><img src="media/14878150947508/14878173172518.png" alt=""/></p>

<p>其中  <img src="media/14878150947508/14878173370916.png" alt=""/>代表权重，<img src="media/14878150947508/14878173498830.png" alt=""/> 代表数字 i 类的偏置量，j 代表给定图片 x 的像素索引用于像素求和。然后用softmax函数可以把这些证据转换成概率 y：<br/>
<img src="media/14878150947508/14878173861592.png" alt=""/></p>

<p>这里的softmax可以看成是一个激励（activation）函数或者链接（link）函数，把我们定义的线性函数的输出转换成我们想要的格式，也就是关于10个数字类的概率分布。因此，给定一张图片，它对于每一个数字的吻合度可以被softmax函数转换成为一个概率值。softmax函数可以定义为：</p>

<p><img src="media/14878150947508/14878174086894.png" alt=""/></p>

<p>展开等式右边的子式，可以得到：<br/>
<img src="media/14878150947508/14878174196316.png" alt=""/></p>

<p>但是更多的时候把softmax模型函数定义为前一种形式：把输入值当成幂指数求值，再正则化这些结果值。这个幂运算表示，更大的证据对应更大的假设模型（hypothesis）里面的乘数权重值。反之，拥有更少的证据意味着在假设模型里面拥有更小的乘数系数。假设模型里的权值不可以是0值或者负值。Softmax然后会正则化这些权重值，使它们的总和等于1，以此构造一个有效的概率分布。（更多的关于Softmax函数的信息，可以参考Michael Nieslen的书里面的这个<a href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax">部分</a>，其中有关于softmax的可交互式的可视化解释。）</p>

<p>对于softmax回归模型可以用下面的图解释，对于输入的<code>xs</code>加权求和，再分别加上一个偏置量，最后再输入到softmax函数中：</p>

<p><img src="media/14878150947508/14878174795982.png" alt=""/></p>

<p>如果把它写成一个等式，我们可以得到：<br/>
<img src="media/14878150947508/14878175085060.png" alt=""/></p>

<p>我们也可以用向量表示这个计算过程：用矩阵乘法和向量相加。这有助于提高计算效率。（也是一种更有效的思考方式）</p>

<p><img src="media/14878150947508/14878174918084.png" alt=""/></p>

<p>更进一步，可以写成更加紧凑的方式：<br/>
<img src="media/14878150947508/14878175309181.png" alt=""/></p>

<h2 id="toc_2">实现回归模型</h2>

<p>为了用python实现高效的数值计算，我们通常会使用函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果你用GPU来进行外部计算，这样的开销会更大。用分布式的计算方式，也会花费更多的资源用来传输数据。</p>

<p>TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorflow不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）</p>

<p>使用TensorFlow之前，首先导入它：</p>

<pre><code class="language-py">import tensorflow as tf
</code></pre>

<p>我们通过操作符号变量来描述这些可交互的操作单元，可以用下面的方式创建一个：</p>

<pre><code class="language-py">x = tf.placeholder(&quot;float&quot;, [None, 784])
</code></pre>

<p><code>x</code>不是一个特定的值，而是一个占位符<code>placeholder</code>，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。我们用2维的浮点数张量来表示这些图，这个张量的形状是<code>[None，784 ]</code>。（这里的<code>None</code>表示此张量的第一个维度可以是任何长度的。）</p>

<p>我们的模型也需要权重值和偏置量，当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：<code>Variable</code> 。 一个<code>Variable</code>代表一个可修改的张量，存在在TensorFlow的用于描述交互性操作的图中。它们可以用于计算输入值，也可以在计算中被修改。对于各种机器学习应用，一般都会有模型参数，可以用<code>Variable</code>表示。</p>

<pre><code>y = tf.nn.softmax(tf.matmul(x,W) + b)
</code></pre>

<p>首先，我们用<code>tf.matmul(​​X，W)</code>表示<code>x</code>乘以<code>W</code>，对应之前等式里面的<img src="media/14878150947508/14878180799833.png" alt=""/>，这里<code>x</code>是一个2维张量拥有多个输入。然后再加上<code>b</code>，把和输入到<code>tf.nn.softmax</code>函数里面。</p>

<p>至此，我们先用了几行简短的代码来设置变量，然后只用了一行代码来定义我们的模型。TensorFlow不仅仅可以使softmax回归模型计算变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！</p>

<h2 id="toc_3">训练模型</h2>

<p>为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。其实，在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。但是，这两种方式是相同的。</p>

<p>一个非常常见的，非常漂亮的成本函数是“交叉熵”（cross-entropy）。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：<br/>
<img src="media/14878150947508/14878181838749.png" alt=""/></p>

<p>y 是我们预测的概率分布, y&#39; 是实际的分布（我们输入的one-hot vector)。比较粗糙的理解是，交叉熵是用来衡量我们的预测用于描述真相的低效性。更详细的关于交叉熵的解释超出本次试验的范畴，但是很有必要好好<a href="http://colah.github.io/posts/2015-09-Visual-Information/">理解它</a>。</p>

<p>为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值：</p>

<pre><code>y_ = tf.placeholder(&quot;float&quot;, [None,10])
</code></pre>

<p>然后我们可以用<img src="media/14878150947508/14878182871547.png" alt=""/>计算交叉熵:</p>

<pre><code>cross_entropy = -tf.reduce_sum(y_*tf.log(y))
</code></pre>

<p>首先，用 <code>tf.log</code> 计算 <code>y</code> 的每个元素的对数。接下来，我们把 <code>y_</code>的每一个元素和 <code>tf.log(y_)</code> 的对应元素相乘。最后，用 <code>tf.reduce_sum</code> 计算张量的所有元素的总和。（注意，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100幅图片的交叉熵的总和。对于100个数据点的预测表现比单一数据点的表现能更好地描述我们的模型的性能。</p>

<p>现在我们知道我们需要我们的模型做什么啦，用TensorFlow来训练它是非常容易的。因为TensorFlow拥有一张描述你各个计算单元的图，它可以自动地使用<a href="http://colah.github.io/posts/2015-08-Backprop/">反向传播算法(backpropagation algorithm)</a>来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后，TensorFlow会用你选择的优化算法来不断地修改变量以降低成本。</p>

<pre><code>train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
</code></pre>

<p>在这里，我们要求TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵。梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow只需将每个变量一点点地往使成本不断降低的方向移动。当然TensorFlow也提供了<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#optimizers">其他许多优化算法</a>：只要简单地调整一行代码就可以使用其他的算法。</p>

<p>TensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>

<p>现在，我们已经设置好了我们的模型。在运行计算之前，我们需要添加一个操作来初始化我们创建的变量：</p>

<pre><code>init = tf.initialize_all_variables()
</code></pre>

<p>现在我们可以在一个<code>Session</code>里面启动我们的模型，并且初始化变量：</p>

<pre><code>sess = tf.Session()
sess.run(init)
</code></pre>

<p>然后开始训练模型，这里我们让模型循环训练1000次！</p>

<pre><code>for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>该循环的每个步骤中，我们都会随机抓取训练数据中的100个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行<code>train_step</code>。</p>

<p>使用一小部分的随机数据来进行训练被称为随机训练（stochastic training）- 在这里更确切的说是随机梯度下降训练。在理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>

<h2 id="toc_4">评估我们的模型</h2>

<p>那么我们的模型性能如何呢？</p>

<p>首先让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如<code>tf.argmax(y,1)</code>返回的是模型对于任一输入x预测到的标签值，而 <code>tf.argmax(y_,1)</code> 代表正确的标签，我们可以用 <code>tf.equal</code> 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</p>

<pre><code>correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
</code></pre>

<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code> 会变成 <code>[1,0,1,1]</code> ，取平均值后得到 <code>0.75</code>.</p>

<pre><code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))
</code></pre>

<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>

<pre><code>print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
</code></pre>

<p>这个最终结果值应该大约是91%。</p>

<p>这个结果好吗？嗯，并不太好。事实上，这个结果是很差的。这是因为我们仅仅使用了一个非常简单的模型。不过，做一些小小的改进，我们就可以得到97％的正确率。最好的模型甚至可以获得超过99.7％的准确率！（想了解更多信息，可以看看这个关于各种模型的<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">性能对比列表</a>。)<br/>
附件：<a href="media/14878150947508/input_data.py">input_data</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/2/23</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html'>基础知识</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_31.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_33.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="/asset/img/logn.png" /></div>
            
                <h1>LZH007</h1>
                <div class="site-des">LZH的技术杂事小博客~</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/lockxmonk" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lzhabc007@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="MAC%20OS.html"><strong>MAC OS</strong></a>
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>图像去雾技术</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html"><strong>算法学习</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15197807986881.html">2015年蓝桥杯省赛C/C++ A组题解</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15196931743279.html">2016年第七届蓝桥杯C/C++程序设计本科B组省赛</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15196116405308.html">2017第八届蓝桥杯C/C++ B组省赛</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15166061069994.html">Longest Word in Dictionary through Deleting</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15165851014842.html">Longest Word in Dictionary</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
          <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1265629731'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s22.cnzz.com/z_stat.php%3Fid%3D1265629731%26online%3D1' type='text/javascript'%3E%3C/script%3E"));</script>    
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2017
Powered by <a target="_blank" href="https://lockxmonk.github.io/index.html">LZH</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
