<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  LZH007
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="LZH007" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:lockxmonk.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; LZH007</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="MAC%20OS.html">MAC OS</a></li>
        
            <li><a href="Effective%20OC2.0.html">Effective OC2.0</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">æœºå™¨å­¦ä¹ </a></li>
        
            <li><a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</a></li>
        
            <li><a href="Python%E7%BB%83%E4%B9%A0.html">Pythonç»ƒä¹ </a></li>
        
            <li><a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html">å›¾åƒå»é›¾æŠ€æœ¯</a></li>
        
            <li><a href="iOS.html">iOS</a></li>
        
            <li><a href="English%20Study.html">English Study</a></li>
        
            <li><a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html">ç®—æ³•å­¦ä¹ </a></li>
        
            <li><a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html">å¸¸è§é¢è¯•é—®é¢˜</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="14974281629389.html">
                
                  <h1>æ€æ ·ç§»é™¤OSXRESERVEDåˆ†åŒº(å¦‚æœBootCamp Assistantåœ¨å®‰è£…ä¹‹åæ²¡æœ‰æˆåŠŸåˆ é™¤è¿™ä¸ªåˆ†åŒº)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ol>
<li>æ‰“å¼€ç£ç›˜å·¥å…·</li>
<li>ç‚¹å‡»ä¸»ç‰©ç†ç£ç›˜(ä¸æ˜¯ä¸‹è¾¹çš„åˆ†åŒº)</li>
<li>ç‚¹å‡»åˆ†åŒºæŒ‰é’®</li>
<li>When you see the pie chart, click on the OSXRESERVED partition in the pie chart</li>
<li>ç‚¹å‡»<code>-</code>æ ‡å¿—</li>
<li>click apply</li>
</ol>

<p>Thats it! </p>

<p><strong>DON&#39;T use the ERASE Button in Disk Utility! That will cause problems and it won&#39;t reallocate space back.</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/14</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MAC%20OS.html'>MAC OS</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14974276192148.html">
                
                  <h1>é‡æ–°åˆ©ç”¨boot campå®‰è£…win10</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>ç”±äºåŸæ¥ç»™win10åˆ†çš„ç£ç›˜å®¹é‡å¤ªå°,è€Œmacåˆä¸èƒ½åœ¨åç»­åŠ¨æ€ç»™winåˆ†åŒºå¢åŠ ç£ç›˜å®¹é‡,æ‰€ä»¥åªèƒ½é‡æ–°å®‰è£…åˆ†åŒºäº†.</p>

<ol>
<li>æ‰“å¼€macç³»ç»Ÿå·¥å…·ä¸­çš„ç£ç›˜<code>ç£ç›˜å·¥å…·</code>é€‰æ‹©æ€»çš„ç£ç›˜,ç‚¹å‡»ä¸Šæ–¹çš„<code>åˆ†åŒº</code>æŒ‰é’®,å°†ç³»ç»Ÿä¸­å…¶å®ƒåˆ†åŒºåˆ æ‰(é€šè¿‡ç‚¹å‡»é¥¼å›¾ä¸­çš„å¯¹åº”åŒºåŸŸ,ç‚¹å‡»ä¸‹æ–¹çš„<code>-</code>å·),å°±å¯ä»¥å°†åˆ†åŒºåˆ æ‰,åˆå¹¶åˆ°macç³»ç»Ÿç£ç›˜.å…¶ä»–åˆ†åŒºæ¯”å¦‚ä¹‹å‰å®‰è£…winäº§ç”Ÿçš„<code>OSXRESERVED Partition</code> </li>
<li>æ‰“å¼€bootcamp,ç‚¹å‡»æ¢å¤æŒ‰é’®,å°†ä¹‹å‰çš„winç³»ç»ŸæŠ¹æ‰,ä¹‹åå°±ä¼šå‘ç°,ä½ çš„macç³»ç»Ÿç£ç›˜å˜å›æ¥äº†.</li>
<li>é‡å¯ä¸€ä¸‹,ä¹‹åæ‰“å¼€bootcamp,ç‚¹å‡»ä¸‹æ–¹çš„<code>ç»§ç»­</code>æŒ‰é’®,é€‰æ‹©ç³»ç»Ÿisoé•œåƒ,ä¸ºwinç³»ç»Ÿé€‰æ‹©åˆ†åŒºå¤§å°(è¿™å›åˆ†å¤šäº›ğŸ˜“).</li>
<li>ç‚¹å‡»ç¡®å®š,ç­‰å¾…ä¸‹è½½Windowsæ”¯æŒè½¯ä»¶.
<img src="media/14974276192148/14974281373736.jpg" alt=""/></li>
<li>å®‰è£…ä¹‹å,åœ¨OSRESEVERåˆ†åŒºä¸­,çš„bootcampæ–‡ä»¶å¤¹ä¸­æ‰“å¼€å®‰è£…é©±åŠ¨çš„ç¨‹åº.</li>
</ol>

<p>åˆ°æ­¤win10 åº”è¯¥å°±å®‰è£…å¥½äº†~</p>

<h2 id="toc_0">æ³¨æ„</h2>

<p><strong>ç”±äºappleå…¬å¸æ›´æ–°äº†æœ€æ–°çš„æ–‡ä»¶ç³»ç»ŸAPFSï¼Œå¯¼è‡´bootcampå¹¶æ²¡æœ‰å¾ˆå¥½çš„æ”¯æŒã€‚ç°åœ¨ï¼Œwinä¸‹æ˜¯ä¸èƒ½è¯†åˆ«åˆ°macç³»ç»Ÿå¯åŠ¨ç¨‹åºçš„ã€‚</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/14</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='MAC%20OS.html'>MAC OS</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14970763342670.html">
                
                  <h1>caffeæ•°æ®ç»“æ„æè¿°</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>æ‰“å¼€caffeç›®å½•ä¸‹çš„<code>src/caffe/proto/caffe.proto</code>æ–‡ä»¶,é¦–å…ˆè®²çš„å°±æ˜¯Blobçš„æè¿°.</p>

<pre><code class="language-protobuf">// è¯¥ç»“æ„æè¿°äº† Blobçš„å½¢çŠ¶ä¿¡æ¯
message BlobShape {
  repeated int64 dim = 1 [packed = true];  //åªåŒ…æ‹¬è‹¥å¹²int64ç±»å‹å€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºBlobæ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚packedè¡¨ç¤ºè¿™äº›å€¼åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒï¼Œæ²¡æœ‰ç©ºæ´
}

//è¯¥ç»“æ„æè¿°Blobåœ¨ç£ç›˜ä¸­åºåˆ—åŒ–åçš„å½¢æ€
message BlobProto {
  optional BlobShape shape = 7;    //å¯é€‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªBlobShapeå¯¹è±¡
  repeated float data = 5 [packed = true]; // //åŒ…æ‹¬è‹¥åƒæµ®ç‚¹å…ƒç´ ï¼Œå­˜å‚¨æ•°æ®æˆ–æƒå€¼ï¼Œå…ƒç´ æ•°ç›®ç”±shapeæˆ–ï¼ˆnum, channels, height, width)ç¡®å®šï¼Œè¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­ç´§å¯†æ’å¸ƒ.
  repeated float diff = 6 [packed = true];  ////åŒ…æ‹¬è‹¥å¹²æµ®ç‚¹å…ƒç´ ï¼Œç”¨äºå­˜å‚¨å¢é‡ä¿¡æ¯ï¼Œç»´åº¦ä¸data æ•°ç»„ä¸€è‡´
  repeated double double_data = 8 [packed = true];  // ä¸ dataå¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸ºdouble
  repeated double double_diff = 9 [packed = true];  // ä¸ diff å¹¶åˆ—ï¼Œåªæ˜¯ç±»å‹ä¸º double

  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}
</code></pre>

<p><strong>è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨protobufferä¸»è¦æ˜¯å› ä¸ºå®ƒå…·æœ‰å¾ˆå¥½çš„å¥å£®æ€§,å°†ç¼–ç¨‹æœ€å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹åŠ ä»¥éšè—ï¼Œè®©æœºå™¨è‡ªåŠ¨å¤„ç†.</strong></p>

<h2 id="toc_0">Blobçš„æ„æˆ</h2>

<p>Blobæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»,å£°æ˜åœ¨<code>include/caffe/blob.hppä¸­</code>,é‡Œé¢å°è£…äº†ä¸€äº›åŸºæœ¬çš„Layer,Net,Solverç­‰,è¿˜æœ‰syncedmemç±»:</p>

<pre><code class="language-C++">
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;//ç”±protocç”Ÿæˆçš„å¤´æ–‡ä»¶ï¼Œå£°æ˜äº† BlobProtoã€BlobShapeç­‰éµå¾ªcaffe.protoåè®®çš„æ•°æ®ç»“æ„ å¯ä»¥åœ¨src/caffe/protoæ–‡ä»¶ä¸‹è¿è¡Œprotoc caffe.proto --cpp_out=./å‘½ä»¤ç”Ÿæˆè¯¥å¤´æ–‡ä»¶.
#include &quot;caffe/syncedmem.hpp&quot;  //CPU/GPUå…±äº«å†…å­˜ç±»ï¼Œç”¨äºæ•°æ®åŒæ­¥

const int kMaxBlobAxes = 32;    //Blobæœ€å¤§ç»´æ•°ç›®
template &lt;typename Dtype&gt;
class Blob {    //ç±»å£°æ˜
 public:
    //é»˜è®¤æ„é€ å‡½æ•°
  Blob()
       : data_(), diff_(), count_(0), capacity_(0) {}
    //æ˜¾å¼æ„é€ å‡½æ•°
  explicit Blob(const int num, const int channels, const int height, const int width);
  explicit Blob(const vector&lt;int&gt;&amp; shape);

 //å˜å½¢å‡½æ•°ï¼ŒæŠ¥æ®è¾“å…¥å‚æ•°é‡æ–°è®¾ç½®å½“å‰Blobå½¢çŠ¶,å¿…è¦æ—¶é‡æ–°åˆ†é…å†…å­˜
  void Reshape(const int num, const int channels, const int height,
      const int width);
  
  void Reshape(const vector&lt;int&gt;&amp; shape);
  void Reshape(const BlobShape&amp; shape);
  void ReshapeLike(const Blob&amp; other);
  //å¾—åˆ°Blobå½¢çŠ¶å­—ç¬¦ä¸²ç”¨äºæ‰“å°log,è§Caffeè¿è¡Œlog,ç±»ä¼¼&quot;Top shape: 100 1 28 28 (78400)&quot;
  inline string shape_string() const {
    ostringstream stream;
    for (int i = 0; i &lt; shape_.size(); ++i) {
      stream &lt;&lt; shape_[i] &lt;&lt; &quot; &quot;;
    }
    stream &lt;&lt; &quot;(&quot; &lt;&lt; count_ &lt;&lt; &quot;)&quot;;
    return stream.str();
  }
  //è¿”å›Blobå½¢çŠ¶
  inline const vector&lt;int&gt;&amp; shape() const { return shape_; }
    //è¿”å›æŸ1ç»´åº¦çš„å°ºå¯¸
  inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  //è¿”å›ç»´åº¦æ•°ç›®
  inline int num_axes() const { return shape_.size(); }
  //è¿”å›Blobä¸­å…ƒç´ æ€»æ•°
  inline int count() const { return count_; }
    //è¿”å›Blobä¸­æŸå‡ ç»´å­é›†çš„å…ƒç´ æ€»æ•°
    inline int count(int start_axis, int end_axis) const {
    CHECK_LE(start_axis, end_axis); //ä¿è¯ start_axis &lt;= end_axis
    CHECK_GE(start_axis, 0);  // ä¿è¯ start_axis &gt;= 0
    CHECK_GE(end_axis, 0);      // ä¿è¯ end_axis &gt;= 0
    CHECK_LE(start_axis, num_axes()); //ä¿è¯start_axis    &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    CHECK_LE(end_axis, num_axes()); //ä¿è¯end_axis &lt;=æ€»çš„ç»´åº¦æ•°ç›®
    int count = 1;
    for (int i = start_axis; i &lt; end_axis; ++i) {
      count *= shape(i);
    }
    return count;
  }
  //è®¡ç®—ä»æŸä¸€ç»´åº¦å¼€å§‹çš„å…ƒç´ æ€»æ•°
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
  //è½¬æ¢åæ ‡è½´ç´¢å¼•[-N,N)ä¸ºæ™®é€šç´¢å¼•[0,N)
  inline int CanonicalAxisIndex(int axis_index) const {
    CHECK_GE(axis_index, -num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    CHECK_LT(axis_index, num_axes())
        &lt;&lt; &quot;axis &quot; &lt;&lt; axis_index &lt;&lt; &quot; out of range for &quot; &lt;&lt; num_axes()
        &lt;&lt; &quot;-D Blob with shape &quot; &lt;&lt; shape_string();
    if (axis_index &lt; 0) {
    //è´Ÿç´¢å¼•è¡¨ç¤ºä»åå‘å‰è®¿é—®ï¼Œ-1è¡¨ç¤ºæœ€åä¸€ä¸ªä¸ªå…ƒç´ ï¼Œæ™®é€šç´¢å¼•å€¼ä¸º N-1:åŒç†ï¼Œ-2 =&gt; N-2, -3 =&gt; N-3,â€¦
      return axis_index + num_axes();
    }
    return axis_index;
  }
  //è·å–æŸä¸€ç»´çš„å°ºå¯¸
  /// @brief Deprecated legacy shape accessor num: use shape(0) instead.
  inline int num() const { return LegacyShape(0); }
  /// @brief Deprecated legacy shape accessor channels: use shape(1) instead.
  inline int channels() const { return LegacyShape(1); }
  /// @brief Deprecated legacy shape accessor height: use shape(2) instead.
  inline int height() const { return LegacyShape(2); }
  /// @brief Deprecated legacy shape accessor width: use shape(3) instead.
  inline int width() const { return LegacyShape(3); }
  inline int LegacyShape(int index) const {
    CHECK_LE(num_axes(), 4)
        &lt;&lt; &quot;Cannot use legacy accessors on Blobs with &gt; 4 axes.&quot;;
    CHECK_LT(index, 4);
    CHECK_GE(index, -4);
    if (index &gt;= num_axes() || index &lt; -num_axes()) {
      // Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse
      // indexing) -- this special case simulates the one-padding used to fill
      // extraneous axes of legacy blobs.
      return 1;
    }
    return shape(index);
  }
  //ä¸‹é¢çš„æ˜¯è®¡ç®—åç§»é‡çš„å‡½æ•°
  inline int offset(const int n, const int c = 0, const int h = 0,
      const int w = 0) const {
    CHECK_GE(n, 0);
    CHECK_LE(n, num());
    CHECK_GE(channels(), 0);
    CHECK_LE(c, channels());
    CHECK_GE(height(), 0);
    CHECK_LE(h, height());
    CHECK_GE(width(), 0);
    CHECK_LE(w, width());
    return ((n * channels() + c) * height() + h) * width() + w;
  }

  inline int offset(const vector&lt;int&gt;&amp; indices) const {
    CHECK_LE(indices.size(), num_axes());
    int offset = 0;
    for (int i = 0; i &lt; num_axes(); ++i) {
      offset *= shape(i);
      if (indices.size() &gt; i) {
        CHECK_GE(indices[i], 0);
        CHECK_LT(indices[i], shape(i));
        offset += indices[i];
      }
    }
    return offset;
  }
  //æŒ‰å€¼æ‹·è´Blobåˆ°å½“å‰Blob
  void CopyFrom(const Blob&lt;Dtype&gt;&amp; source, bool copy_diff = false, bool reshape = false);
  
  //ä¸‹é¢å‡ ä¸ªå‡½æ•°æ˜¯å­˜å–å™¨(getter/setter)
  inline Dtype data_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_data()[offset(n, c, h, w)];
  }

  inline Dtype diff_at(const int n, const int c, const int h,
      const int w) const {
    return cpu_diff()[offset(n, c, h, w)];
  }

  inline Dtype data_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_data()[offset(index)];
  }

  inline Dtype diff_at(const vector&lt;int&gt;&amp; index) const {
    return cpu_diff()[offset(index)];
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;
  }

  inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;
  }
  
  //åªè¯»è®¿é—®cpu_date
  const Dtype* cpu_data() const;
  //è®¾ç½®cpu_date
  void set_cpu_data(Dtype* data);
  const int* gpu_shape() const;
  //åªè¯»è®¿é—®gpu_date
  const Dtype* gpu_data() const;
  //è®¾ç½®gpu_date
  void set_gpu_data(Dtype* data);
  //åªè¯»è®¿é—®cpu_diff
  const Dtype* cpu_diff() const;
  //åªè¯»è®¿é—®gpu_diff
  const Dtype* gpu_diff() const;
  //ä¸‹é¢å››ä¸ªæ˜¯è¯»å†™è®¿é—®æ•°æ®
  Dtype* mutable_cpu_data();
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  void Update();    //Blobæ›´æ–°è¿ç®—ï¼Œå¯ç®€å•ç†è§£ä¸ºdataä¸diffçš„mergeè¿‡ç¨‹
  //ååºåˆ—åŒ–å‡½æ•°ï¼Œä»BlobProtoä¸­æ¢å¤ä¸ªBlobå¯¹è±¡
  void FromProto(const BlobProto&amp; proto, bool reshape = true);
  //åºåˆ—åŒ–å‡½æ•°ï¼Œå°†å†…å­˜ä¸­çš„Blobå¯¹è±¡ä¿å­˜åˆ°BlobProtoä¸­
  void ToProto(BlobProto* proto, bool write_diff = false) const;

  /// @brief Compute the sum of absolute values (L1 norm) of the data.
  Dtype asum_data() const;
  /// @brief Compute the sum of absolute values (L1 norm) of the diff.
  Dtype asum_diff() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the data.
  Dtype sumsq_data() const;
  /// @brief Compute the sum of squares (L2 norm squared) of the diff.
  Dtype sumsq_diff() const;

/// @brief Scale the blob data by a constant factor.
  void scale_data(Dtype scale_factor);
  /// @brief Scale the blob diff by a constant factor.
  void scale_diff(Dtype scale_factor);
 // å…±äº«å¦ä¸€ä¸ª Blob çš„ diff
  void ShareData(const Blob&amp; other);
  void ShareDiff(const Blob&amp; other);
  
  protected:
  shared_ptr&lt;SyncedMemory&gt; data_;   //å­˜æ”¾æŒ‡å‘dataçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; diff_;   //å­˜æ”¾æŒ‡å‘diffçš„æŒ‡é’ˆ
  shared_ptr&lt;SyncedMemory&gt; shape_data_; 
  vector&lt;int&gt; shape_;   //å½¢çŠ¶ä¿¡æ¯
  int count_;   //å­˜æ”¾æœ‰æ•ˆå…ƒç´ æ•°ç›®ä¿¡æ¯
  int capacity_;    //å­˜æ”¾Blobå®¹å™¨çš„å®¹é‡ä¿¡æ¯

  DISABLE_COPY_AND_ASSIGN(Blob);    //ç¦ç”¨æ‹·è´æ„é€ å‡½æ•°ã€é™šå€¼è¿ç®—ç¬¦é‡è½½
};  // class Blob

</code></pre>

<p><strong>æ³¨æ„åˆ°Caffeç±»ä¸­æˆå‘˜å˜é‡åéƒ½å¸¦æœ‰åç¼€ï¼Œè¿™æ ·åœ¨å‡½æ•°å®ç°ä¸­å®¹æ˜“åŒºåˆ†ä¸´æ—¶å˜é‡å’Œç±»æˆå‘˜å˜é‡ã€‚</strong></p>

<p>æ‰“å¹µ<code>include/caffe/syncedmem.hpp</code>ï¼ŒæŸ»çœ‹è¯¥ç±»çš„ç”¨æ³•:</p>

<pre><code class="language-c++">#ifndef CAFFE_SYNCEDMEM_HPP_
#define CAFFE_SYNCEDMEM_HPP_

#include &lt;cstdlib&gt;

#ifdef USE_MKL
  #include &quot;mkl.h&quot;
#endif

#include &quot;caffe/common.hpp&quot;

namespace caffe {

//å¦‚æœåœ¨GPUæ¨¡å¼ï¼Œä¸”CUDAä½¿èƒ½ï¼Œé‚£ä¹ˆä¸»æœºå†…å­˜ä¼šä»¥é¡µé”å®šå†…å­˜æ–¹å¼åˆ†é…ï¼ˆä½¿ç”¨cudaMallocHostUå‡½æ•°ã€‚å¯¹f-å•GPUçš„æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œä½†å¤šGPUä¼šéå¸¸æ˜æ˜¾)
inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) {
#ifndef CPU_ONLY
  if (Caffe::mode() == Caffe::GPU) {
    CUDA_CHECK(cudaMallocHost(ptr, size));
    *use_cuda = true;
    return;
  }
#endif
#ifdef USE_MKL
  *ptr = mkl_malloc(size ? size:1, 64);
#else
  *ptr = malloc(size);
#endif
  *use_cuda = false;
  CHECK(*ptr) &lt;&lt; &quot;host allocation of size &quot; &lt;&lt; size &lt;&lt; &quot; failed&quot;;
}
// ä¸CaffeMallocHostå¯¹åº”
inline void CaffeFreeHost(void* ptr, bool use_cuda) {
#ifndef CPU_ONLY
  if (use_cuda) {
    CUDA_CHECK(cudaFreeHost(ptr));
    return;
  }
#endif
#ifdef USE_MKL
  mkl_free(ptr);
#else
  free(ptr);
#endif
}

//è¯¥ç±»è´Ÿè´£å­˜å‚¨åˆ†é…ä»¥åŠä¸»æœºå’Œè®¾å¤‡é—´åŒæ­¥
class SyncedMemory {
 public:
 //æ„é€ å‡½æ•°
  SyncedMemory();
  //æ˜¾å¼æ„é€ å‡½æ•°
  explicit SyncedMemory(size_t size);
  //ææ„å‡½æ•°
  ~SyncedMemory();
  const void* cpu_data();       //åªè¯»è·å–cpu data
  void set_cpu_data(void* data);    //è®¾ç½®cpu data
  const void* gpu_data();       //åªè¯»è·å–gpu data
  void set_gpu_data(void* data);    //è®¾ç½®gpu data
  void* mutable_cpu_data();     // è¯»å†™è·å– cpu data
  void* mutable_gpu_data();     // è¯»å†™è·å– gpu data
  //çŠ¶æ€æœºå˜é‡ï¼Œè¡¨ç¤º4ç§çŠ¶æ€ï¼šæœ¯åˆå§‹åŒ–ã€CPUæ•°æ®å¥‹æ•ˆã€GPUæ•°æ®æœ‰æ•ˆã€å·±åŒæ­¥
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  //è·å¾—å½“å‰çŠ¶æ€æœºå˜é‡å€¼
  SyncedHead head() { return head_; }
  //è·å¾—å½“å‰å­˜å‚¨ç©ºé—´å°ºå¯¸
  size_t size() { return size_; }

#ifndef CPU_ONLY
  void async_gpu_push(const cudaStream_t&amp; stream);
#endif

 private:
  void check_device();

  void to_cpu();    //æ•°æ®åŒæ­¥è‡³CPU
  void to_gpu();    //æ•°æ®åŒæ­¥è‡³GPU
  void* cpu_ptr_;   //ä½äºCPUçš„æ•°æ®æŒ‡é’ˆ
  void* gpu_ptr_;   //ä½äºGPUçš„æ•°æ®æŒ‡é’ˆ
  size_t size_;     //å­˜å‚¨ç©ºé—´å¤§å°
  SyncedHead head_; //çŠ¶æ€æœºå˜é‡
  bool own_cpu_data_;   //æ ‡å¿—æ˜¯å¦æ‹¥æœ‰CPUæ•°æ®æ‰€æœ‰æƒï¼ˆå¦ï¼Œå³ä»åˆ«çš„å¯¹è±¡å…±äº«)
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;   ////æ ‡å¿—æ˜¯å¦æ‹¥æœ‰GPUæ•°æ®æ‰€æœ‰æƒ
  int device_;      //è®¾å¤‡å·

  DISABLE_COPY_AND_ASSIGN(SyncedMemory);
};  // class SyncedMemory

}  // namespace caffe

#endif  // CAFFE_SYNCEDMEM_HPP_

</code></pre>

<p>Blobç±»å®ç°çš„æºç ä½äº<code>src/caffe/blob.cpp</code>ä¸­ï¼Œå†…å®¹å¦‚ä¸‹:</p>

<pre><code class="language-c++">
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {
//å˜ç»´å‡½æ•°ï¼Œå°†ï¼ˆnum, channels, height, width}å‚æ•°è½¬æ¢ä¸ºvector&lt;int&gt;ï¼Œç„¶åè°ƒç”¨é‡è½½çš„å˜ç»´å‡½æ•°void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape)
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
//çœŸæ­£å˜ç»´å‡½æ•°
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes); //ä¿è¯vectorç»´åº¦&lt;=kMaxBlobAxes
  count_ = 1;   //ç”¨äºè®¡ç®—å…ƒç´ æ€»æ•°=num * channels * height * width 
  shape_.resize(shape.size());  //æˆå‘˜å˜é‡ç»´åº¦ä¹Ÿè¢«é‡ç½
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);  // ä¿è¯æ¯ç»´åº¦å°ºå¯¸éƒ½&gt;=0
    if (count_ != 0) {
    //è¯count_ä¸æº¢å‡º
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];     //count_ç´¯ä¹˜
    shape_[i] = shape[i];   //ä¸ºæˆå‘˜å˜é‡èµ‹å€¼
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {     //å¦‚æœæ–°çš„count_å¤§äºå½“å‰å·±åˆ†fé…ç©ºé—´å®¹é‡
    capacity_ = count_;         //æ‰©å®¹ï¼Œé‡æ–°åˆ†é…data_å’Œdif f_ç©ºé—´
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

//void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) å’Œvoid Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)ä¸ä¸Šé¢ç±»ä¼¼. 

//æ„é€ å‡½æ•°
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  // è°ƒç”¨Reshapeä¹‹å‰å¿…é¡»åˆå§‹åŒ–capacity_ï¼Œå¦åˆ™ä¼šå¯¼è‡´ä¸å¯é¢„æœŸç»“æœ
  : capacity_(0) {
  Reshape(num, channels, height, width);
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(shape);
}

template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}
//åªè¯»è·å–cpu dateæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);     //ä¿è¯data_ä¸ä¸º NULL
  return (const Dtype*)data_-&gt;cpu_data();
}
//ä¿®æ”¹cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}
//åªè¯»è·å–cpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}
//åªè¯»è·å–gpu_diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}
//è¯»å†™è®¿é—®cpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}
//è¯»å†™è®¿é—®gpu dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}
//ä¸ä¸Šé¢ç›¸åŒ
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„dataæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}
//å…±äº«å¦ä¸€ä¸ªBlobçš„diffæŒ‡é’ˆ
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}
//Update()å‡½æ•°ç”¨äºç½‘ç»œå‚æ•°Blobçš„æ›´æ–°ã€‚å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  // We will perform update based on where the data is located.dataåœ¨å“ªé‡Œæˆ‘ä»¬å°±åœ¨é‚£é‡Œæ›´æ–°
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:       //dataä½äºcpuç«¯
    // æ‰§è¡ŒCPUè®¡ç®—
        caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:   //dataä½äºGPUç«¯,æˆ–è€…CPU/GPUå·²ç»åŒæ­¥
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // æ‰§è¡Œ CPU ä¸Šçš„è®¡ç®—ï¼Œdata_[iã€‘=data_[i] - diff_[i], i = 0,1,2,â€¦ï¼Œcount_-1
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;     //ç¼–æ³½æ—¶æ‰“å¼€äº†CPU_ONLYé€‰é¡¹ï¼Œé‚£ä¹ˆGPUæ¨¡å¼ç¦ç”¨
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
  }
}
//è®¡ç®—data_çš„L1-èŒƒæ•°,å…¶ä¸­intå’Œunsigned intç±»å‹å¤„ç†å¹¶æœªå®ç°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());  //æ‰§è¡ŒCPUä¸Šçš„asumè®¡ç®—
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
//åŒä¸Š,è®¡ç®—diff_çš„L1èŒƒæ•°
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
  return 0;
}
//è®¡ç®—data_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);  //æ‰§è¡Œ CPUä¸Šçš„dotè®¡ç®—
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//åŒä¸Š,è®¡ç®—diff_çš„L2-èŒƒæ•°
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
//å¯¹data_è¿›è¡Œå¹…åº¦ç¼©æ”¾
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:   //æ‰§è¡ŒCPUä¸Šçš„è®¡ç®—
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}
//å¯¹diff_è¿›è¡Œç¼©æ”¾,åŒç†
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
}
//åˆ¤æ–­å½¢çŠ¶æ˜¯å¦ç›¸åŒ
template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    // Using deprecated 4D Blob dimensions --
    // shape is (num, channels, height, width).
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.
    // methods as these index from the beginning of the blob shape, where legacy parameter blobs were indexed from the end of the blob shape (e.g., bias Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).
    //è¾“å…¥çš„ç»´åº¦è‹¥ä½¿ç”¨è¿‡æ—¶çš„ç»´åº¦ä¿¡æ¯ï¼ˆnum, channels,height, width)ï¼Œåˆ™éœ€è¦è½¬æ¢ä¸ºæ–°çš„vectorå‚æ•°,ä»£ç ä½¿ç”¨äº†C++ä¸­çš„â€œæ‡’â€é€»è¾‘
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  //ç›´æ¥å¯¹æ¯”
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}
//ä»å¦ä¸€ä¸ªBlobå¯¹è±¡æ‹·è´data (å¯é€‰diff),å¿…è¦æ—¶è¿›è¡Œå˜ç»´
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);      //å¦‚æœè¦å˜ç»´,åˆ™æ‰§è¡Œè¿™ä¸ª
    } else {    //ä¸¤ä¸ªblobå½¢çŠ¶ä¸åŒ,åˆ™æŠ¥é”™
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:      //GPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:      //CPUæ¨¡å¼
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

//ä»BlobProtoä¸­åŠ è½½ä¸€ä¸ªBlob,é€‚ç”¨äºä»ç£ç›˜è½½å…¥ä¹‹å‰å¯¼å‡ºçš„Blob
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {        //ä»BlobProtoå¯¹è±¡ä¸­è·å¾—æ‰€éœ€å„ä¸ªç»´åº¦ä¿¡æ¯
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      // Using deprecated 4D Blob dimensions --
      // shape is (num, channels, height, width).
      shape.resize(4);
      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }
    Reshape(shape);     //BlobæŒ‰ç…§ç»´åº¦ä¿¡æ¯è¿›è¡Œå˜ç»´
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
  }
  // copy data åŠ è½½æ•°æ®
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯doubleç±»å‹ data
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);   //åŠ è½½double date
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);  //å¦åˆ™åŠ è½½float data
    }
  }
  if (proto.double_diff_size() &gt; 0) {   // å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯ double ç±»å‹ diff
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}
//å°†Blobä¸­çš„data(å¯é€‰diff)å¯¼å‡ºåˆ°BlobProtoç»“æ„ä½“.ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜æ–‡ä»¶ä¸­
template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();     //é‡ç½®protoçš„ç»´åº¦,ä¿è¯ä¸blobç›¸åŒ
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();   //æ¸…é™¤data
  proto-&gt;clear_double_diff();   //æ¸…é™¤diff
  const double* data_vec = cpu_data();  //å°†dataå¯¼å‡ºåˆ°proto
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {         //  è‹¥æœ‰write_diffçš„éœ€æ±‚
    const double* diff_vec = cpu_diff();    //å°†diffå¯¼å‡ºåˆ°proto
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}
//åŒä¸Š,åªä¸è¿‡ç±»å‹ä¸ºfloat
template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}
//å®ä¾‹åŒ–Blob   ç±»æ¨¡æ¿ï¼ˆfloat, double)
INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;

}  // namespace caffe

</code></pre>

<p><strong>åˆ°æ­¤,æˆ‘ä»¬å°±äº†è§£äº†Caffeä¸€äº›åŸºæœ¬çš„æ•°æ®ç»“æ„.åé¢å°±åº”è¯¥å­¦ä¹ Layerå±‚ä¸­å¯¹æ•°æ®çš„ä¸€äº›å¤„ç†.</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Caffe%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html'>Caffe æ•°æ®ç»“æ„</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14969765744787.html">
                
                  <h1>caffeæ•°æ®ç»“æ„</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Blob</a>
</li>
<li>
<a href="#toc_1">Blobçš„åŸºæœ¬ç”¨æ³•</a>
</li>
</ul>


<p>ä¸€ä¸ªCNNç½‘ç»œæ˜¯ç”±å¤šä¸ªLayerå †å è€Œæˆçš„.å¦‚å›¾æ‰€ç¤º:<br/>
<img src="media/14966518170972/14966520513325.jpg" alt=""/></p>

<p>caffeæŒ‰ç…§æˆ‘ä»¬è®¾è®¡çš„å›¾çº¸(prototxt),ç”¨Blobè¿™äº›ç –å—å»ºæˆä¸€å±‚å±‚(Layer)æ¥¼æˆ¿,æœ€åé€šè¿‡æ–¹æ³•SGDæ–¹æ³•(Solver)è¿›è¡Œç®€è£…ä¿®(Train),ç²¾è£…ä¿®(Finetune)å®ç°çš„.æˆ‘ä»¬è¿™é‡Œå°±æ˜¯å­¦ä¹ è¿™äº›åŸºæœ¬æ¦‚å¿µ.</p>

<h2 id="toc_0">Blob</h2>

<p>Caffeä½¿ç”¨ç§°ä¸ºBlobçš„4ç»´æ•°ç»„ç”¨äºå­˜å‚¨å’Œäº¤æ¢æ•°æ®.Blobæä¾›äº†ç»Ÿä¸€çš„å­˜å‚¨å™¨æ¥å£,æŒæœ‰ä¸€æ‰¹å›¾åƒæˆ–å…¶å®ƒæ•°æ®,æƒå€¼,æƒå€¼æ›´æ–°å€¼. å…¶å®ƒæœºå™¨å­¦ä¹ æ¡†æ¶ä¹Ÿæœ‰ç±»ä¼¼çš„æ•°æ®ç»“æ„.</p>

<p><strong>Blobåœ¨å†…å­˜ä¸­ä¸º4ç»´æ•°ç»„,åˆ†åˆ«ä¸º<code>(width_,height_,channels_,num_)</code>,width_å’Œheight_è¡¨ç¤ºå›¾åƒçš„å®½å’Œé«˜,channel_è¡¨ç¤ºé¢œè‰²é€šé“RGB,num_è¡¨ç¤ºç¬¬å‡ å¸§,ç”¨äºå­˜å‚¨æ•°æ®æˆ–æƒå€¼(data)å’Œæƒå€¼å¢é‡(diff),åœ¨è¿›è¡Œç½‘è·¯è®¡ç®—æ—¶,æ¯å±‚çš„è¾“å…¥,è¾“å‡ºéƒ½éœ€è¦Blobå¯¹è±¡ç¼“å†².Blobæ˜¯Caffeçš„åŸºæœ¬å­˜å‚¨å•å…ƒ.</strong></p>

<h2 id="toc_1">Blobçš„åŸºæœ¬ç”¨æ³•</h2>

<p>Blobæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»,æ‰€ä»¥åˆ›å»ºå¯¹è±¡æ—¶éœ€è¦åˆ¶å®šæ¨¡æ¿å‚æ•°.æˆ‘ä»¬è¿™é‡Œå†™ä¸€ä¸ªç®€å•çš„æµ‹è¯•ç¨‹åº<code>blob_demo.cpp</code>å°†å®ƒæ”¾åœ¨<code>caffe</code>çš„å®‰è£…ç›®å½•ä¸‹:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>ä¸Šé¢ä»£ç é¦–å…ˆåˆ›å»ºäº†æ•´å‹Blobå¯¹è±¡a,æ‰“å°å…¶ç»´åº¦ä¿¡æ¯,ç„¶åè°ƒç”¨å…¶<code>Reshape()</code>æ–¹æ³•,å†æ¬¡æ‰“å°å…¶ç»´åº¦ä¿¡æ¯.</p>

<p>ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æ¥ç¼–è¯‘ä¸Šé¢çš„æ–‡ä»¶.</p>

<pre><code>g++ -o app blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe
</code></pre>

<p>ç”Ÿæˆäº†å¯æ‰§è¡Œç¨‹åº<code>app</code></p>

<p>è¿™ä¸ªæ—¶å€™è¿è¡Œ<code>app</code>çš„è¯å¯èƒ½ä¼šé‡åˆ°ä¸‹é¢è¿™ä¸ªé”™è¯¯:<br/>
<img src="media/14966518170972/14968048615065.jpg" alt=""/><br/>
è¿™ä¸ªå› ä¸º<code>app</code>æ²¡æœ‰é“¾æ¥åˆ°è¿™ä¸ªåŠ¨æ€åº“æ–‡ä»¶,æ‰§è¡Œä¸‹è¾¹è¿™ä¸ªå‘½ä»¤é“¾æ¥:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app
</code></pre>

<p><code>/usr/local/Cellar/caffe/build/lib/</code>ä¸º<code>@rpath/libcaffe.so.1.0.0</code>åŠ¨æ€åº“çš„è·¯å¾„.</p>

<p>æ‰§è¡Œå,å†æ¬¡è¿è¡Œä¼šé‡åˆ°é”™è¯¯:<br/>
<img src="media/14966518170972/14968049981988.jpg" alt=""/></p>

<p>ä¸ä¸Šé¢ç±»ä¼¼,è¿™æ˜¯å› ä¸ºæ²¡æœ‰é“¾æ¥åˆ°<code>@rpath/libhdf5_hl.10.dylib</code><br/>
æ‰§è¡Œä¸‹é¢è¿™ä¸ªå‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/Users/liangzhonghao/anaconda2/lib&#39;  /usr/local/Cellar/caffe/build/lib/libcaffe.so.1.0.0
</code></pre>

<p>å…¶ä¸­<code>/Users/liangzhonghao/anaconda2/lib</code>åŒ…å«è¿™ä¸ªåº“æ–‡ä»¶.</p>

<p>å†æ¬¡æ‰§è¡Œapp,ç»ˆäºæˆåŠŸäº†!<br/>
<img src="media/14966518170972/14968051024802.jpg" alt=""/></p>

<p>åˆ›å»ºäº†Blobå¯¹è±¡ä¹‹å,æˆ‘ä»¬å¯ä»¥é€šè¿‡<code>mutable_cpu[gpu]_data[diff]</code>å‡½æ•°æ¥ä¿®æ”¹å…¶å†…éƒ¨æ•°å€¼:</p>

<p>ä»£ç ä¸º:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    for(int i=0;i&lt;a.count();i++){
        p[i]=i;
    }
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    return 0;
}
</code></pre>

<p>è·Ÿä¸Šé¢ä¸€æ ·ç»§ç»­ç¼–è¯‘å’Œæ‰§è¡Œ,è¿™é‡ŒæŒ‰ç…§ä¸Šé¢çš„å‘½ä»¤ç»§ç»­æ¥ç¼–è¯‘çš„è¯,é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯:<br/>
<img src="media/14966518170972/14968061663863.jpg" alt=""/></p>

<p>ä¹‹åæ¢æˆä¸‹è¾¹çš„å‘½ä»¤æ‰§è¡ŒåæˆåŠŸ:</p>

<pre><code>g++ -o app2 blob_demo.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe -lglog -lboost_system -lprotobuf
</code></pre>

<p>å·®åˆ«åœ¨äº,åè¾¹åŠ ä¸Šäº†<code>-lglog -lboost_system -lprotobuf</code>å‘½ä»¤,å…·ä½“ä½œç”¨åç»­å°†ç ”ç©¶(æš‚æ—¶ä¸ç†è§£),ç»§ç»­è¿è¡Œå,åˆå‡ºç°äº†é”™è¯¯:<br/>
<img src="media/14966518170972/14968063024883.jpg" alt=""/></p>

<p>åŒæ ·æ˜¯åŠ¨æ€åº“çš„è¿æ¥é—®é¢˜:<br/>
è¿è¡Œå‘½ä»¤:</p>

<pre><code>install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/./app2
</code></pre>

<p>æ‰§è¡Œå‘½ä»¤,ç„¶åè¿è¡Œ<code>app2</code>.å¾—åˆ°è¾“å‡º:<br/>
<img src="media/14966518170972/14968063915684.jpg" alt=""/></p>

<p><strong>å¯è§,Blobä¸‹æ ‡çš„è®¿é—®ä¸c/c++é«˜ç»´æ•°ç»„å‡ ä¹ä¸€è‡´,è€ŒBlobå¥½å¤„åœ¨äºå¯ä»¥ç›´æ¥åŒæ­¥CPU/GPUä¸Šçš„æ•°æ®.</strong></p>

<p>Blobè¿˜æ”¯æŒè®¡ç®—æ‰€æœ‰å…ƒç´ çš„ç»å¯¹å€¼ä¹‹å’Œ(L1-èŒƒæ•°),å¹³æ–¹å’Œ(L2-èŒƒæ•°):</p>

<pre><code>cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
</code></pre>

<p>è¾“å‡ºç»“æœä¸º:</p>

<pre><code>ASUM = 276
SUMSQ = 4324

</code></pre>

<p>é™¤äº†data,æˆ‘ä»¬è¿˜å¯ä»¥æ”¹difféƒ¨åˆ†,ä¸dataçš„æ“ä½œåŸºæœ¬ä¸€è‡´:</p>

<pre><code class="language-c++">#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //å°†dataåˆå§‹åŒ–ä¸º1,2,3....
        q[i]= a.count()-1-i;   //å°†diffåˆå§‹åŒ–ä¸º23,22,21,...
    }
    
    a.Update();         //æ‰§è¡Œupdateæ“ä½œ,å°†diffä¸dataèåˆ,è¿™ä¹Ÿæ˜¯CNNæƒå€¼æ›´æ–°æ­¥éª¤çš„æœ€ç»ˆå®æ–½è€…
   
    for(int u=0;u&lt;a.num();u++){
        for(int v=0;v&lt;a.channels();v++){
            for(int w=0;w&lt;a.height();w++){
                for(int x=0;x&lt;a.width();x++){
                    cout&lt;&lt;&quot;a[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;a.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;a.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;a.sumsq_data()&lt;&lt;endl;
    
    return 0;
}

</code></pre>

<p>ç„¶åæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ç¼–è¯‘,é“¾æ¥åº“æ–‡ä»¶:</p>

<pre><code>g++ -o app blob_demo_diff.cpp -I /usr/local/Cellar/caffe/include/ -D CPU_ONLY -I /usr/local/Cellar/caffe/.build_release/src/ -L /usr/local/Cellar/caffe/.build_release/lib/ -lcaffe  -lglog -lboost_system -lprotobuf

install_name_tool -add_rpath &#39;/usr/local/Cellar/caffe/build/lib/&#39;  /usr/local/Cellar/caffe/LZHcaffe/./app
</code></pre>

<p><img src="media/14969765744787/14969953318311.jpg" alt=""/></p>

<p>è¿è¡Œ.app,ç»“æœä¸º:<br/>
<img src="media/14969765744787/14969953744774.jpg" alt=""/></p>

<p>ä¸Šé¢è¡¨æ˜,åœ¨<code>Update()</code>å‡½æ•°ä¸­,å®ç°äº†<code>data = data -diff</code>æ“ä½œ,è¿™ä¸ªä¸»è¦æ˜¯åœ¨CNNæƒå€¼æ›´æ–°æ—¶ä¼šç”¨åˆ°,åé¢ç»§ç»­å­¦ä¹ .</p>

<p>å°†Blobå†…éƒ¨å€¼ä¿å­˜åˆ°ç¡¬ç›˜,æˆ–è€…å†²ç¡¬ç›˜è½½å…¥åˆ°å†…å­˜,å¯ä»¥åˆ†åˆ«é€šè¿‡<code>ToProto(),FromProto()</code>å®ç°:</p>

<pre><code class="language-c++">
#include&lt;vector&gt;
#include&lt;iostream&gt;
#include&lt;caffe/blob.hpp&gt;
#include&lt;caffe/util/io.hpp&gt;   //éœ€è¦åŒ…å«è¿™ä¸ªå¤´æ–‡ä»¶
using namespace caffe;
using namespace std;
int main(void)
{
    Blob&lt;float&gt; a;
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    a.Reshape(1,2,3,4);
    cout&lt;&lt;&quot;Size:&quot;&lt;&lt;a.shape_string()&lt;&lt;endl;
    
    float *p=a.mutable_cpu_data();
    float *q=a.mutable_cpu_diff();
    
    for(int i=0;i&lt;a.count();i++){
        p[i]= i;     //å°†dataåˆå§‹åŒ–ä¸º1,2,3....
        q[i]= a.count()-1-i;   //å°†diffåˆå§‹åŒ–ä¸º23,22,21,...
    }
    
    a.Update();         //æ‰§è¡Œupdateæ“ä½œ,å°†diffä¸dataèåˆ,è¿™ä¹Ÿæ˜¯CNNæƒå€¼æ›´æ–°æ­¥éª¤çš„æœ€ç»ˆå®æ–½è€…
   
    BlobProto bp;          //æ„é€ ä¸€ä¸ªBlobProtoå¯¹è±¡
    a.ToProto(&amp;bp,true);    //å°†aåºåˆ—åŒ–,è¿åŒdiff(é»˜è®¤ä¸å¸¦)
    WriteProtoToBinaryFile(bp,&quot;a.blob&quot;);     //å†™å…¥ç£ç›˜æ–‡ä»¶&quot;a.blob&quot;
    BlobProto bp2;           //æ„é€ ä¸€ä¸ªæ–°çš„BlobProtoå¯¹è±¡
    ReadProtoFromBinaryFileOrDie(&quot;a.blob&quot;,&amp;bp2);    //è¯»å–ç£ç›˜æ–‡ä»¶
    Blob&lt;float&gt; b;          //æ–°å»ºä¸€ä¸ªBlobå¯¹è±¡b
    b.FromProto(bp2,true);  //ä»åºåˆ—åŒ–å¯¹è±¡bp2ä¸­å…‹éš†b(è¿åŒå½¢çŠ¶)
    
    for(int u=0;u&lt;b.num();u++){
        for(int v=0;v&lt;b.channels();v++){
            for(int w=0;w&lt;b.height();w++){
                for(int x=0;x&lt;b.width();x++){
                    cout&lt;&lt;&quot;b[&quot;&lt;&lt;u&lt;&lt;&quot;][&quot;&lt;&lt;w&lt;&lt;&quot;][&quot;&lt;&lt;x&lt;&lt;&quot;]=&quot;&lt;&lt;b.data_at(u,v,w,x)&lt;&lt;endl;
                }
            }
        }
    }
    
    cout&lt;&lt;&quot;ASUM = &quot;&lt;&lt;b.asum_data()&lt;&lt;endl;
    cout&lt;&lt;&quot;SUMSQ = &quot;&lt;&lt;b.sumsq_data()&lt;&lt;endl;
    
    
    return 0;
}

</code></pre>

<p>ç¼–è¯‘,è¿æ¥åº“æ–‡ä»¶å(æ³¨æ„ç¼–è¯‘æ—¶æœ«å°¾åŠ å…¥<code>&quot;-lglog -lboost_system -lprotobuf&quot;</code>é€‰é¡¹),è¾“å‡ºå¦‚ä¸‹:</p>

<p><img src="media/14969765744787/14969964533804.jpg" alt=""/></p>

<p>å¯ä»¥å‘ç°ä¸ä¸Šé¢æ²¡æœ‰å·®åˆ«,åªæ˜¯åœ¨æ–‡ä»¶å¤¹ä¸­å¤šäº†ä¸€ä¸ª<code>Blob.a</code>æ–‡ä»¶,<strong>æ‰€ä»¥<code>BlobProto</code>å¯¹è±¡å®ç°äº†ç¡¬ç›˜ä¸å†…å­˜ä¹‹é—´çš„æ•°æ®é€šä¿¡.å¯ä»¥å¸®åŠ©ä¿å­˜ä¸­é—´æƒå€¼å’Œæ•°æ®</strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14964511552671.html">
                
                  <h1>æ¿€æ´»å‡½æ•°</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>æ·±åº¦ç¥ç»ç½‘ç»œä¹‹æ‰€ä»¥å…·æœ‰ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›ï¼Œé™¤äº†æœ‰æ·±å±‚æ¬¡çš„ç½‘ç»œä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦å› ç´ å³éçº¿æ€§å¤„ç†å•å…ƒ,ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼ˆActivation Function)æˆ–æŒ¤å‹å‡½æ•°ï¼ˆSquashing Function).<strong>æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è¦å…³æ³¨æ€ä¹ˆåœ¨caffeä¸­å®ç°è¿™äº›å‡½æ•°.</strong></p>

<p>ä¸‹å›¾æ˜¯ä¸€ä¸ªç¥ç»å…ƒæ¨¡å‹.\(\varphi(.)\)ä¸ºæ¿€æ´»å‡½æ•°.ä¸»è¦ä½œç”¨æ˜¯å°†ä¸Šä¸€å±‚çš„è¾“å…¥çº¿æ€§ç»„åˆç»“æœ\(u_k\)åŠ¨æ€èŒƒå›´å‹ç¼©åˆ°ç‰¹å®šå€¼åŸŸ(ä¾‹å¦‚[-1,1]).ä¸€èˆ¬æ¥è¯´å…·å¤‡éçº¿æ€§å¤„ç†å•å…ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œ(å¤§äºç­‰äº3å±‚),ç†è®ºä¸Šå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°.<br/>
<img src="media/14964511552671/14964521741597.jpg" alt=""/></p>

<p>å…¶ä¸­å‡ ä¸ªå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°å¦‚ä¸‹:<br/>
1.Sigmoidå‡½æ•°,å€¼åŸŸä¸º(0,1)<br/>
\[<br/>
\varphi(x) = \frac{1}{1+e^{-ax}}<br/>
\]<br/>
<img src="media/14964511552671/14964523348860.jpg" alt=""/></p>

<p>2.tanhå‡½æ•°,å€¼åŸŸä¸º(-1,1):<br/>
\[<br/>
\varphi(x) = \frac{1-e^{-2x}}{1+e^{-2x}}<br/>
\]<br/>
<img src="media/14964511552671/14964526906602.jpg" alt=""/></p>

<p>3.ReLu(Rectified Linear Unitï¼Œè§„æ•´åŒ–çº¿æ€§å•å…ƒ)å‡½æ•°,å€¼åŸŸä¸º\([0,+ \infty)\),æ˜¯ä¸€ç§éé¥±å’Œæ¿€æ´»å‡½æ•°.<br/>
\[<br/>
\varphi(x) = max(0,x)<br/>
\]<br/>
<img src="media/14964511552671/14964530576767.jpg" alt=""/></p>

<p>è¿œä¸æ­¢ä¸Šé¢è¿™äº›æ¿€æ´»å‡½æ•°,éšç€å‘å±•,é™†ç»­åˆå‡ºç°äº†å¾ˆå¤šæ¿€æ´»å‡½æ•°.è¿™é‡Œä¸å¤šä»‹ç».åé¢è¿˜è¦è‡ªå­¦å¾ˆå¤šè¿™ç±»ç›¸å…³çŸ¥è¯†.</p>

<p>ç¥ç»ç½‘ç»œä¸­æœ€å¤§çš„é—®é¢˜æ˜¯æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆGradient Vanishing Problem),è¿™åœ¨ä½¿ç”¨ <code>Sigmoidã€tanh</code>ç­‰é¥±å’Œæ¿€æ´»å‡½æ•°æƒ…å†µä¸‹å°¤ä¸ºä¸¥é‡(ç¥ç»ç½‘ç»œè¿›è¡Œè¯¯å·®åå‘ä¼ æ’­æ—¶ï¼Œå„å±‚éƒ½è¦ä¹˜ä»¥æ¿€æ´»å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°\(G=e\cdot \varphi&#39;(x) \cdot x\)),æ¢¯åº¦æ¯ä¼ é€’ä¸€å±‚éƒ½ä¼šè¡°å‡ä¸€æ¬¡,ç½‘ç»œå±‚æ•°è¾ƒå¤šæ—¶,æ¢¯åº¦Gå°±ä¼šä¸åœçš„è¡°å‡è‡³æ¶ˆå¤±),ä½¿å¾—è®­ç»ƒç½‘ç»œæ—¶æ”¶æ•›ææ…¢,è€ŒReLUè¿™ç±»éé¥±å’Œæ¿€æ´»å‡½æ•°æ”¶æ•›é€Ÿåº¦å°±å¿«å¾ˆå¤š.æ‰€ä»¥å­¦ä¹ ç½‘ç»œæ¨¡å‹ä¸­ä¸€èˆ¬éƒ½ä¼šé€‰ç”¨ç±»ä¼¼ReLuè¿™ç§æ­»æ´»å‡½æ•°.</p>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬å­¦ä¹ åœ¨caffeç”¨ä»£ç å®ç°å¯¹åº”å±‚çš„è®¡ç®—,åŒ…æ‹¬å‰å‘ä¼ æ’­è®¡ç®—å’Œåå‘ä¼ æ’­è®¡ç®—.Caffeçš„æ‰€æœ‰ä¸æ¿€æ´»å‡½æ•°ç›¸å…³çš„Layerç±»å£°æ˜åœ¨<code>include/caffe/layers</code>æ–‡ä»¶å¤¹ä¸­åˆ†åˆ«ä¸º<code>sigmoid_layer.hpp,relu_layer.hpp,tanh_layer.hpp</code>,æˆ‘ä»¬å°†å®ƒä»¬ç»Ÿç§°ä¸º<strong>éçº¿æ€§å±‚</strong>,æˆ‘ä»¬é‡ç‚¹å…³æ³¨<code>ReLULayer,SigmoidLayerå’ŒTanHLayer</code>è¿™ä¸‰ç±».</p>

<p>åœ¨å‰é¢æˆ‘ä»¬æµ‹è¯•çš„LeNet-5æ¨¡å‹ä¸­ä½¿ç”¨äº†ReLuå±‚,æˆ‘ä»¬åœ¨<code>example/mnist/lenet_train_test.prototxt</code>ä¸­æ‰¾åˆ°æè¿°:</p>

<pre><code>layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
</code></pre>

<p>ä¸å·ç§¯å±‚ã€å…¨è¿æ¥å±‚æœ€å¤§çš„ä¸åŒ,å°±æ˜¯æ²¡æœ‰æƒå€¼ç›¸å…³çš„å‚æ•°ï¼Œæè¿°ç›¸å¯¹ç®€å•ã€‚å¦å¤–ä¸¤ç§å±‚æ²¡æœ‰å®é™…æ ·ä¾‹ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿè¿™æ—¶æŒ‰ç…§æˆ‘ä»¬çš„Caffeæºç é˜…è¯»æ–¹æ³•è®º.ä»<code>src/caffe/proto/caffe.proto</code>ä¸­è·å¾—çµæ„Ÿã€‚</p>

<pre><code class="language-c++">// ReLUå±‚å‚æ•°
message ReLUParameter {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  // Leaky ReLUå‚æ•°ï¼Œæˆ‘ä»¬æš‚ä¸å…³å¿ƒ
  optional float negative_slope = 1 [default = 0];
  enum Engine {     //è®¡ç®—å¼•æ“é€‰æ‹©
    DEFAULT = 0;
    CAFFE = 1;      // Caffe å®ç°
    CUDNN = 2;      // CUDNN å®ç°
  }
  optional Engine engine = 2 [default = DEFAULT];
}
</code></pre>

<pre><code class="language-c++">// Sigmoidå±‚å‚æ•°
message SigmoidParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}

</code></pre>

<pre><code class="language-c++">//  tanh å±‚å‚æ•°
message TanHParameter {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
}
</code></pre>

<p>éçº¿æ€§å±‚çš„å…±åŒç‰¹ç‚¹å°±æ˜¯å¯¹å‰ä¸€å±‚blobä¸­çš„æ•°å€¼é€ä¸€è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¹¶æ”¾å›åŸblobä¸­ã€‚æ¿€æ´»å‡½æ•°çš„ç±»å£°æ˜å¦‚ä¸‹:</p>

<pre><code class="language-c++">namespace caffe {
//éçº¿æ€§å±‚çš„é¼»ç¥–NeuronLayerï¼Œæ´¾ç”ŸäºLayerç±»ï¼Œç‰¹ç‚¹æ˜¯è¾“å‡ºblob(y)ä¸è¾“å…¥blob(x)å°ºå¯¸ç›¸åŒ

/**
 * @brief An interface for layers that take one blob as input (@f$ x @f$)
 *        and produce one equally-sized blob as output (@f$ y @f$), where
 *        each element of the output depends only on the corresponding input
 *        element.
 */
template &lt;typename Dtype&gt;
class NeuronLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit NeuronLayer(const LayerParameter&amp; param)
     : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline int ExactNumBottomBlobs() const { return 1; }
  virtual inline int ExactNumTopBlobs() const { return 1; }
};

}  // namespace caffe

#endif  // CAFFE_NEURON_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// ReLULayerï¼Œæ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†ReLuæ¿€æ´»å‡½æ•°è®¡ç®—

/**
 * @brief Rectified Linear Unit non-linearity @f$ y = \max(0, x) @f$.
 *        The simple max is fast to compute, and the function does not saturate.
 */
template &lt;typename Dtype&gt;
class ReLULayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
 
  /**
   * @param param provides ReLUParameter relu_param,
   *     with ReLULayer options:
   *   - negative_slope (\b optional, default 0).
   *     the value @f$ \nu @f$ by which negative values are multiplied.
   */
  explicit ReLULayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;ReLU&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \max(0, x)
   *      @f$ by default.  If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed outputs are @f$ y = \max(0, x) + \nu \min(0, x) @f$.
   */
   //å‰å‘ä¼ æ³¢å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the ReLU inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            0 &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$ if propagate_down[0], by default.
   *      If a non-zero negative_slope @f$ \nu @f$ is provided,
   *      the computed gradients are @f$
   *        \frac{\partial E}{\partial x} = \left\{
   *        \begin{array}{lr}
   *            \nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\
   *            \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0
   *        \end{array} \right.
   *      @f$.
   */
   
   //åå‘ä¼ æ³¢å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_RELU_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// SigmoidLayer,æ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†Sigmoidæ¿€æ´»å‡½æ•°çš„è®¡ç®—
/**
 * @brief Sigmoid function non-linearity @f$
 *         y = (1 + \exp(-x))^{-1}
 *     @f$, a classic choice in neural networks.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class SigmoidLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
  explicit SigmoidLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;Sigmoid&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = (1 + \exp(-x))^{-1}
   *      @f$
   */
   
   //å‰å‘ä¼ æ’­å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y} y (1 - y)
   *      @f$ if propagate_down[0]
   */
   
   //åå‘ä¼ æ’­å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_SIGMOID_LAYER_HPP_
</code></pre>

<pre><code class="language-c++">namespace caffe {
// TanHLayerï¼Œæ´¾ç”ŸäºNeuronLayerï¼Œå®ç°äº†tanhæ¿€æ´»å‡½æ•°è®¡ç®—
/**
 * @brief TanH hyperbolic tangent non-linearity @f$
 *         y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
 *     @f$, popular in auto-encoders.
 *
 * Note that the gradient vanishes as the values move away from 0.
 * The ReLULayer is often a better choice for this reason.
 */
template &lt;typename Dtype&gt;
class TanHLayer : public NeuronLayer&lt;Dtype&gt; {
 public:
 //æ˜¾å¼æ„é€ å‡½æ•°
  explicit TanHLayer(const LayerParameter&amp; param)
      : NeuronLayer&lt;Dtype&gt;(param) {}
//è¿”å›ç±»åå­—ç¬¦ä¸²
  virtual inline const char* type() const { return &quot;TanH&quot;; }

 protected:
  /**
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$
   * @param top output Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the computed outputs @f$
   *        y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
   *      @f$
   */
   
   //å‰å‘ä¼ æ’­å‡½æ•°
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the sigmoid inputs.
   *
   * @param top output Blob vector (length 1), providing the error gradient with
   *      respect to the outputs
   *   -# @f$ (N \times C \times H \times W) @f$
   *      containing error gradients @f$ \frac{\partial E}{\partial y} @f$
   *      with respect to computed outputs @f$ y @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 1)
   *   -# @f$ (N \times C \times H \times W) @f$
   *      the inputs @f$ x @f$; Backward fills their diff with
   *      gradients @f$
   *        \frac{\partial E}{\partial x}
   *            = \frac{\partial E}{\partial y}
   *              \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
   *            = \frac{\partial E}{\partial y} (1 - y^2)
   *      @f$ if propagate_down[0]
   */
   
   //åå‘ä¼ æ’­å‡½æ•°
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
};

}  // namespace caffe

#endif  // CAFFE_TANH_LAYER_HPP_
</code></pre>

<p>ä¸Šé¢ç±»çš„å£°æ˜æ¯”è¾ƒç®€å•,å„è‡ªå£°æ˜äº†Forwardå’ŒBackwardå‡½æ•°.ä¸‹é¢å¯¹è¿™äº›å‡½æ•°çš„å®ç°è¿›è¡Œè§£æ.æˆ‘ä»¬é¦–å…ˆçœ‹ä¸‹<code>src/caffe/layers/relu_layer.cpp</code>ä¸­å‰å‘ä¼ æ’­å‡½æ•°çš„å®ç°ä»£ç ã€‚</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
    // (åªè¯») è·å¾—è¾“äººblobçš„dataæŒ‡é’ˆ
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  // (è¯»å†™ï¼‰è·å¾—è¾“å‡ºblobçš„dataæŒ‡é’ˆ
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  //è·å¾—è¾“å…¥blobå…ƒç´ ä¸ªæ•°
  const int count = bottom[0]-&gt;count();
  // Leaky ReLUå‚æ•°ï¼Œä»layer_paramä¸­è·å¾—ï¼Œé»˜è®¤ä¸º0ï¼Œå³æ™®é€šReLU
  Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
  //æ‰§è¡ŒReLUæ“ä½œæˆ‘ä»¬å§‘ä¸”è®¤ä¸ºnegative_slopå€¼ä¸º0,ä¸è€ƒè™‘Leaky ReLU
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = std::max(bottom_data[i], Dtype(0))
        + negative_slope * std::min(bottom_data[i], Dtype(0));
  }
}
</code></pre>

<p>ä¸å‡ºæ‰€æ–™ï¼Œç”¨ä¸€å±‚forå¾ªç¯å°±æå®šäº†,ä¸‹é¢æˆ‘ä»¬æ¥çœ‹<strong>åå‘ä¼ æ’­å‡½æ•°</strong>çš„å®ç°ä»£ç .</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
void ReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
    // å¦‚æœéœ€è¦åšåå‘ä¼ æ’­è®¡ç®—
  if (propagate_down[0]) {
    //(åªè¯»ï¼‰è·å¾—å‰ä¸€å±‚çš„dataæŒ‡é’ˆ
    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
    //(åªè¯») è·å¾—åä¸€å±‚çš„diffæŒ‡é’ˆ
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    //(è¯»å†™) è·å¾—å‰ä¸€å±‚çš„diffæŒ‡é’ˆ
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    //è·å¾—è¦å‚è®¡ç®—çš„å…ƒç´ æ€»æ•°
    const int count = bottom[0]-&gt;count();
    // Leaky ReLUå‚æ•°ï¼Œå§‘ä¸”è®¤ä¸ºæ˜¯0
    Dtype negative_slope = this-&gt;layer_param_.relu_param().negative_slope();
    for (int i = 0; i &lt; count; ++i) {
    // ReLUçš„å¯¼å‡½æ•°å°±æ˜¯ï¼ˆbottom_data[i] &gt; 0)ï¼Œæ ¹æ®æ±‚å¯¼é“¾å¼æ³•åˆ™ï¼Œåä¸€å±‚çš„è¯¯å·®ä¹˜ä»¥å¯¼å‡½æ•°å¾—åˆ°å‰ä¸€å±‚çš„è¯¯å·®
      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)
          + negative_slope * (bottom_data[i] &lt;= 0));
    }
  }
}
</code></pre>

<p>åˆ°è¿™é‡Œå¯ä»¥çœ‹åˆ°ReLuè®¡ç®—éå¸¸ç®€å•(ç›®å‰å¦‚æ­¤)</p>

<p>å…¶å®ƒæ¿€æ´»å‡½æ•°æºç ,ä¹‹åä¹Ÿè®¸ç”¨çš„æ¯”è¾ƒå°‘,è¿™é‡Œä¸åšå¤šçš„ä»‹ç».</p>

<p>æ‰€ä»¥,éçº¿æ€§å±‚è™½ç„¶å…¬å¼è¡¨ç¤ºè¾ƒä¸ºå¤æ‚,ä½†ä»£ç å®ç°éƒ½éå¸¸ç®€æ´ã€ç›´è§‚ï¼Œåªè¦æŒæ¡äº†åŸºæœ¬æ±‚å¯¼æŠ€å·§ï¼ŒåŒæ ·å¯ä»¥æ¨å¯¼å‡ºéçº¿æ€§å±‚å…¶ä»–ç±»çš„åå‘ä¼ æ’­å…¬å¼.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/3</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='caffe%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0.html'>caffeæ¡†æ¶å­¦ä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14957648679184.html">
                
                  <h1>UDPç¼–ç¨‹</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>TCPæ˜¯å»ºç«‹å¯é è¿æ¥ï¼Œå¹¶ä¸”é€šä¿¡åŒæ–¹éƒ½å¯ä»¥ä»¥æµçš„å½¢å¼å‘é€æ•°æ®ã€‚ç›¸å¯¹TCPï¼ŒUDPåˆ™æ˜¯<strong>é¢å‘æ— è¿æ¥</strong>çš„åè®®ã€‚</p>

<p><font color=red>ä½¿ç”¨UDPåè®®æ—¶ï¼Œä¸éœ€è¦å»ºç«‹è¿æ¥ï¼Œåªéœ€è¦çŸ¥é“å¯¹æ–¹çš„IPåœ°å€å’Œç«¯å£å·ï¼Œå°±å¯ä»¥ç›´æ¥å‘æ•°æ®åŒ…ã€‚ä½†æ˜¯ï¼Œèƒ½ä¸èƒ½åˆ°è¾¾å°±ä¸çŸ¥é“äº†ã€‚</p>

<p>è™½ç„¶ç”¨UDPä¼ è¾“æ•°æ®ä¸å¯é ï¼Œä½†å®ƒçš„ä¼˜ç‚¹æ˜¯å’ŒTCPæ¯”ï¼Œé€Ÿåº¦å¿«ï¼Œå¯¹äºä¸è¦æ±‚å¯é åˆ°è¾¾çš„æ•°æ®ï¼Œå°±å¯ä»¥ä½¿ç”¨UDPåè®®ã€‚</font></p>

<p>æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•é€šè¿‡UDPåè®®ä¼ è¾“æ•°æ®ã€‚å’ŒTCPç±»ä¼¼ï¼Œä½¿ç”¨<mark>UDPçš„é€šä¿¡åŒæ–¹ä¹Ÿåˆ†ä¸ºå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨</mark>ã€‚æœåŠ¡å™¨é¦–å…ˆéœ€è¦ç»‘å®šç«¯å£ï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# ç»‘å®šç«¯å£:
s.bind((&#39;127.0.0.1&#39;, 9999))
</code></pre>

<p>åˆ›å»ºSocketæ—¶ï¼Œ<code>SOCK_DGRAM</code>æŒ‡å®šäº†è¿™ä¸ªSocketçš„ç±»å‹æ˜¯UDPã€‚ç»‘å®šç«¯å£å’ŒTCPä¸€æ ·ï¼Œä½†æ˜¯ä¸éœ€è¦è°ƒç”¨<code>listen()</code>æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥æ¥æ”¶æ¥è‡ªä»»ä½•å®¢æˆ·ç«¯çš„æ•°æ®ï¼š</p>

<pre><code class="language-py">print &#39;Bind UDP on 9999...&#39;
while True:
    # æ¥æ”¶æ•°æ®:
    data, addr = s.recvfrom(1024)
    print &#39;Received from %s:%s.&#39; % addr
    s.sendto(&#39;Hello, %s!&#39; % data, addr)
</code></pre>

<p><code>recvfrom()</code>æ–¹æ³•è¿”å›æ•°æ®å’Œå®¢æˆ·ç«¯çš„åœ°å€ä¸ç«¯å£ï¼Œè¿™æ ·ï¼ŒæœåŠ¡å™¨æ”¶åˆ°æ•°æ®åï¼Œç›´æ¥è°ƒç”¨<code>sendto()</code>å°±å¯ä»¥æŠŠæ•°æ®ç”¨UDPå‘ç»™å®¢æˆ·ç«¯ã€‚</p>

<p>æ³¨æ„è¿™é‡Œçœæ‰äº†å¤šçº¿ç¨‹ï¼Œå› ä¸ºè¿™ä¸ªä¾‹å­å¾ˆç®€å•ã€‚</p>

<p>å®¢æˆ·ç«¯ä½¿ç”¨UDPæ—¶ï¼Œé¦–å…ˆä»ç„¶åˆ›å»ºåŸºäºUDPçš„Socketï¼Œç„¶åï¼Œä¸éœ€è¦è°ƒç”¨<code>connect()</code>ï¼Œç›´æ¥é€šè¿‡<code>sendto()</code>ç»™æœåŠ¡å™¨å‘æ•°æ®ï¼š</p>

<pre><code class="language-py">s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
for data in [&#39;Michael&#39;, &#39;Tracy&#39;, &#39;Sarah&#39;]:
    # å‘é€æ•°æ®:
    s.sendto(data, (&#39;127.0.0.1&#39;, 9999))
    # æ¥æ”¶æ•°æ®:
    print s.recv(1024)
s.close()
</code></pre>

<p>å®¢æˆ·ç«¯:<br/>
<img src="media/14957648679184/14957678758780.jpg" alt=""/><br/>
æœåŠ¡å™¨:<br/>
<img src="media/14957648679184/14957678951579.jpg" alt=""/><br/>
å®¢æˆ·ç«¯ä»æœåŠ¡å™¨æ¥æ”¶æ•°æ®ä»ç„¶è°ƒç”¨<code>recv()</code>æ–¹æ³•ã€‚</p>

<h2 id="toc_0">å°ç»“</h2>

<p>UDPçš„ä½¿ç”¨ä¸TCPç±»ä¼¼ï¼Œä½†æ˜¯ä¸éœ€è¦å»ºç«‹è¿æ¥ã€‚æ­¤å¤–ï¼ŒæœåŠ¡å™¨ç»‘å®šUDPç«¯å£å’ŒTCPç«¯å£äº’ä¸å†²çªï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒUDPçš„9999ç«¯å£ä¸TCPçš„9999ç«¯å£å¯ä»¥å„è‡ªç»‘å®šã€‚</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/5/26</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%BB%83%E4%B9%A0.html'>Pythonç»ƒä¹ </a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_11.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_13.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="/asset/img/logn.png" /></div>
            
                <h1>LZH007</h1>
                <div class="site-des">LZHçš„æŠ€æœ¯æ‚äº‹å°åšå®¢~</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/lockxmonk" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lzhabc007@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="MAC%20OS.html"><strong>MAC OS</strong></a>
        
            <a href="Effective%20OC2.0.html"><strong>Effective OC2.0</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>æœºå™¨å­¦ä¹ </strong></a>
        
            <a href="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html"><strong>ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</strong></a>
        
            <a href="Python%E7%BB%83%E4%B9%A0.html"><strong>Pythonç»ƒä¹ </strong></a>
        
            <a href="%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%8A%80%E6%9C%AF.html"><strong>å›¾åƒå»é›¾æŠ€æœ¯</strong></a>
        
            <a href="iOS.html"><strong>iOS</strong></a>
        
            <a href="English%20Study.html"><strong>English Study</strong></a>
        
            <a href="%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0.html"><strong>ç®—æ³•å­¦ä¹ </strong></a>
        
            <a href="%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html"><strong>å¸¸è§é¢è¯•é—®é¢˜</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15114072257756.html">Best Time to Buy and Sell Stock II</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15114062990936.html">Best Time to Buy and Sell Stock</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15114035796843.html">Array Partition I</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15114026353930.html">3Sum</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15113375347657.html">Arranging Coins</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
          <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1265629731'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s22.cnzz.com/z_stat.php%3Fid%3D1265629731%26online%3D1' type='text/javascript'%3E%3C/script%3E"));</script>    
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2017
Powered by <a target="_blank" href="https://lockxmonk.github.io/index.html">LZH</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
